{"/":{"title":"ü™¥ Start Here","content":"Authored By:: [[P- Rob Haisfield]], [[P- Joel Chan]], and [[P- Brendan Langen]]\n\nPublished - May 2022\n\nWelcome to the decentralized discourse graph hypertext notebook. This notebook will continue to evolve going into the future and is by no means a finished product. Please leave comments to further the discussion and help direct its evolution.\n\n**All you need to do is click on what is interesting.** This is a special trait of this website - hypertext notebooks are designed for exploratory browsing. In a linear piece of writing, all ideas would be sequenced into one narrative. In hypertext writing, the narrative is emergent and the ideas are associative. It is like a book where the pages rearrange themselves for you based on your current curiosity each time you revisit. *For more on what this is, see [[Q- What is a hypertext notebook]]?*  \n\nThe goal of this research project is to find data structures and interfaces that support synthesis and innovation in a decentralized discourse graph. *See: [[Q- What is a decentralized discourse graph]]*\n\nSome specific how-tos:\n- Pages have prefixes in their page titles that indicate what type of node it is:\n\t- \"C- \" Claim\n\t- \"Q- \" Question\n\t- \"R- \" Resource\n\t- \"I- \" Idea\n- Hover your mouse over a link to see a preview of its content\n- If a page is empty, look through its backlinks! If a page isn't empty... you can still look through its backlinks! Those are all of the times where the ideas presented in the page were used in context.\n- If a page is not yet created, the hyperlink will appear to be faded.\n- Builders: View this as a framework for navigating the idea maze, and form your own conclusions about the claims, questions, and trails between them.\n- Command+k or ctrl+k to search\n- Email hello@robhaisfield.com for personal inquiries or private questions, but please defer to public comments when possible, as other people may have the same question as you!\n\nSome suggested starting points:\n- For broader context about what we are doing:\n\t- [[C- Discourse graphs could significantly accelerate human synthesis work]]\n\t- [[Q- What is a decentralized discourse graph]]\n\t- [[Research Grant Application]]\n- To begin with key questions:\n\t- [[Q- What user behaviors are people doing already that imply structure that is not being instantiated into a literal structure]]\n\t- [[Q- What are powerful interfaces for entering information into a discourse graph]]\n\t- [[Q- What are powerful primitives for a user of a decentralized knowledge graph]]\n- To jump straight into the ideas:\n\t- [[I- Search as a part of the primitive design]]\n\t- [[I- A DSL for a discourse graph with information entry, visualization, and retrieval]]","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/-Andy-Matuschak":{"title":"@ Andy Matuschak","content":"","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/About-this-notebook":{"title":"About this notebook","content":"\nAuthored By: \n","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/Adding-structure-later-feels-like-a-chore":{"title":"Adding structure later feels like a chore","content":"\nAuthored By:: [[P- Rob Haisfield]], [[P- Brendan Langen]]\n\nIn the [[structure now vs. later (uncertain payoff, regret)|structure now vs. later]] tradeoff, people often defer structure until later. For more details on that, see [[Why people prefer to structure later or not at all]]. \n\nAdding structure later is a viable alternative, as people likely have a better idea of how their current idea fits into the wider whole. But, we frequently heard that the act of adding structure later **feels like a chore**. \n\nThis showed up when people described processing reading notes, refining old ideas, or managing their queue of work. The idea of a queue of work showed up in many interviews, in a variety of forms.  People would create indexes, pages of TODOs, resource lists, pipelines, areas to review flashcards, or tagged items to return to later. \n\nIdeas were even processed through a queue in certain notebooks, with different stages of an idea categorized (fleeting, literature note, evergreen, etc.) or labeled in a pipeline (premise, develop, completed, etc.).  Some queues were managed specifically, with a spaced repetition algorithm prompting them to return to a prior block.  Others were less formally organized, and people returned to them as a regular, unscheduled part of their work.\n\nA common failure mode for queues was to process the same list multiple times.  For example, someone might look through an inbox of ideas, find the best ones, and then move those over into another list. Then, a month later, they would look through the inbox again, repeating the process.  The problem is that it was difficult to distinguish which ideas they had already processed, so they would repeat the work of screening through ideas they had already screened out. We can see systems like [Allen Wilson's](https://zettelkasten.de/posts/playing-zettelkasten-rpg-through-arbitrary-constraints/) evolve to prevent this repeat work.\n\nOne interviewee went as far as mentioning avoiding the tasks they created to review old notes. Others mentioned reviewing notes felt like repeat effort. \n\nWhile queue processing can be extremely effective to those who have internalized the practice, it was done by very few of our interviewees.\n\nFor more, see: [[C- Current tools do not support flexible exploration and refactoring of structures as they inevitably evolve]]. \n\ntl;dr summary\n\t- Existing structural systems are often seen by people as a chore. Existing queue management systems are too tedious. The story is clear ‚Äì most people don't want to spend time refactoring notes. That feels like [[wasted repeated effort]].\n\t- People may decide not to regularly return to past ideas, which will enable them to stay in flow of their current work. Provided the note can stand on its own, or they don‚Äôt need to add on, this quick capture ‚Äúset it and forget it‚Äù can be fine. \n\t- Problems arise when we want to reuse these old ideas. Inevitably the noise in our notebooks is too loud to find the signal we‚Äôre looking for. Creating queues of work to return to later feels like a chore, and our tools don‚Äôt help us resolve this.\n\n","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/Agenda":{"title":"Agenda","content":"\nAuthored by:: [[P- Brendan Langen]]\n\nPlayful time-based note-taking system from [[P- Alexander Griekspoor]], built with a series of sample templates to prompt users into creating .\n\nAn interesting aspect of Agenda is an embedded community forum within the app that enables users to discuss workflows, build on, etc. +\n\n![[Pasted image 20211001101109.png]]\n\n![[Pasted image 20211001101437.png]]\n\nDiscussed in the Metamuse podcast with [[P- Adam Wiggins]] and [[P- Mark McGranahan]].","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/Agora":{"title":"Agora","content":"\nAuthored by:: [[P- Brendan Langen]]\n\nAgora is a wiki-like distributed knowledge graph blended with a social network. The shared Agora can be contributed to by anyone who agrees to the social contract. Agora aims to facilitate sharing of synthesis.\n\n[[Q- What synthesis behaviors must be done by an individual and what responsibilities can be distributed to many people]]\n\n[[C- An ideal decentralized knowledge graph would map a social graph and a knowledge graph]]\n\nbuilt by [[P- Flancian]]\n\n![[Pasted image 20210916165952.png]]","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/Airtable":{"title":"Airtable","content":"\nAuthored by:: [[P- Brendan Langen]]\n\nAirtable is database platform that enables [[end-user programming]] through its [[DSL]]. Airtable markets themselves as a low code editor for building collaborative apps, as many macros and functions are built in as design primitives. \n\nOn the surface, Airtable looks like a spreadsheet, but built-in relational database features allow for greater scaling and advanced uses, including active app storage and maintenance. \n\n![[Pasted image 20210928162721.png]]","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/Archiving":{"title":"Archiving","content":"\nForgetting is powerful. If something is no longer important, its presence in your memory is a burden, turning your \"search results\" into a noisy mess that decreases your likelihood of finding the right answer.\n\nMany people use personal knowledge management software to augment their memories. \"Your mind is for having ideas, not holding them\" as stated in the infamous productivity text \"Getting Things Done.\" We saw this again and again in our interviews. Expert synthesizers use thought processors to ensure good ideas don't fall through the cracks. Personally, I use thought processors in conjunction with personal knowledge management in order to ensure the lessons I learn today retain their usefulness in 10 years. If at that point my questions and ideas are dependent on recent memory rather than a decade of learning, I've failed.\n\n[[C- People process complex information in multiple levels and stages of processing]]. Over the course of 1 year using Obsidian, I wrote more than a million words. Maybe 30% of that was from educational materials. The rest was \"thinking out loud\" in order to process thoughts. Eventually, many of those thoughts would be processed into an information artifact. Those artifacts became my canonical point of reference.\n\nWhile this multiplicity is generally useful during the process of thinking through an issue, later down the line reviewing it contributes to the feeling of [[wasted repeated effort]]. Early versions of information in the final artifact can likely be forgotten without much consequence.\n\nAdditionally, over the course of a decade, I'll certainly update my beliefs and mental models. If that happens, I'll have to ask myself, \"Is it important to keep a record of my beliefs at that point in time, or is leaving it in my database weighing it down?\"\n\nI don't know how software can automatically archive this information without making users mad at some point or another. This is a hard problem, but it should be clear that [[C- Bulk refactors are a necessary primitive to maintaining a decentralized discourse graph]].\n\nAt the extreme end, Venkatesh Rao jokingly referred to an archetype of user as [\"mind-palace pack rats\"](https://twitter.com/vgr/status/1370583027512872961). These are the people who don't know how to delete information and want to either memorize or save all information they encounter. I occasionally fall into this trap. [[C- Predicting trajectories of future reuse of information objects is hard]], and information hoarders may believe that everything will be reused at some point. Additionally, they may believe that it's better to have information and not need it than need it and not have it.\n\nWhether or not either of these beliefs turn out to be true, the hoarders will produce a massive amount of information over the course of 10 years. \n\nIf the first belief is false and they never delete or archive information, then this means they will have too much information to search through in 10 years, turning their personal database into a noisy mess with far too much to sift through for efficient reuse. Whether or not the second belief is true, it must be weighed against the immense cost of bloating your knowledge graph.\n\nThey also may believe that it's better to have information and not need it than \n\nThe problem is that over the course of 10 years, I'll generate so much content that many of it will simply appear to be noise. Not everything you believe to be important today will be important then. Most likely, you will only want the most important artifacts due to time constraints.\n","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/Atlas.TI":{"title":"Atlas.TI","content":"Authored by:: [[P- Brendan Langen]] and [[P- Joel Chan]]\n\nAtlas.TI is a is a qualitative data analysis and research tool. While Atlas.TI focuses on a broader set of tools to assist researchers, many \n[[C- Scholars repurpose qualitative data analysis software to facilitate synthesis]]. \n\nAtlas.TI has a robust feature set, including a [[DSL]] that extends the researcher's reach. \n![[Pasted image 20210921172215.png]]\n\nWriteup of using Atlas.TI - https://atlasti.com/2017/02/09/lit-reviews/\n\n![[Pasted image 20210921172109.png]]\n\nAnnotation in the margins.\n![[Pasted image 20210921172131.png]]","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-A-DSL-allows-people-to-expand-their-use-cases-far-beyond-the-imagination-of-the-designer":{"title":"C- A DSL allows people to expand their use cases far beyond the imagination of the designer","content":"[[C- A DSL allows for abstraction]]. \n\n[[C- End-user programming enables the developers to be lazy about their backlog of feature requests]]. \n\nWe see this in communities like [[Roam Research]] and [[Obsidian]], where members design new add-ons and build the ecosystem of the product, picking off most of the low-hanging fruit from the product's current version. This also enables development teams to focus on their own areas of expertise, leaving other work for interested community developers. \n","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-A-DSL-could-be-a-powerful-interface-for-entering-information-into-a-discourse-graph":{"title":"C- A DSL could be a powerful interface for entering information into a discourse graph","content":"Authored By:: [[P- Rob Haisfield]]\n\n[[C- A structural editor can make a DSL approachable to end-users]]\n\n[[C- A DSL would let people write in a certain syntax and notation that gets transformed by functions into a data structure that can be manipulated by pre-built or custom-built functions]]\n\n[[C- End user programming enables people to bulk process notes]] and a DSL enables that.\n\nSome examples might include using attributes in Roam to indicate key-value pairs, or [[I- I should be able to leave a hole to fill in the blanks for an idea or domain|being able to leave a hole]] for later input (like in algebra, you figure out what x is in x+3=11 after writing out the equation).\n\n[[C- A DSL speeds up the author]]","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-A-DSL-speeds-up-the-author":{"title":"C- A DSL speeds up the author","content":"See this example here: doing some relatively common action in a GUI is never going to really get that much faster, whereas in programming, with abstractions and copy-paste, a DSL can get faster. Additionally, if a [[GUI]] requires you to click on things and then type text, it's probably not going to get faster than a textual user interface. https://www.figma.com/file/dTePTU7khNGg53ho1pbH8S/DSL-vs.-GUI-Speed-Comparison?node-id=0%3A1","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-A-key-requirement-to-participating-in-a-discourse-graph-for-a-specific-domain-is-knowing-the-vocabulary-used-in-that-graph":{"title":"C- A key requirement to participating in a discourse graph for a specific domain is knowing the vocabulary used in that graph","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\nSearch queries only work when the searcher knows the vocabulary used in the information being searched. If I were trying to find the answer to a question about cancer biology, I might search Google Scholar or PubMed and find nothing, while an expert would easily find the answer, simply because they know the vocabulary and I don't. \n\n[[Q- How do we solve the problem of different people referring to the same concept with different language]]? A key barrier to interdisciplinary synthesis is simply that people are unaware of the fact that people in other fields are asking the same questions with different language. This means that it will be important for the [[Search Behavior]] affordances to support people's ability to learn the vocabulary of the fields they are querying. \n\n*See [[R- Information Foraging Video]] discussing Mr. Taggy as a potential solution.*\n","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-A-structural-editor-can-make-a-DSL-approachable-to-end-users":{"title":"C- A structural editor can make a DSL approachable to end-users","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\nText does not need to be people's interface for editing code. Structural editors like Fructure can make it impossible to write code that doesn't work. Based on the function you are writing, it will suggest valid arguments to you. Advanced, context-aware autocomplete can perform the same job.\n\n{{\u003c youtube CnbVCNIh1NA \u003e}}\n\nSpreadsheets make CSVs more approachable. Airtable makes relational databases approachable. The issue with text on its own is often that it requires users to memorize a syntax, and that scares people. ","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-A-structural-editor-could-allow-people-to-make-and-edit-data-structures":{"title":"C- A structural editor could allow people to make and edit data structures","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\nIn  [[Notion]], you have many different views of the same underlying relational databases. You can edit or view them as Tables, Calendars, Boards, Lists, Galleries, and Gantt Charts. This means Notion provides multiple structural editors for relational databases.\n\n[[TheBrain]] has a graphical and textual interface for navigating and editing your documents.\n","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-An-ideal-decentralized-knowledge-graph-would-map-a-social-graph-and-a-knowledge-graph":{"title":"C- An ideal decentralized knowledge graph would map a social graph and a knowledge graph","content":"\nAuthored By::\n[[Q- Should social knowledge management be thought of as social networks with really solid defaults, conventions, and incentives]]\n\n[[Q- Can the blockchain be used to improve citation chains]]","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Breaking-ideas-down-into-component-parts-facilitates-creative-reinterpretation":{"title":"C- Breaking ideas down into component parts facilitates creative reinterpretation","content":"Authored By:: [[P- Joel Chan]]\n\nBreaking ideas down into their component parts allows us to creatively reinterpret ideas for new purposes, such as creating a new understanding of a problem, or creatively reuse ideas in new contexts.\n\nIn insight problem solving, there is the idea of chunk decomposition (see, e.g., [[@knoblichConstraintRelaxationChunk1999]]). The basic idea goes something like this:\n\n- People create chunks to organize experience. Knoblich characterizes Chunks as \"patterns that capture recurring constellations of features or components\" (p. 1535)\n- Encountering a novel problem activates some chunks from past experiences. Not all of them will be helpful; and some will be actively unhelpful. And some will need to be decomposed (compression reversed??)\n- Evidence for this comes from experiments where chunk decomposition was impaired, leading to worse performance on solving the problem.\n\n[[@mccaffreyInnovationReliesObscure2012]] has extended this idea to detail how being able to \"break down\" an idea or object and see its component obscure features in a function-free manner is a powerful strategy for breaking fixation by creatively recombining objects to come up with novel solutions.","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Bulk-refactors-are-a-necessary-primitive-to-maintaining-a-decentralized-discourse-graph":{"title":"C- Bulk refactors are a necessary primitive to maintaining a decentralized discourse graph","content":"","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Composability-facilitates-synthesis":{"title":"C- Composability facilitates synthesis","content":"The basic intuition here is that because synthesis involves creating a whole from new parts, composability facilitates synthesis. As it is difficult to compose large chunks of highly nuanced information, [[C- Compression facilitates synthesis|compression]] facilitates composability by giving individuals smaller, easier to remember \"handles\" to piece together, through a mechanism more fully described in [[C- Hypertext enables communication with high information density]].","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Compression-and-contextualizability-are-in-tension":{"title":"C- Compression and contextualizability are in tension","content":"","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Compression-facilitates-synthesis":{"title":"C- Compression facilitates synthesis","content":"Authored By:: [[P- Joel Chan]]\n\nSynthesis is creating a new whole out of component parts. But what should the \"parts\" look like? What kinds of building blocks would facilitate synthesis?\n\nOne important dimension of conceptual building blocks for synthesis is \"**size.**\"\n\nIntuitively, having building blocks that are too \"large\" or complex would make reassembly into a new whole impractical or impossible. Thus, the most basic requirement for synthesis is having appropriate(ly sized) building blocks to start with.\n\nThere are a few ways to think about size and why it matters for synthesis.\n\n- A simple benefit is just making it easier to find what you're actually looking for! For example, [[@ribaupierreExtractingDiscourseElements2017]] found that scientists who compared an enhanced faceted search engine (with ability to search a large corpus of articles by discourse elements) to a regular keyword search system in terms of efficacy for various targeted search tasks, self-reported a significantly higher signal-to-noise ratio in results with the faceted search system compared to the keyword search system\n- Decomposing ideas into smaller pieces also enables us to **connect ideas in richer and more meaningful ways**. For example, we learn from creativity theory that [[C- Breaking ideas down into component parts facilitates creative reinterpretation]]. This is because synthesis is a creative act, and conceptual combination is-fundamental-to creative knowledge production.\n\nIt is important to distinguish atomicity from compression - in creative thought, it is less about decomposition in the atomic and disconnected sense, and more about flexibly using compression to move between different levels and states of \"granularity\".\n\n- We see this chiefly from the literature on insight problem solving, which informs us that [[C- Breaking ideas down into component parts facilitates creative reinterpretation]].\n- A related idea is that [[C- People process complex information in multiple levels and stages of processing]].\n\nWe might wonder: if we break complex documents down in a [synthesis] [infrastructure], what should the component parts look like? What defines an \"idea\" level, or an appropriately \"small\" building block for scholarly synthesis?\n\n-   One answer to this is that scholarly synthesis is argumentation and scholarly argumentation operates on atomic statements and concepts as fundamental units.\n-   This should be nuanced, though, because restricting ourselves to only linguistic / symbolic representations would probably be a mistake. There are other forms of knowledge, visual and otherwise, that are important \"units\" that can't be reduced to their \"underlying\" linguistic representation.\n    -   See, for example, **prototypes** [[@galeyHowPrototypeArgues2010]], or **strong concepts**[[@hookStrongConceptsIntermediatelevel2012]]\n    -   Even [[@mccrickardMakingClaimsKnowledge2012]]'s idea of **standard/Claims**\n-   We also may not want to \"reduce\" ideas to a single representation, since [[C- Multiplicity is necessary for synthesis]].\n\nUnfortunately, [[C- Most scholarly communication infrastructure operates on the document as the base unit]]\n\n- This is important, because, like insight problem solving, if chunk decomposition is harder, then the task of synthesis gets harder.\n- Scholars encounter a problem with \"chunks\" (papers, sources, etc.), either ones they have already, or ones they seek out.\n- They want to construct a new understanding. To do that, they need to decompose the chunk of the paper/source/chapter, etc. into the component parts they care about, to be able to manipulate them, combine them, move them around, etc. (cf. scholarly argumentation operates on atomic statements and concepts as fundamental units).\n- The source of difficulty for chunk decomposition in this case isn't necessarily the fact that the chunks are \"tight\" in the specific sense that Knoblich meant: that is, the components of the chunk *are* meaningful. It's just that the scholar has to \"chisel\" them out of the paper. It takes work!\n- But the point remains that having access to the chunks is important!","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Context-is-necessary-for-knowledge-reuse":{"title":"C- Context is necessary for knowledge reuse","content":"\nAuthored By:: [[P- Joel Chan]]\n\nThere is a long history of research in computer-supported cooperative work (CSCW) that documents how knowledge needs to be contextualized to be usefully reused. \n\nOne common way that this insight has been demonstrated is when reuse fails due to a lack of context. For example, in a case study of calendar systems, [[R- Information and Context|@dourishInformationContextLessons1993]] documented how having metadata for event information such as the title of the event or arrival time of the speaker, in addition to who the author of the information is, were critical for the interpretation of the events. Similarly, [[R- Context Grabbing Assigning Metadata in Large Document Collections|@hinrichsContextGrabbingAssigning2005]] documented how decentralized project documentation made recontextualization difficult for engineers because they didn't know the history of changes for a document or if a particular document was up to date, leading to issues such as \"exploratory digging by hand\" to avoid damaging power lines. Finally, in a field study of collaborative information reuse in aircraft technical support [[R- Beyond Boundary Objects|@luttersBoundaryObjectsCollaborative2007]], engineers lamented reusing old records because information was missing, outdated, or not appropriate anymore because of procedural changes. Over the years if any changes to the records were not tagged, the context of those changes were lost.\n\nCloser to the setting of synthesis that we care about, context can refer to methodological details like who was studied and with what measures, as well as the intellectual history of the authors of a study [[R- Collaborative information synthesis 1]]. Properly contextualizing knowledge claims with these details can mean the difference between loosely or superficially assembling an argument, or an [[Q- What is synthesis | effective synthesis]]","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Contextualizability-is-necessary-for-synthesis":{"title":"C- Contextualizability is necessary for synthesis","content":"\nAuthored By:: \n","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Curation-is-an-important-role-in-maintaining-a-decentralized-discourse-graph":{"title":"C- Curation is an important role in maintaining a decentralized discourse graph","content":"","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Current-tools-do-not-support-flexible-exploration-and-refactoring-of-structures-as-they-inevitably-evolve":{"title":"C- Current tools do not support flexible exploration and refactoring of structures as they inevitably evolve","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n\n\nPredefining a system limits our ability to explore alternatives outside the current structure.  This cascades into the decisions to be made while in the act of work ‚Äì what folder to place the note in, what naming conventions to use, what metadata should be added, and on.  \n\nWhen something doesn't feel like it fits into a pre-existing structure, participants would feel stressed out. This happens frequently while exploring a new domain of knowledge, as people don‚Äôt yet know what the most useful schema will be. People also like to change their schemas as their skill with a thought processor increases and they learn more optimal paths.\n\nA simple, practical example: \n\nImagine you have a template for each book you read, but you want to add additional attributes later. Updating your template is nice, but it doesn't highlight what's missing from current files. This creates more work for yourself whenever you want to make an update!\n\nIn note writing tools like Obsidian and Athens, adding this new metadata to instances of the template in hindsight is entirely manual.  [[It's hard to change structure later]]. In a relational database like Notion or Airtable, the process is a little easier: add a new column to the table and filter or apply conditional formatting for empty fields.\n\nOf course, done correctly, this structure can also enable easier reuse of past thoughts and the sharing of ideas with others. As we‚Äôve discussed, structure is not a bad thing! However, the limitations of predefining structure are worth noting. Choosing not to define consistent attributes, or simply using implicit structure in notes, allows for flexibility in our thought process and preserves optionality.  On the downside, this limits our ability for rich querying and flexibility of ‚Äúviews‚Äù down the road. Then we have to rely on our calcified memory of attributes to refactor our work. \n\nIf we want structure in our notes, our current tools require that we explicitly add it, in the form of names, placement or attributes. Certain sorts of structure are more rigid than others. For example, open ended metadata enabled email to be expanded into new use cases.\n    - \u003e¬†You can add any headers you like. This makes the set of headers an open set, enabling new use-cases to evolve from the bottom-up. Want to record the title, author, and publish date of your books? Add some headers. Want to include a location? Add a header.¬†\n\n    [[P- Gordon Brander]] [If headers did not exist, it would be necessary to invent them](https://subconscious.substack.com/p/if-headers-did-not-exist-it-would)\n\nAlso, whether there are multiple paths to the data is important.  For example, someone using Roam might be more comfortable throwing many types of structure at the wall because they know if one system fails to resurface a note¬†they likely have a fallback. In a file / folder system like Evernote, the folder location of a note matters much more because search is your only fallback if you don't remember which folder a note was in. For example, someone using Roam might be more comfortable throwing things at the wall because they know if one system fails to resurface something¬†they might have a fallback\n\nFor more details on how to enable restructuring in hindsight, see [[I- Search as a part of the primitive design]] and [[Agora]], as well as the question [[Q- How might we allow people to adapt their past system and notes to current needs]]\n","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Designing-for-ambient-review-is-a-rich-opportunity-space":{"title":"C- Designing for ambient review is a rich opportunity space","content":"If [[People don‚Äôt intentionally review old notes]] but still want old notes to resurface, we might ask how to display that information in an ambient manner.\n\nAs described in [[C- Reviewing past notes in the process of creating new notes is a key user behavior to promote synthesis]], as you look for a spot to place a new card in a [[Zettelkasten]], you unintentionally pass other notes, sparking new ideas. [[search or create]] and [[autocomplete]] accomplish similar outcomes, leading people to [incrementally process their notes](https://roamresearch.com/#/app/write-hypertext-notebook-graph-research/page/FgDkjHlEG) as they review information they didn't intend to review.\n\nMany apps will augment search with additional suggestions from previously saved data. For example, [[Evernote]] has a web browser extension that displays possibly relevant notes whenever you search Google. [[WorldBrain Memex]] maintains a full text index of all of the web pages you have saved or annotated and does the same.\n\nOn [[Twitter]], the right side of the desktop screen displays trending information. For people who use Twitter as a social knowledge management tool, trending stories is often distracting. [[P- Geoffrey Litt]] created the [[Twemex]] browser extension to replace that with more helpful ambient information. For example, if you are on a profile, it displays either their top tweets or your conversations with the individual. While viewing a tweet, it displays quote tweets, which can be thought of as [[backlinks]].\n\n[[Threadhelper]] is another browser extension that searches your tweet, bookmark, and retweet history while you are writing a tweet to help you compose quote tweet heavy threads. While [[Twemex]] enables you to manually write searches while tweeting, those searches are, by nature, intentional. Since Threadhelper creates constantly updating searches for similar tweets, it is more likely to surprise the author.\n\nHowever, [[C- Linked references are a smart default for related items]]. What if we could have a smarter concept of relatedness? For example, if I'm writing on a page about the intersection of two topics, wouldn't it be more helpful to find other areas where those two topics co-occur? If I'm talking about 5 topics, areas where those 5 topics co-occur?\n\nThe problem is that as you increase the number of topics that need to intersect, you also decrease the total number of results. It's unlikely that you have another page in your notes that reference all 10 of the pages you referenced in the note you are writing right now!\n\nFor this, I propose a slider. This slider would allow you to say: \"show me any pages that mention at least 3/10 of these\n\n-   ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwrite-hypertext-notebook-graph-research%2FbltwPhVuwk.png?alt=media\u0026token=1e4fbbb3-9659-4fe5-a39e-99c65a967a36)\n\nFor more detail on this idea, see part 3 of this: [https://www.figma.com/file/5shwLdUCHxSaPNEO7pazbe/Dhrumil%26Robert---RoamGames-Challenge-2?node-id=0%3A1](https://www.figma.com/file/5shwLdUCHxSaPNEO7pazbe/Dhrumil%26Robert---RoamGames-Challenge-2?node-id=0%3A1)\n\nA constraint here is that this could be seen as distracting, as sometimes people even complain that options presented by wikilink autocompletion would distract them from the thought they were actually trying to get down. In these instances, the user should be able to toggle it off.\n\nHowever, distraction can be valuable. [[P- Devon Zuegel]] claims that her most important insights came in conversations with others, where they would ask surprising questions that force her to reconsider ideas, rotate them, or examine them through a different lens. Surprise is a key ingredient for synthesis.\n\nSimple UX changes can often be enough to facilitate ambient review. For example, [[CatoMinor]] created CSS that would move the linked references for a page in [[Roam]] to the right side of the view. When they were at the bottom of the view for a page, this meant that users needed to manually scroll to view linked references, which would take them out of the act of writing. By displaying linked references on the right, [it was easier to review without breaking their flow](https://twitter.com/CatoMinor3/status/1496467417098248192?s=20\u0026t=TaH4nzPwDSSr5t3DW5x9uQ).\n\nIn our interviews, we encountered multiple people who would set up [[Spaced repetition systems]] to resurface their most important notes periodically. Spaced repetition is typically used with flashcards to memorize information, but the people we interviewed had little interest in memorization.\n\nInstead, their goal was to review notes in a context independent way. If they used linked references as a primary method to resurface notes, they would only see entries that were contextually relevant to their current tasks.\n\nMany people incorporate spaced repetition directly into their note taking tools, as can be seen in [[P- Maggie Appleton]]'s [roam tour](https://youtu.be/RXXXHN516qc?t=1601). This enables Maggie to gradually cultivate her evergreen notes without deliberately seeking them out.","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Discourse-graphs-could-significantly-accelerate-human-synthesis-work":{"title":"C- Discourse graphs could significantly accelerate human synthesis work","content":"\n\nAuthored By:: [[P- Joel Chan]]\n\nAn exciting hypothesis that motivates this work is that making discourse graphs widely available could accelerate human synthesis work, and thereby [[C- Effective synthesis is necessary for innovation and scientific progress | accelerate innovation and scientific discovery]].\n\nTo understand why this information model might augment synthesis, consider a researcher who wants to understand what interventions might be most promising for mitigating online harassment. To synthesize a formulation of this complex interdisciplinary problem that can advance the state of the art, she needs material that can help her work through detailed answers to a range of questions. For example, which theories have the most empirical support in this particular setting? Are there conflicting theoretical predictions that might signal fruitful areas of inquiry? What are the key phenomena to keep in mind when designing an intervention (e.g., perceptions of human vs. automated action, procedural considerations, noise in judgments of wrongdoing, scale considerations for spread of harm)? What intervention patterns (whether technical, behavioral, or institutional) have been proposed that are both a) judged on theoretical and circumstantial grounds as likely to be effective in this setting, and b) lacking in direct evidence for efficacy?\n\nThe answers to these questions cannot be found simply in the titles of research papers, in groupings of papers by area, or even in citation or authorship networks. The answers lie at lower levels of granularity: the level of theoretical and empirical **claims** or statements made within publications. For example, \"viewers in a Twitch chat engaged in less bad behaviors after a user was banned by a moderator for bad behavior\", and \"banning bad actors from a subreddit in 2015 was somewhat effective at mitigating spread of hate speech on other subreddits\" are claims that interrelate in complex ways, both supporting other claims/theories that are in tension with each other. \n\nThis level of granularity is crucial not just for finding relevant claims to inform the synthesis, but also for constructing more complex arguments and theories, by connecting statements in logical and discursive relationships. \n\nBeyond operating at the claim level, our researcher will also need to work through a range of **contextual details**. For example, to judge which studies, findings, or theories are most applicable to her setting, she needs to know key methodological details including the comparability of different studies' interventions, settings, populations, and outcome measures. She might need to reason over the fact that two studies that concluded limited efficacy of bans had ban interventions that were quite short, on a forum with no identity verification. Or she might reason through the fact that a prominent theory of bad faith and discourse was proposed by a philosopher from the early 2000's (before the rise of modern social media). To judge the validity of past findings (e.g., what has been established with sufficient certainty, where the frontier might be), she would need to know, for example, which findings came from which measures (e.g., self-report, behavioral measures), and the extent to which findings have been replicated cross authors from different labs, and across a variety of settings (e.g., year, platform, scale).\n\n### Hypothesized individual benefits: Creative synthesis and exploration\n\nA discourse graph as a data structure has key affordances that are hypothesized to enable just these sorts of synthesis operations. Information is represented primarily at the claim or statement level, and embedded in a graph of relationships with other claims and context. In a discourse graph, claims have many-to-many relationships to support composition of more complex arguments and theories, or \"decompression\" into component supporting/opposing claims. Contextual entities and information, such as methodological details and metadata, are explicitly included in the discourse graph. This supports direct analysis of claims with their evidentiary context, supporting critical engagement, integration, and even reinterpretation of individual findings. The following figure shows how this might be supported in the specific worked example above. Note that discourse graphs need not be represented or manipulated in this visual format; the underlying graph model can be instantiated in a variety of media, such as hypertext notebooks, and also implicitly in various analog implementations that allow for cross-referencing. What is important is the information architecture of representing networks of claims and their context.\n\n![[Fig - Discourse graph example for effects of bans on online antisocial behavior.png]]\n\nBeyond the theoretical match between the kinds of queries scientists need to run over their evidence collection for synthesis, a discourse-centric representation that encodes granular claims instead of document \"buckets\" could facilitate exploration and conceptual combination. There is theoretical precedent for this in research on expertise and creative problem solving, where breaking complex ideas down into more meaningful, smaller conceptual \"chunks\" may be necessary for creative recombination into new conceptual wholes ([[R- Achieving Both Creativity and Rationale]], [[R- The minds eye in chess]] [[R- Constraint relaxation and chunk decomposition in insight problem solving]], [[R- Innovation Relies on the Obscure]]). Removing contextual details (though not losing the ability to recover them) may also be necessary and useful for synthesizing ideas and reusing knowledge across boundaries ([[R- Institutional Ecology Translations and Boundary Objects]], [[R- Ambiguity and Engagement]]). \n\nAt the same time, constructive and creative engagement with contextual details, is thought to be necessary for developing novel conceptual wholes from \"data\", such as in sensemaking ([[R- The Cost Structure of Sensemaking]]), systematic reviews ([[R- Collaborative information synthesis 1]]), or formal theory development [[R- Theoretical musings]], [[R- Theory Before the Test]], [[R- Are theoretical results Results]], [[R- Darwin on man A psychological study of scientific creativity]]). This is a more specific application of the general insight that [[C- Context is necessary for knowledge reuse]]. Further, accurately predicting just which contextual details are necessary to represent directly in an information object is a difficult task ([[R- Organizational Memory as Objects Processes and Trajectories]], [[R- Beyond Boundary Objects]]) that may be functionally impossible in creative settings.\n\nThe conjunction of these affordances --- having both granular information objects like claims, and the ability to progressively expand their context by traversing/retrieving from the discourse graphs --- may help to resolve this tension between granularity and contextualizability. For instance, graph-model and hypertext affordances like hyperlinking or transclusion might enable scientists to hide \"extraneous details\" (to facilitate compression) without destructively blocking future reusers from obtaining necessary contextual details for reuse [[R- Organizational Memory as Objects Processes and Trajectories]], [[R- Beyond Boundary Objects]]. \n\n### Hypothesized collective benefits: Reduced overhead, and enhanced creative reuse\n\n**Discourse graphs (or parts thereof) could also significantly reduce the overhead to synthesis through reuse and repurposing over time, across projects, and potentially even across people.**[^1] For example, imagine collaborators sharing discourse graphs with each other, rather than simple documents full of unstructured notes, to speed up the process of working towards shared mental models and identifying productive areas of divergence; or a lab  onboarding new researchers not with long reading lists, but with discourse graph subsets they can build on over time. How much effort could be reduced if this were a reality?\n\n[^1]: [[P- Rob Haisfield|Rob]] comment: In case it was not clear the gravity of what's being communicated here: this is learning through portfolio effects at scale, spread across people. This drastically reduces the amount of work necessary to synthesize solutions to new problems. This could rapidly accelerate the pace of innovation, as discussed in [[C- Discourse graphs could significantly accelerate human synthesis work]].\n\nThe same affordances of discourse graphs around granularity and contextualizability that are hypothesized to augment individual synthesis should also facilitate exploration and reuse of an evidence collection that was created by someone else, or by oneself in the past. For example, granular representation of scientific ideas at the claim level is a much better theoretical match for the kinds of queries that scientists want to ask of an evidence collection during synthesis ([[R- Micropublications a semantic model]], [[R- From Proteins to Fairytales]],  [[R- ScholOnto an ontology-based digital library server for research documents and discourse]]).\n\nThese claims may also be more to the level of processing required to be understood and reused by others, compared to raw annotations and marginalia [[R- Exploring the Relationship Between Personal and Public Annotations]]. Also, ambiguity around concepts can be a significant barrier to reuse across knowledge boundaries. For example, keyword search is only really useful when there is a stable, shared understanding of ontology ([[R- Reasons for the use and non‚Äêuse of electronic journals and databases]]): this condition is almost certainly not present when crossing knowledge boundaries ([[R- Ambiguity and Engagement]], [[Q- How do we solve the problem of different people referring to the same concept with different language|Q- same concept, different language]]), and perhaps not even within fields of study with significant ongoing controversy amongst different schools of thought [[R- Epistemology and the socio-cognitive perspective in information science]] [^2]\n\n[^2]:[[P- Rob Haisfield|Rob]] comment: Keyword searches can also be useful when there is a system in place to help people new to the domain learn a new ontology. See [[R- Information Foraging Video#^OPpY9M]].\n \nIn these settings, judging that two things are \"the same\" is problematic and difficult task; doing so without engagement with context can sometimes introduce *more* destructive ambiguity, not less, a hard-won lesson from the history of Semantic Web [[R- In Defense of Ambiguity]], [[R- Digital Futures Sociological Challenges and Opportunities in the Emergent Semantic Web]]), ontology ([[R- What is a Distributed Knowledge Graph]], [[R- Distributed ontology building as practical work]]) and classification efforts [[R- Sorting Things Out Classification and Its Consequences]]. A discourse-centric graph that embeds concepts in discourse contexts, traversing through networks of contextual details (such as authors, measures, contexts), and perhaps augmented by formal concepts as hooks, may be a better match for exploring ideas across knowledge boundaries. Further, although in many instances of knowledge reuse, contextual details tend to vary substantially across reuse tasks ([[R- Organizational Memory as Objects Processes and Trajectories]], [[R- Sharing Knowledge and Expertise The CSCW View of Knowledge Management]]), there might be sufficient overlap of useful contextual details (e.g., participant information, study context) that remain stable across reuse tasks ([[R- Collaborative information synthesis 1]]). ","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Effective-individual-synthesis-systems-seem-to-mostly-exist-for-a-select-few":{"title":"C- Effective individual synthesis systems (seem to mostly) exist (for a select few)","content":"\nAuthored By:: [[P- Joel Chan]], [[P- Rob Haisfield]]\n\nReferences bricolage, using a slipbox, synthetic notes, framework of qualitative analysis to literature reviewing.","lastmodified":"2022-05-20T00:31:25.867404903Z","tags":null},"/C-Effective-synthesis-is-necessary-for-innovation-and-scientific-progress":{"title":"C- Effective synthesis is necessary for innovation and scientific progress","content":"\nAuthored By:: [[P- Joel Chan]]\n\nThe advanced understanding from an effective synthesis can be a powerful force multiplier for choosing effective studies and operationalizations ([[R- Theory Before the Test]], [[R- Replication Communication and the Population Dynamics of Scientific Discovery]], ,[[R- Why Hypothesis Testers Should Spend Less Time Testing Hypotheses]]), and may be especially necessary for problems where it is difficult or impossible to construct decisive experimental tests. The issue of mask efficacy for reducing community transmission is a powerful example of this; as [[R- Face Masks Against COVID-19]] put it, \n\n\u003e \"The standard RCT paradigm is well-suited to medical interventions in which a treatment has a measurable effect at the individual level and furthermore, interventions and their outcomes are independent across persons comprising a target population. By contrast, the effect of masks on a pandemic is a population-level outcome where individual-level interventions have an aggregate effect on their community as a system. Consider, for instance, the impact of source control ‚Äî its effect occurs to other individuals in the population, not the individual who implements the intervention by wearing a mask...Even then, ethical issues prevent the availability of an unmasked control arm (27). The lack of direct causal identifiability requires a more integrative systems view of efficacy. We need to consider first principles ‚Äî transmission properties of the disease, controlled biophysical characterizations alongside observational data, partially informative RCTs (primarily with respect to PPE), natural experiments (28), and policy implementation considerations ‚Äî **a discursive synthesis of interdisciplinary lines of evidence which are disparate by necessity**.\" (p. 3, emphasis ours)\n\nTo illustrate the power of synthesis for accelerating scientific progress, consider the example of Esther Duflo, who attributed her Nobel-Prize-winning work to the detailed synthesis of problems in developmental economics she obtained from a handbook chapter [[R- How to Find the Right Questions]]. Indeed, scientific progress may not even be tractable without adequate synthesis (as theory), even with advanced methods and data [[R- Could a Neuroscientist Understand a Microprocessor]]: as Allen Newell famously said, \"You can't play twenty questions with nature and win\" [[R- newellYouCanPlay1973]]. ","lastmodified":"2022-05-20T00:31:25.871404907Z","tags":null},"/C-Emoji-reactions-are-a-form-of-social-tagging":{"title":"C- Emoji reactions are a form of social tagging","content":"","lastmodified":"2022-05-20T00:31:25.871404907Z","tags":null},"/C-End-user-programming-enables-people-to-bulk-process-notes":{"title":"C- End user programming enables people to bulk process notes","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\nBulk processing is basically what computation was made for! We can see promising directions with [[Q- How might we apply map filter reduce to notes and what other primitives are relevant to this domain|map, filter, and reduce]].\n\nWith respect to [[refactoring tools]], I want to be able to bulk type / categorize notes in hindsight. Through the [[Jump]] model, you would [[C- Selection is a core primitive for Jump|select through search and manual addition and subtraction]], then apply some refactoring to all of them.","lastmodified":"2022-05-20T00:31:25.871404907Z","tags":null},"/C-End-user-programming-enables-the-developers-to-be-lazy-about-their-backlog-of-feature-requests":{"title":"C- End-user programming enables the developers to be lazy about their backlog of feature requests","content":"This was a claim from [[P- Geoffrey Litt]] on his episode of the Metamuse Podcast.","lastmodified":"2022-05-20T00:31:25.871404907Z","tags":null},"/C-End-user-scripting-enables-creative-workarounds":{"title":"C- End-user scripting enables creative workarounds","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n[User skill level increases over time](https://robhaisfield.com/notes/user-skill-level-increases-over-time), and people with high User Involvement understand your app‚Äôs core mechanisms so well that, when they face a new problem, they are able to come up with creative workarounds by rubber-banding multiple actions together. It's not uncommon for a new user to request a feature, and then have a more experienced user to mention that their request is already possible if they do X, Y, and Z things together, but the best the experienced user can do is recommend that sequence of steps. \n\nIt‚Äôs probably not uncommon for your users to have series of behaviors that they repeat frequently. [Joel Chan](https://twitter.com/JoelChan86), when writing in Roam, often writes both a block reference and a reference to the page the block was on in the same bullet. [Maxim Leyzerovich](https://twitter.com/round) often creates a ‚ÄúTable of Contents‚Äù for himself in Figma, where he turns a frame into a component, duplicates it, drags it over to his ToC location, and then shrinks it. I like to copy a link to a frame in Figma and place it as hyperlinked text elsewhere to create ‚Äúportals‚Äù around my Figma files.\n\nAdvanced functionalities can be pieced together from sequences of less advanced functionalities, but sometimes there are so many steps that it's not worth the effort! What if it was easy for users to collapse those sequences of actions into just one action that they could call at any point? The ability to script these actions together and call that script would be an act of effort-saving kindness.\n\n### Some possible UX mechanisms for enabling this:\n\nWhat if the user could ‚Äúrecord‚Äù their actions, and when they stopped the recording, it created a text script that just shows the commands you ran in a sequence? The user could edit it to their liking and save it with their own title, creating a new action that they could call at any point.\n\nWhat if you didn‚Äôt have to decide to record ahead of time? What if you could generate a script at any point from looking at your commands from X seconds back?\n\n![[CmdK-Superhuman.gif]]\n\nAlternatively, any app with a command palette like VS Code, Superhuman with their infamous Command+K, or Sketch with Sketch Runner, has a straightforward path to scriptability. These apps let you search for any action that is possible in the application by name to run it from there rather than having to move your mouse around.\n\nWhat if, any time you ran a command in the palette, you had an option to paste that command into a script? If you perform a sequence of commands, you would then be able to generate a script on the fly.\n","lastmodified":"2022-05-20T00:31:25.871404907Z","tags":null},"/C-Highlighted-and-lowlighted-search-results-map-to-how-well-results-map-to-intentions":{"title":"C- Highlighted and lowlighted search results map to how well results map to intentions","content":"Authored By:: [[P- Rob Haisfield]]\n\nWhat if the choice architecture for interacting with search results allowed you to indicate the strength of the links and how well it maps to your intended outcome? Maybe upvoting and downvoting results. Or collapsing, or clicking. [[C- User behavior within a well-designed choice architecture can be a signal of preferences]] This would allow people to weight connections.\n\nIf [[C- Search terms express intentions]], then highlighted (emphasized) and lowlighted (de-emphasized) results show how well the search matches the intention. [[Knovigator]] allows people to use quadratic voting on each block in order to indicate how important or relevant it is.\n\nLook at this search. A good amount of the results are irrelevant to my current query. I should be able to remove results and move them to the top.\n![[Pasted image 20211203210050.png]]  \n","lastmodified":"2022-05-20T00:31:25.871404907Z","tags":null},"/C-Hypertext-enables-communication-with-high-information-density":{"title":"C- Hypertext enables communication with high information density","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n[[C- Compression facilitates synthesis]] and hypertext facilitates compression.\n\nAndy Matuschak's working notes, like a wiki, use hypertext to communicate in an information dense way. This facilitates easy navigation where people are able to find what they are looking for [even when they don't quite know how to express](https://twitter.com/RobertHaisfield/status/1265306759356223493?s=20) it.\n\nPeople need an interface for convergence as well. [[P- Joel Chan]] writes about this as [incremental formalization](https://roambrain.com/knowledge-synthesis/). [[C- Incrementally processing notes is a key user behavior to promote synthesis|Incrementally processing notes is a key user behavior to promote synthesis]]. In doing so, they progressively compress large amounts of information into smaller amounts, which allows people to more easily compose ideas and define their relationships. Writing in hypertext allows people to have an incredible amount of information density. As Andy [likes to say in \"Evergreen Note Titles are like APIs\"](https://notes.andymatuschak.org/Evergreen_note_titles_are_like_APIs): \n\n \u003e When [Evergreen notes](https://notes.andymatuschak.org/z4SDCZQeRo4xFEQ8H4qrSqd68ucpgE6LU155C) are factored and titled well, those titles become an abstraction for the note itself. The entire note‚Äôs ideas can then be referenced using that handle (see [Concept handles, after Alexander](https://notes.andymatuschak.org/z5vA4vw86DKNq22xt6pRWhumeRmSzwV6hxRHE)). In fact, this property itself functions as a kind of litmus: as you develops ideas in notes over time and improve the ‚ÄúAPIs,‚Äù you‚Äôll be able to write individual notes which abstract over increasingly large subtrees (e.g. [Enacted experiences have incredible potential as a mass medium](https://notes.andymatuschak.org/z6oXuXLZ7Wq1eBqskyfph2wz9gjohQUKSBFzx), [Evergreen note-writing as fundamental unit of knowledge work](https://notes.andymatuschak.org/z3SjnvsB5aR2ddsycyXofbYR7fCxo7RmKW2be)).\n\nWhen the title of the note is able to substitute for the whole idea, it compresses a large amount of knowledge into a sentence, allowing knowledge workers to create a shorthand supported by autocomplete wikilinks. Through bidirectional linking, those wikilinks also create a collection of relevant material. \n\nHypertext structured in this manner is excellent for exploratory search. When people read a page they do not fully understand, hyperlinks will take them to the pages that explain component concepts, recursively, until they have learned what they need to know to grasp the original page. This style of search is efficient, because if people already understand a concept, they do not need to pursue the links further.\n\nWhen you have read much of a person's work in a well structured hypertext notebook, you can build a rich mental model of their thinking. I've read so much by [[P- Tom Critchlow]] that, when I read a 6 tweet long twitter thread, I am able to connect it to his [bigger worldview](https://twitter.com/RobertHaisfield/status/1418236841275183111) and even simulate conversations with him. This is because hypertext writing is information dense and refers to sets of ideas in consistent ways. Given that our goal is to combine knowledge at a large scale, information density is crucial.\n\nIf a key challenge of decentralized synthesis is bringing people up to speed, then hypertext is a promising direction.\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Incrementally-processing-notes-is-a-key-user-behavior-to-promote-synthesis":{"title":"C- Incrementally processing notes is a key user behavior to promote synthesis","content":"Authored By:: [[P- Rob Haisfield]], [[P- Brendan Langen]]\n\nThis claim is a response to [[Q- What workflows and behaviors facilitate synthesis]].\n\nThe thought behind this claim is that [[C- People are lazy]] and needing to process 6 months worth of research at once is more difficult than processing 1 month worth of information 6 times. As we use it here, the word processing refers to the act of going through old information and sorting it, either into new groups, by importance, or by stage in some pipeline. [[Q- What friction underlies the act of processing notes]]? \n\n[[C- Synthesis is supported by Active Reading]], and one method of active reading is the act of [progressive summarization](https://fortelabs.co/blog/progressive-summarization-a-practical-technique-for-designing-discoverable-notes/) (as coined by [[P- Tiago Forte]]). \n\nTools like [[Readwise|Readwise]], [[LiquidText]], and [[Hypothesis]] promote this act within their via annotations, highlighting, and focused views to gradually process notes. This is an area where [[C- It will be important to capture the potential energy of information consumption]], as other individuals can help process notes while they are reading.\n\nTools alone do not enable incremental processing, though. This has led the PKM community to reference routines and [algorithms for thought](https://www.cortexfutura.com/getting-started-algorithms-of-thought/) in order to turn ideas into synthesized knowledge. [[C- Multiplicity is necessary for synthesis]].\n\nMany people will [[C- Apply progressive summarization to your own notes|progressively summarize their own notes]]. [[C- People process complex information in multiple levels and stages of processing]]. \n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-It-will-be-important-to-capture-the-potential-energy-of-information-consumption":{"title":"C- It will be important to capture the potential energy of information consumption","content":"Authored By:: [[P- Rob Haisfield]]\n\n[[Q- How do people tacitly annotate information]]? [[C- People are lazy]], so if we can really leverage tacit annotation by enriching the choice architecture with options that add value, then we can perhaps fill in metadata and generate discourse. This could be interpreted as a response to the question: [[Q- How do we increase the frequency of social tagging behaviors]]. These people can verify information, agree with it, disagree, and connect it to other information. **We must leverage this- imagine if [[Readwise|Readwise]] was ubiquitous, people more frequently commented and tagged information, and people could reply to each other?**\n\n[[Q- What community roles are necessary in a decentralized knowledge graph]]? [[C- Most people will primarily consume information]], so they need something valuable to do. That valuable thing could be commenting on claims and building up RDF relationships between granular parts of the content on the page and other stuff. Social tagging, or social relationship description.\n\nWe want to capture the potential energy of information consumption because of problems associated with the question: [[Q- How can people maintain a decentralized discourse graph with a high quantity of information in it]]. It will be crucial to be able to improve the signal to noise ratio.\n\nPerhaps people annotating information could put stake behind their annotations and get rewarded if others believe their annotations to be valuable.\n\n[[C- Incrementally processing notes is a key user behavior to promote synthesis]], and capturing the potential energy latent in people's annotations will be crucial.\n\n[[C- People need a way of promoting and demoting knowledge in a decentralized knowledge graph]]. Specific sets of emoji reactions could facilitate this, as could [quadratic voting](https://twitter.com/metamitya/status/1248768114768084994?s=20) as implemented in [[Knovigator]]! If we can harness people's ability to react with commentary and allow that commentary to carry metadata, then the many eyes on it will result in highly queryable and malleable data structure. If it is structured in a way that's abstract enough to be recursive (e.g \"It's turtles all the way down\"), then that enhances composability. [[I- I wish I could freely add metadata to each block, so I could drag a slider indicating a belief score]] \n\nThe claims made in this page are different from [[C- There is a wealth of creative exhaust generated by researchers that is going to waste]] primarily in the sense that creative exhaust refers to content that is created that does not make it to publishing. In the titular page, we are making the claim that there is exhaust to capture from consumption as well.\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Linked-references-are-a-smart-default-for-related-items":{"title":"C- Linked references are a smart default for related items","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\nAssuming you're not using aliases, a plain text search in [[Obsidian]] for within quotations is functionally indistinguishable from the backlinks for that page. Linked references can then be thought of as a search for related items, where all the app knows is that backlinks have a high likelihood of being related because the user explicitly linked the items together.\n\nUnlinked references can be thought of as a search for . These have a lower likelihood of relatedness, as the connections were not explicitly made by the author while writing, but may represent latent connections in imported text or [[quick capture]] thoughts. Linking unlinked references is tool for [[structure in hindsight]].\n\nIf we [[I- Populate the related items section through a search term]], there needs to be a [[smart default]] term, as requiring the user to manually specify a search to represent related items for each page would be too much work. As wikilink matches are high signal, should be the default term. That will often be enough. However, if it isn't, the user should be able to adjust the search term to more accurately express what they consider to be related.\n\n[[P- Gordon Brander]] made a similar point, taking inspiration from [[Notational Velocity]]. As he phrases it in [Search reveals useful dimensions in latent idea space](https://subconscious.substack.com/p/search-reveals-useful-dimensions):\n\n\u003e **Any sufficiently advanced search is indistinguishable from a hyperlink**. A search query is kind of like a hyperlink that can be constructed on the fly. Our question forges a link between notes, just-in-time. When a search becomes extremely specific, it functions like a coordinate to a specific point in latent idea space.","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Making-editors-identifiable-incentivizes-high-quality-reviews":{"title":"C- Making editors identifiable incentivizes high quality reviews","content":"Authored By:: [[P- Brendan Langen]]\n\nIn [[R- Science as Pull Requests]], [[P- Cortex Futura]] paints a picture of an open source, engineering-focused academic review process of the future. \n\nPart of our challenge is solving, [[Q- How can people maintain a decentralized discourse graph with a high quantity of information in it]]? \n\nBy allowing [[C- Anonymous contributions to a decentralized discourse graph enable balanced review]], we enable for reviews to be unbiased. However, because we want our information to be valid, we also need to answer, [[Q- How can we incentivize generative contributions to a decentralized discourse graph]]? [[C- Curation is an important role in maintaining a decentralized discourse graph]], and staking rewards for editing is a step to ensuring high quality. \n\n(Rough idea here) [[I- Incentivize high quality review through token distribution]].\n\nImagine if editors received a token for their peer-reviewed contributions. [[Q- What would a Web3 Wikipedia look like]]. In a way, we are applying the same principles as Audius. [[C- The incentives of Audius explicitly value curation]]\n\n[[Q- How do we increase the frequency of social review]]? ","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Many-products-are-simply-specialized-interfaces-on-top-of-Excel":{"title":"C- Many products are simply specialized interfaces on top of Excel","content":"\nAuthored By:: \n\u003e 1.  **The Unbundling of Excel**. Hundreds of B2B startups have been built by taking a job currently being done in Excel and trying to accomplish the job in more optimized, purpose-built B2B software. Every time you hear an entrepreneur say, ‚ÄúWe‚Äôre replacing siloed spreadsheets and outdated processes with purpose-built software,‚Äù you‚Äôre hearing the Unbundling of Excel in real time. Many popular SaaS applications fall in this category. And yet, despite being ‚Äúunbundled,‚Äù Excel keeps getting stronger. [[R- Excel never dies]]\n\nThink about all of the CRM or task management applications that are glorified spreadsheets.","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Most-people-will-primarily-consume-information":{"title":"C- Most people will primarily consume information","content":"Authored By:: [[P- Brendan Langen]], [[P- Rob Haisfield]]\n\nStatistics over the years cite a [90-9-1](https://news.ycombinator.com/item?id=22622574) rule to contributors in online communities, and although that does not hold up across all communities, the majority of people fall into the consumption-only group (90%). Most site data is not publicly shared, but estimates range from 80-98% of people as lurkers, 1.9%-19% of people as editing/updating content, and .1-1% of people as primary contributors. \n\nLooked at differently, the 90-9-1 rule also indicates the breakdown of contributions from users. In this group, 90% of posts come from 1% of users, 10% of posts come from 9% of users, while no posts come from 90% of users. Our closest public comparison to a discourse graph is Wikipedia. [In 2006, 99.8%](https://www.nngroup.com/articles/participation-inequality/) of visitors were lurkers, 0.2% edited pages, and only .003% were contributors.\n\nThis speaks to a large gap in the activity we want to see in a discourse graph. [[C- It will be important to capture the potential energy of information consumption|It will be important to capture the potential energy of information consumption]]. Additionally, we need to [[Q- What workflows and behaviors facilitate synthesis|enable workflows and behaviors to facilitate synthesis]]. How can you lower the barriers for someone to meaningfully contribute?\n\n[[C- Synthesis is supported by Active Reading|Synthesis is supported by active reading]], and a number of tools assist with this. \n\n[[LiquidText]] is built on the claim that systems must enable people to trace excerpts back to their original context to support active reading, because [[C- Knowledge must be recontextualized to be usefully reused|knowledge must be recontextualized to be usefully reused]]. The tool allows you to \"pull out\" excerpts and make pointers to context, and use these units on a canvas to weave together a larger understanding, albeit in a less formal fashion.\n\n[[Hypothesis]] allows users to annotate webpages and documents in their margins, providing the option to further enrich a snippet with context. Sharing is built in to Hypothes.is, affording added context and active reading across groups. This enables social tagging, which helps users find related content and build community. [[C- Social tagging is a key user behavior to managing a decentralized knowledge graph|Social tagging is a key user behavior to managing a decentralized knowledge graph]]. This can come in the form of text, likes or [[C- Emoji reactions are a form of social tagging|emoji reactions]]. \n\n[[Readwise|Readwise]] offers a view towards actions a reader can take to save important notes and passages without considerable effort. \n[[P- Tiago Forte]] famously coined the term [\"progressive summarization,\"](https://fortelabs.co/blog/progressive-summarization-a-practical-technique-for-designing-discoverable-notes/) which is the behavior we are looking to develop in discourse graph communities - [[C- Incrementally processing notes is a key user behavior to promote synthesis|incrementally processing notes is a key user behavior to promote synthesis]]. Progressive summarization refers to consuming an information resource, taking the important parts out of it, rereading the important parts at a later date, summarizing the most important parts of that, and so on, until you only have the most important content.\n\nHow can we incentivize users to contribute? \nPerhaps [[C- Anonymous contributions to a decentralized discourse graph enable balanced review|anonymous author contributions]] combined with [[C- Making editors identifiable incentivizes high quality reviews|identifiable editors]] can increase the volume and quality of reviews while reducing bias. A helpful proxy to ask ourselves is [[Q- What would a Web3 Wikipedia look like|what would a Web3 Wikipedia look like]]?\n\n[[Q- How can people maintain a decentralized discourse graph with a high quantity of information in it|Our goal is to enable people to maintain a decentralized discourse graph with a high quantity of information as it grows over time]].\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Most-scholarly-communication-infrastructure-operates-on-the-document-as-the-base-unit":{"title":"C- Most scholarly communication infrastructure operates on the document as the base unit","content":"Even though we know that [[C- Scholarly argumentation operates on atomic statements and concepts as fundamental units]], our scholarly communication infrastructure doesn't make it easy for scholars to find and work with these units: far from it! In reality today, scholars spend a lot of time and effort \"breaking down\" more complex components like papers and books into the constituent building blocks they actually care about (heuristics, findings, claims, concepts, definitions).\n\nThis directly conflicts with the idea that [[C- Compression facilitates synthesis]], and might be one reason that effective synthesis is hard.","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Multiplicity-is-necessary-for-synthesis":{"title":"C- Multiplicity is necessary for synthesis","content":"","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Newsfeed-management-can-enable-users-to-express-their-preferences-through-a-combination-of-revealed-preferences-and-declared-preferences":{"title":"C- Newsfeed management can enable users to express their preferences through a combination of revealed preferences and declared preferences","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n[[C- Newsfeeds are an intervention to distribute an infinite quantity of recent information]]\n\nUsually, social media newsfeeds use your revealed preferences to construct an algorithm that will keep you looking for longer. It might do this by looking at what you like and comment on, or by what you look at for extended periods of time. However, by solely interpreting revealed preferences, you can end up with noisy algorithms or showing the user information that isn't in their interests. For example, I may look at a tweetstorm that enrages me, but I don't want to see more of that. Or I might click on a trending topic to realize what it's about but then realize that it's not of interest, but the algorithm just knows I clicked on it. My behavioral signal is being misinterpreted.\n\nIdeally, there is some way to also express declared preferences. For example, as I'm going through a newsfeed, I can click on a tweet and say \"see less like this.\" Or I can use the tools that the algorithm has for revealing my preferences intentionally.  [[C- User behavior within a well-designed choice architecture can be a signal of preferences]].\n\nFor more on this subject, see [here](https://www.linkedin.com/pulse/how-do-you-make-users-happy-behavior-what-social-media-rob-haisfield/?trackingId=GdEOMIOmOS6w%2BpM8rfBnTg%3D%3D)","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Newsfeeds-are-a-poor-intervention-for-distributing-and-discovering-relevant-information":{"title":"C- Newsfeeds are a poor intervention for distributing and discovering relevant information","content":"Authored By:: [[P- Rob Haisfield]]\n\n[[Q- What are powerful interfaces for entering information into a discourse graph]]\n\nNewsfeeds are an intervention for displaying a never ending stream of information to social media users. \n\nNewsfeeds show you primarily what is new, which is not ideal when considering a discourse graph to facilitate synthesis. We want people to be able to bring together information from the past and surface it precisely when it is relevant.\n\nIn their attempt to show you only what's relevant to your interests, newsfeeds generally overindex their algorithms on revealed preferences. They show you more of what you pay attention to, but this may not always reflect people's true intentions for what they want to consume, especially for their stated research goals. There are meaningful directions to explore in creating [a better choice architecture for newsfeed management](https://twitter.com/RobertHaisfield/status/1329828499167711232).","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-People-are-lazy":{"title":"C- People are lazy","content":"Authored By:: [[P- Rob Haisfield]]\n\nCitation:: [[R- Metacrap- Putting the torch to seven straw-men of the meta-utopia]]\n\n[[C- The worst data structure for a collaborative decentralized knowledge graph would be individual people tagging things according to a preset ontology]]\n\nIf a system is reliant on each person tagging things appropriately, then it is brittle. Crowdsourcing may be a solution: [[C- Social tagging is a key user behavior to managing a decentralized knowledge graph]].\n\n[[R- Metacrap- Putting the torch to seven straw-men of the meta-utopia]], see section 2.2","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-People-naturally-try-to-enact-typed-distinctions-in-their-notes":{"title":"C- People naturally try to enact typed distinctions in their notes","content":"Authored by:: [[P- Joel Chan]]\n\n- Let's start with articulating some desire paths we've observed. I'll focus on Roam first because this is what the [[Discourse Graph Plugin]] hooks into, but we can also generalize to other settings.\n    - In [[Roam Research]]\n        - [[@ Maggie Appleton]]'s evergreen note system: https://maggieappleton.com/roam-garden\n            - Loom\n                - https://www.loom.com/share/8a17c7277f0442b8963e72a0475171a8?t=681\n            - Maggie prefixes evergreen note titles with * to enable her to Efficiently retrieve high-signal conceptual building blocks for synthesis and writing\n            - Some notes are tagged in-line as \"seedlings\" to guide attention (needs more work). These \"seedlings\" are integrated with a \"writing inbox\" practice adapted from Andy. This enables her to Incrementally develop ideas over time\n            - She enhances this by applying CSS styles to enable visual distinction. This is a very common practice in the Roam (and larger hypertext notebook community; see, e.g., [[Obsidian]] supercharged links plugin: https://github.com/mdelobelle/obsidian_supercharged_links): https://maggieappleton.com/paintingroam\n            - Note: Maggie's [[Zettelkasten]] system is a synthesis with [[Evergreen Notes]]\n            - https://youtu.be/RXXXHN516qc\n        - [[@ Lisa Hardy]]'s hyperfine village\n            - Tweet\n                - Lisa Hardy on Twitter: \"OK, I made my \"Hyperfine Village\" into a public @RoamResearch database. Come take a look around, #roamcult! https://t.co/ukfvKbc8aW\" / Twitter\n                    - https://twitter.com/hardy_lisa_a/status/1293625272764264449\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fj4TySbg_kn.png?alt=media\u0026token=43f58b3c-5e70-4fc5-958d-e3d2f889d343)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FaKBsQwbopq.png?alt=media\u0026token=aa09ac2a-27cf-49e0-bbd8-0cdab135db0a)\n            - https://roamresearch.com/#/app/hyperfinelabs/page/zNhX395Zu\n        - [[@ Norman Chella]]'s thoughtmine: https://thatsthenorm.com/thoughtmine/\n            - It's tuned to enable Norm to Incrementally develop ideas over time\n        - [[@ Shu Omi]]'s [[Zettelkasten]] system: \n            - https://youtu.be/KmGrK1YxUPM\n        - [[@ Beau Haan]]'s Zettelkasten\n            - https://www.youtube.com/watch?v=KoddCmn3eL0\n        - [[@ Maarten van Doorn]]'s system\n            - https://roambrain.com/the-complete-guide-to-effective-note-taking/\n        - [[@ Lisa-Marie Cabrelli]]'s \"realization notes\" and similar\n            - influenced by \n        - [[@ Maggie Delano]]'s \"hive mind\" for [[syntopical reading]]  https://www.maggiedelano.com/garden/syntopical-reading-in-roam\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FdbQur6sg1i.png?alt=media\u0026token=a7ec75e8-31d2-4da4-994b-465a372ee73d)\n    - Outside Roam\n        - [[Obsidian]] supercharged links plugin: https://github.com/mdelobelle/obsidian_supercharged_links\n        - [[@ Andy Matuschak]]'s [[sys/Evergreen Notes]], taxonomy of notes, and writing inbox practice\n            - https://notes.andymatuschak.org/Taxonomy_of_note_types\n            - Writing inbox practice: \n                - https://notes.andymatuschak.org/z5aJUJcSbxuQxzHr2YvaY4cX5TuvLQT7r27Dz\n        - [[@ Nick Milo]] distinguishing \"Maps of Content\" from evergreen notes\n        - [[@ Eleanor Konik]]\n- Observations\n    - Typing unlocks affordances that people find useful for their knowledge work. A shortlist includes the following:\n        - Incrementally develop ideas over time\n        - Efficiently retrieve high-signal conceptual building blocks for synthesis and writing\n        - Trace provenance of ideas to high-signal types of sources, including one's own thought context, and references/literature\n    - There are pretty good workarounds and techniques for specifying types of nodes. \n    - There is significant diversity in the specific language used for the terms, including wholesale differences in metaphors\n    -  Specifying types of relationships remains clunky. \n        - This is where the d/graph extension enhances things.\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-People-need-a-way-of-promoting-and-demoting-knowledge-in-a-decentralized-knowledge-graph":{"title":"C- People need a way of promoting and demoting knowledge in a decentralized knowledge graph","content":"##### Work in Progress\n#WIP\n\nThis is because of [[Q- What changes in a discourse graph as quantity of content increases|a need to reduce the overall amount of quantity of content being consumed]].\n\n[[C- Highlighted and lowlighted search results map to how well results map to intentions]]","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-People-process-complex-information-in-multiple-levels-and-stages-of-processing":{"title":"C- People process complex information in multiple levels and stages of processing","content":"Authored By:: [[P- Joel Chan]] [[P- Rob Haisfield]]\n\nWhen we're exposed to new information, we have to ask questions and learn more before we feel confident in our understanding. Knowledge evolves over time, and we adapt our thoughts as we see other perspectives. This is the [sensemaking](sensemaking.md) process. This is our introduction to something new, and we know the least we ever will about the topic at this time. \n\nSimilarly, in the moment a note is created, it is very difficult to know where it __belongs__. We may not yet have a name for what we're seeing or know how it relates to another insight. Our understanding will likely evolve after reading other sources. On later review, we may even find a unique detail of the wider point to be the most interesting. It's hard to know everything up front! \n\n[[C- Sensemaking requires iterative loops of (re)interpreting data in light of evolving schemas]]. The primary models that represent this process are the [Learning Loop Complex](Learning%20Loop%20Complex.md) and the [Notional Model of Sensemaking](Notional%20Model%20of%20Sensemaking.md), which loops between and within foraging and sensemaking loops, progressively increasing in structure and effort, starting from raw data sources and culminating in a synthesized set of hypotheses. \n\nIn our interviews, we saw multiple people talk about their process of [[conversations, chats with others]] to crystallize new information, like [R16](R16) in their synthesis process. Sometimes this can be a frame, or a prompt, that the person presented to themselves, like [R10](R10) writing multiple versions in order to speak fluently about the topic. \n\nThis all speaks to the need to treat notes as a work in progress. Notes aren't in their final form when first written; they evolve, often far beyond the stub they began as. Translating to tool design, [[C- There needs to be an excellent workflow for refactoring in a tool for thought]] to enable [[incremental formalization]].\n\nIn the scope of a [[Thought processor]], we see different approaches. [[Roam Research]] encourages users to write directly into a daily notes page, linking to pages as they write. This contrasted conventional practice in tools like Evernote that ask the user to name the page and place it in a folder up front. Zettelkasten systems, which are agnostic to a specific tool, ask users to create a note without names and later fit it in next to similar notes.\n\nIn the process of synthesis, we often write a number of notes about an idea - some useful, some not. We know that [[C- Multiplicity is necessary for synthesis]], and this writing process helps, but it creates a huge amount of noise for us to later search through. We have a need for [[Archiving]].\n\nForgetting is powerful. If something is no longer important, its presence in your memory is a burden, turning your \"search results\" into a noisy mess that decreases your likelihood of finding the right answer.\n\nMany people use personal knowledge management software to augment their memories. \"Your mind is for having ideas, not holding them\" as stated in the infamous productivity text \"Getting Things Done.\" We saw this again and again in our interviews. Expert synthesizers use thought processors to ensure good ideas don't fall through the cracks. Personally, I use thought processors in conjunction with personal knowledge management in order to ensure the lessons I learn today retain their usefulness in 10 years. If at that point my questions and ideas are dependent on recent memory rather than a decade of learning, I've failed.\n\nHowever, over the course of 1 year using Obsidian, I wrote more than a million words. Maybe 30% of that was from educational materials. The rest was \"thinking out loud\" in order to process thoughts. Eventually, many of those thoughts would be processed into an information artifact. Those artifacts became my canonical point of reference.\n\nWhile this multiplicity is generally useful during the process of thinking through an issue, later down the line reviewing it contributes to the feeling of [[wasted repeated effort]]. Early versions of information in the final artifact can likely be forgotten without much negative consequence. \n\nAdditionally, over the course of a decade, I'll certainly update my beliefs and mental models. If that happens, I'll have to ask myself, \"Is it important to keep a record of my beliefs at that point in time, or is leaving it in my database weighing it down?\"\n\nI don't know how software can automatically archive this information without making users mad at some point or another. This is a hard problem, but it should be clear that [[C- Bulk refactors are a necessary primitive to maintaining a decentralized discourse graph]].\n\nAt the extreme end, Venkatesh Rao jokingly referred to an archetype of user as [\"mind-palace pack rats\"](https://twitter.com/vgr/status/1370583027512872961). These are the people who don't know how to delete information and want to either memorize or save all information they encounter. I occasionally fall into this trap. [[C- Predicting trajectories of future reuse of information objects is hard]], and information hoarders may believe that everything will be reused at some point. Additionally, they may believe that it's better to have information and not need it than need it and not have it.\n\nWhether or not either of these beliefs turn out to be true, the hoarders will produce a massive amount of information over the course of 10 years. \n\nIf the first belief is false and they never delete or archive information, then this means they will have too much information to search through in 10 years, turning their personal database into a noisy mess with far too much to sift through for efficient reuse. Whether or not the second belief is true, it must be weighed against the immense cost of bloating your knowledge graph.\n\nThe problem is that over the course of 10 years, I'll generate so much content that many of it will simply appear to be noise. Not everything you believe to be important today will be important then. Most likely, you will only want the most important artifacts due to time constraints.\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Predicting-trajectories-of-future-reuse-of-information-objects-is-hard":{"title":"C- Predicting trajectories of future reuse of information objects is hard","content":"\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Reviewing-past-notes-in-the-process-of-creating-new-notes-is-a-key-user-behavior-to-promote-synthesis":{"title":"C- Reviewing past notes in the process of creating new notes is a key user behavior to promote synthesis","content":"From [[P- Gordon Brander]]'s [newsletter](https://subconscious.substack.com/p/knowledge-gardening-is-recursive?utm_source=url):\n\n\u003e The core game mechanic of Zettelkasten is to file your note some place where you would want to stumble over it again. As you rifle through old notes to find this place, you¬†__recurse over__¬†notes you had forgotten about, sparking new ideas, which you then write down, and have to file, causing you to rifle through again, sparking more ideas‚Ä¶ in a cascade of idea generation.¬†**Zettelkasten is a feedback system.**\n\u003e Notational Velocity was a tiny note-taking tool built around a single idea:¬†**[[search-or-create]]**.\n\u003e The only way to create a new note in Notational Velocity was through the search bar. Typing would live-searched through your notes. Hitting enter would create a new note, with the contents of the search query as the title for the note.\n\u003e Like Zettelkasten‚Äôs game mechanic,¬†**this search-or-create mechanic closes a feedback loop**. Every time you enter a new idea, you‚Äôre¬†__recursing__¬†over old ideas. Often you find yourself editing, or refactoring an old note, instead of creating a new note. Over time, these microinteractions add up, generating knowledge from the bottom-up.\n\nWe can see this used in [[Roam Research]] and [[Obsidian]] with wikilink autocomplete-or-create, combining the process of linking to a page with reviewing the pages you've already created. [[Threadhelper]] takes this a step further - as you are writing a tweet, it searches through your history to find tweets that may be relevant based on keywords from within the tweet you're writing.\n\n### Misc\n\n[[Q- How do we increase the frequency of social review]]? \n\n[[Q- What are powerful primitives for a user of a decentralized knowledge graph]]? How do we create primitives and feedback loops that encourage reviewing past notes in the process of creating new notes?\n\nReviewing old notes helps provide another lens towards the concept, as well. [[C- Multiplicity is necessary for synthesis]].","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Scholarly-argumentation-operates-on-atomic-statements-and-concepts-as-fundamental-units":{"title":"C- Scholarly argumentation operates on atomic statements and concepts as fundamental units","content":"Ask [[P- Joel Chan]] lol I'll add a link to his website as soon as he publishes, as his discourse graph goes deep. The page in his graph has so many references, none of these links are going to work, but if you're interested in reading more you can look at his Zotero cite keys and that will help you with Google Scholar searches.\n\n- [[üå≤ zettels]]\n    - Tags: #[[D/Synthesis Infrastructure]]\n    - Description:\n        - Philosophical accounts of synthesis in science emphasize that it is a form of [[argumentation]], which operates on statements and concepts as fundamental units\n        - [[@harsDesigningScientificKnowledge2001]] synthesized four major objectivist epistemological models of scientific knowledge from [[Philosophy of Science]], including Popper [[@popperLogicScientificDiscovery1959]], Nagel [[@nagelStructureScienceProblems1979]], Dubin [[@dubinTheoryBuilding1978]], and Bunge [[@bungePhilosophyScience1998]], and demonstrated that they all model argumentation in terms of scientists building systems of scientific statements, which are composed of relationships between concepts.\n        - We see this also in the way that [[[[CLM]] - Scientists read strategically, not linearly]] specifically for these components. This might be an artifact of the current structure of the [[[[scholarly communication]] [[infrastructure]]]] though, since [[[[CLM]] - Most scholarly communication infrastructure operates on the document as the base unit]]\n        - Note: we need to work out whether [[Interpretivist]] epistemologies will have different implications; my initial instinct is no; the structure won't change that much (although [[context]] might become far more important)\n    - A number of prominent models of alt. scholarly comm infrastructures for search and (sometimes machine, but also human) sensemaking of scholarly argumentation\n        - Overall vision is well cast by [[@deWaardProteinsFairytalesDirections2010]] and [[@liddoContestedCollectiveIntelligence2012]] to emphasize that we need discourse, not just knowledge graphs, but definitely not just papers!\n        - Thread of work on claims/statements as primary unit\n            - Claims in HCI [[@mccrickardMakingClaimsKnowledge2012]]\n            - [[std/Nanopublications]]  - statements and their provenance as primary unit - [[@grothAnatomyNanopublication2010]]\n            - [[std/SWAN]] - [[@ciccareseSWANBiomedicalDiscourse2008]] [[@clarkAlzforumSWANPresent2007]]\n            - [[Scholarly Ontologies Project]] and related work from [[Knowledge Media Institute]] focusing on claims/statements\n                - [[@urenSensemakingToolsUnderstanding2006]]\n                - [[@shumScholOntoOntologybasedDigital2000]]\n                - [[@shumModelingNaturalisticArgumentation2006]]\n                - [[@liddoContestedCollectiveIntelligence2012]]\n        - Thread of work pushing forward statements to emphasize distinction between claims and evidence\n            - [[std/Micropublication]]s for claims and \"data\" (evidence) - [[@clarkMicropublicationsSemanticModel2014]] \n            - [[std/HypER]] for hypotheses and evidence [[@dewaardHypothesesEvidenceRelationships2009]]\n            - [[std/SEPIO]] for assertions and evidence [[@brushSEPIOSemanticModel2016]]\n    - [[@harsDesigningScientificKnowledge2001]] articulates some good {{alias: [[example-of]] examples of}} the kinds of queries that are both crucial for [[synthesis]] and functionally impossible to answer in our current scholarly communication infrastructure, such as \"What are unsolved problems in domain X? What solutions have been proposed for problem X? What extensions have been proposed? What theories incorporate concept X? What are alternative interpretations of concept X\"? \n    - R-Sources:\n        - Statements were a core building block of a model of scientific knowledge synthesized from four [[Positivist]] epistemological models of scientific knowledge from philosophy of science [[@popperLogicScientificDiscovery1959]] [[@nagelStructureScienceProblems1979]] [[@dubinTheoryBuilding1978]] [[@bungePhilosophyScience1998]]   #Atomicity [[@harsDesigningScientificKnowledge2001]]\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FBKdsNedWQB?alt=media\u0026token=8d75e1d8-bd58-439f-a232-a25ebf651691) (p. 70)\n        - Concepts are a core building block of a model of scientific knowledge (p. 70) #[[üìù lit-notes]] #Atomicity [[@harsDesigningScientificKnowledge2001]]\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FJYjCmS2ftQ?alt=media\u0026token=3752423e-86e7-4c31-931e-498bbd54b14d)\n        - Four [[Positivist]] epistemological models of scientific knowledge from philosophy of science [[@popperLogicScientificDiscovery1959]] [[@nagelStructureScienceProblems1979]] [[@dubinTheoryBuilding1978]] [[@bungePhilosophyScience1998]] agree that scientists build __theories__ as systems of scientific __statements__, which are composed of relationships between __concepts__ (p. 70) #[[üìù lit-notes]] #Atomicity #compositionality [[@harsDesigningScientificKnowledge2001]]\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FiQqwB1-Ial?alt=media\u0026token=9a46a986-ca6d-4ba8-85f8-f86aab6bf660)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FME-fGcQ9mR?alt=media\u0026token=5881b8ea-a80e-4c5d-a7b0-4099ceafff13)\n        - [[@shumScholOntoOntologybasedDigital2000]]\n    - Todos:\n        - [[Joel Chan]] #TaskWrite Bring this stuff into conversation with the [[UIUC]] model of data model stuff\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Search-terms-express-intentions":{"title":"C- Search terms express intentions","content":"Authored By:: [[P- Rob Haisfield]]\n\nThis is a big claim in [[R- Everybody Lies]], and is how the author justifies using google search data in order to understand people better.\n\nWhen users have advanced search options, they are able to specify what they mean. Perhaps when they are searching for PCT, they really mean they are searching for perceptual control theory. A search with an OR clause could communicate that (e.g. `OR: \"PCT\" \"Perecptual Control Theory\"`. Then if those searches maintain their permanence, the system could know that I always mean perceptual control theory in that situation. [[C- Tools for thought are popular when it feels like they get to know you better over time]]\n\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Social-tagging-is-a-key-user-behavior-to-managing-a-decentralized-knowledge-graph":{"title":"C- Social tagging is a key user behavior to managing a decentralized knowledge graph","content":"This is about diffusing the effort. In [[R- Information Foraging Video]], they present some research on social tagging. \n\nThis supports answering the question of [[Q- What user behavior is required to maintain a decentralized knowledge graph]]","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Specifying-context-for-future-reuse-is-costly":{"title":"C- Specifying context for future reuse is costly","content":"","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Synthesis-as-a-process-is-usefully-modeled-as-a-specialized-form-of-sensemaking":{"title":"C- Synthesis as a process is usefully modeled as a specialized form of sensemaking","content":"Authored By:: [[P- Joel Chan]]\n\nWhile the most common manifestation of synthesis is an academic literature review, the underlying cognitive processes are not so different from what people do all the time: **[[sensemaking]]**. \n\nSensemaking as a model of a core cognitive process has a long history in cognitive and information science. We draw specifically on a family of models that describe sensemaking as an iterative search for representations of input data that can make some task easier ([[R- Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking]], [[R- The Cost Structure of Sensemaking]], [[R- The sensemaking process and leverage points for analyst technology]]). Examples include categorization schemes and comparison tables to facilitate decision-making about what camera to buy, a map of hypotheses for intelligence analysts, and an effective literature review of an area of research.\n\nOne crucial insight from models of sensemaking informs our research on synthesis: sensemaking involves **iterative loops** between [[Search Behavior|searching]] (for data, useful representations) and analysis/composition (classifying data, composing data into representations). This is illustrated well in the following two figures from  following figure from Russell et al's [[Learning Loop Complex]] and Pirolli et al's [[Notional Model of Sensemaking]]:\n\n![[Fig - Learning Loop Complex.png]]\n![[Fig - Notional Model of Sensemaking.png]]\n\nOne implication of this insight are that [[I- Search as a part of the primitive design|search is a fundamental primitive of synthesis workflows]] because of the key involvement of search in this process model. Another implication is that [[C- Synthesis tools need to support incremental formalization]] to accomodate constant changes to the representation that are common with open-ended exploratory sensemaking like intelligence analysis (in contrast to well-trodden sensemaking tasks like deciding what camera to buy).\n\nWhile sensemaking is a reasonable general model of the kind of work that is needed to do synthesis, there are likely to be key differences between synthesis and sensemaking, and other everyday tasks of sensemaking, due to the nature of the inputs (complex research papers, theories, findings) and the nature of the output (a boundary-pushing novel conceptual whole with no obvious precedent). These differences are likely to be consequential for understanding precisely what data structures and user interactions can optimize sensemaking for synthesis, since the process and representational requirements of sensemaking (e.g., whether the representation that is constructed has the shape of a map or timeline or argument, how much iteration and search for representations must happen) vary as a function of the task. Thus, our project tackles core research questions about the [[Q- What workflows and behaviors facilitate synthesis | workflows]] and [[Q- What is the data structure of a graph built to facilitate decentralized knowledge synthesis | data structures]] that optimize synthesis, and how to build on this for decentralized synthesis. Some hypotheses include the importance of [[I- Search as a part of the primitive design|search as a primitive]], and the importance of design patterns like [[incremental formalization]] and [[flexible compression]].\n\nUnderstanding how people perform synthesis is one of our open questions going into [[Interview Guide|user interviews]]. \n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Synthesis-is-hard-to-do-with-people-who-dont-share-context-with-you":{"title":"C- Synthesis is hard to do with people who don‚Äôt share context with you","content":"\nAuthored By::  [[P- Joel Chan]], [[P- Rob Haisfield]], [[P- Brendan Langen]]\n\nAn exciting hypothesis that motivates our work is that making discourse graphs widely available could accelerate human synthesis work, and thereby accelerate innovation and scientific discovery. [[C- Effective synthesis is necessary for innovation and scientific progress|C- Effective synthesis is necessary for innovation and scientific progress]]. Yet, our first challenge is to enable people to perform distributed synthesis. For many people, this is a challenge!\n\n## Synthesis typically occurs in one mind\n\nSynthesis involves individuals putting together many pieces in their minds, but It is often argued that [synthesis occurs in a single mind](https://notes.andymatuschak.org/Great_creative_work_is_usually_the_product_of_a_single_person). In our research, surprisingly few participants had distributed synthesis experiences to share. Why? \n\nIn the early stages of synthesis, when people have [[half-baked ideas not ready yet]], they prefer to work on their own or with a close companion. \n\nFor most people, the primary concern is speed. If you are working with someone who has no shared background, you need to spend time explaining the basics - of the topic, your goals, and your working process - to them. \n\nAs [[P- Ryan Singer]] describes in [Shape Up](https://basecamp.com/shapeup), the most efficient synthesis occurs when you can cover a breadth of topics quickly. This can only happen if you are able to leave out details as you go. When you are on your own or with a trusted collaborator with shared context, you do not need to re-explain things you already understand, using a \"shorthand\" instead.\n\nFirst, we need to have the right people‚Äîor nobody‚Äîin the room. Either we‚Äôre working alone or with a trusted partner who can keep pace with us. Someone we can speak with in shorthand, who has the same background knowledge, and who we can be frank with as we jump between ideas.\n\nHere, the primary concern is being able to [[work at the speed of thought]]. Bringing someone else up to speed - especially someone with a [[lack of shared context]] - slows the process down and reduces the chances of advancing beyond the basics.  \n\nThis lack of shared context makes it difficult to divide the work of synthesis collaboratively. \n- [[B1]] gave an example from his own experience where he was working on a knowledge graph project with people who did not have a background in knowledge graphs. They had 10 sources and divided them up so each person would look at different sources and take notes on them. When [[B1]] read his sources, he took notes in the context of what was relevant to the project, whereas his collaborators would write summaries of the entire sources. They did not fully understand the driving questions and history of the space, so they had no filter for relevance.\n- [[R2]] mentioned that when he reads a paper, he only takes notes on what is relevant for his current projects. While he does have rigorous processes for taking notes on a particularly juicy paper, he finds that often [[C- It is difficult to predict whether structure now will be worthwhile later]].Thus, we see that **shared context** is critical to distributed synthesis. When multiple people share context, they can advance their collective understanding of the topic through conversation and standardized processes and conventions\n\n## Shared context is critical to distributed synthesis\n\nThus, we see that **shared context** is critical to distributed synthesis. When multiple people share context, they can advance their collective understanding of the topic through conversation and standardized processes and conventions. \n\nTo have shared context, people must be on the same page. Their working styles align. They understand how what they're examining should fit into their overall goal. They are aware of the nuance in the field(s) they're researching. In simple terms, they know how to work with one another on the problem they're working on.   \n\nBut there is a very significant barrier to team-based convergence because of this bottleneck of not being able to have a shared \"dataset\" of ideas that satisfy the properties. Our external representations that are **shareable** lack one or more of those properties. \n\nWhen we did see distributed synthesis in our research, people would almost always bring someone else into their work through a feedback role, as that lowers the burden of the shared dataset requirement. \n\nInstead of involving peers in the full synthesis process, [[B5]] would ask concise questions to his peers that would support him  [[B5]] and [[R10]] seek feedback only from trusted individuals. TA comes up with a pitch, bringing people along his line of questioning. He wants to present others with a gateway into the topic he's exploring. \n\nOftentimes, his solution to get people up to speed is a **gateway analogy,** where he presents his idea through the lens of one his audience might understand.\n\nIn our interviews, virtually everyone struggled to bring other people up to speed on their projects, but the theme of a **collective narrative** emerged as a pattern of people who are on the same page.\n\nAt this point, the synthesizer would either find an individual with shared context, or they would need to come up with a concise framing to act as gateway analogies and simple entry points to share. \n\nTo find those entry points, [[B5]] considers the following questions: What's a common way to frame this that others can relate to? How can I get someone else to think this is their own idea? What will help someone else get it? What motivated the project?\n\nOthers talked about conversations as a framing mechanism to drive the process of synthesis.  [[R16]] uses [[conversations, chats with others]] as her primary form of synthesis. She finds the form of chat to be more conversational and feels its easier to solve a problem with an audience in mind or if you're talking to someone.  For her, the very act of framing the thoughts so her partner could understand clarifies her thinking.\n\n## Find or create shared context\nThis suggests two categories of solutions to enable synthesis with others:\n1. **Connect people who share context.** This remains an area for further research, but obvious solutions include finding other people who write about similar concepts or enabling the formation of communities and querying for overlap between people.\n2. **Design mechanisms to help people gain context quickly.** In order to do this, we would need to facilitate knowledge transfer from experts to beginners. This is a difficult problem. As described earlier, experts prefer to work on their own or with people who already share context **because** their ability to move fast depends on omitting the basics. Solutions to this problem will involve **lowering the barrier to add structure to notes.** \n\nA promising direction would be through enhanced composability, see [[C- Hypertext enables communication with high information density]] for more detail.\n\nOn the more radical end, graph queries could show readers X number of paths to traverse between their current understanding and a page that they want to read. It could infer what people already know through a variety of mechanisms - what they have read already, what they've written, what they have commented on.\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Synthesis-is-supported-by-Active-Reading":{"title":"C- Synthesis is supported by Active Reading","content":"##### Work in Progress\n#WIP\n\nActive reading is quite old and established/mature, spans the Education literature as well as [HCI].\n        - [active reading] is also another thing to add to the claim that [[C- People process complex information in multiple levels and stages of processing]]\n    - R-Sources\n        - [[R- LiquidText A Flexible Multitouch Environment to Support Active Reading]] is a decent entry point into the [active reading] literature\n            - the [[LiquidText]] system in particular is nicely grounded in [[R- Active reading and its discontents]], which is a nice formative study of active reading with interviews and [Participatory Design] workshops\n        - another decent one is [[R- Beyond paper supporting active reading with free form digital ink annotations]], which cites [[R- A diary study of work-related reading design implications for digital reading devices]] as a #canonical ref for [active reading]\n\n\n[[C- Curation is an important role in maintaining a decentralized discourse graph]]. [[C- It will be important to capture the potential energy of information consumption]].\n\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Synthesis-tools-need-to-support-incremental-formalization":{"title":"C- Synthesis tools need to support incremental formalization","content":"Authored By:: [[P- Joel Chan]]\n\nFormal structure unlocks the most sophisticated levels of synthesis we want, and is especially helpful for enabling computational systems to provide support (e.g., indexing, searching).\n\nUnfortunately, formal structure is also **very tedious** to specify, especially in the exploratory, open-ended domains where synthesis is most valuable. It's devilishly tricky to make systems that support formalism that users will actually use (without a gun to their head).\n\nTools for synthesis that attempt to incorporate formalism typically have an extremely high barrier to entry *and* ongoing burden on the user. This overhead is exceedingly difficult to overcome for any kind of ongoing work.\n\nThese problems are well-documented in the classic \"Formality Considered Harmful: Experiences, Emerging Themes, and Directions on the Use of Formal Representations in Interactive Systems\" [[R- Formality Considered Harmful]]. In brief, this paper identifies four classes of difficulties:\n1. **Cognitive overhead**: often the task of specifying formalism is extraneous to the primary task, or is just plain annoying to do\n2. **Tacit knowledge**: if relevant info for developing formalism is tacit, asking people to formalize it will interrupt the task, with serious consequences for the quality of the work\n3. **Enforcing Premature Structure**: people don't want to commit until they're sure what formalism is actually useful for their task (and what's extraneous and only annoying) \n4.  **Situational Structure**: Useful structures and formalisms vary significantly across people, situations, and tasks\n\nThe last two (premature and situational structure) are particularly relevant for open-ended synthesis work.\n\nOne powerful general design pattern for overcoming these risks of formality is [[incremental formalization]]. The basic intuition is described well by [[R- Formality Considered Harmful]]: users enter information in a mostly informal fashion, and then incrementally formalize later in the task as appropriate formalisms become more clear and also (more) immediately useful. \n\n[[R- Formality Considered Harmful]] notes a few example systems that help flesh out the concept (these are all older systems, mostly research systems, so unfortunately they're not available to play with):\n    1.  In the Hyper-Object Substrate system, users enter mostly informal text initially, and the system recognizes patterns in the textual information to suggest possible formal attributes or relations for the underlying knowledge base, which the user can then accept/modify/reject as they wish (p. 347).\n    2.  Infoscope is a news reader system that suggests filters based on users' reading patterns; this helps them make their goals explicit which can facilitate formalization after it emerges from their task behaviors (p. 347-348)\n    3.  VIKI is a spatial hypertext system that includes heuristic algorithms to find recurring visual/spatial patterns in layout of objects; users can use these to specify schemas if they wish\n\nAnother more recent example comes from [[R- Gui ‚Äî Phooey]]: their Jourknow system includes a variety of features that can recognize formal structure (e.g., location, time, meeting information) from (relatively) unstructured notes in pidgin or more lightweight entry format, such as [[Notation3]] (p. 195-197)\n\nOther examples of this in production systems include Todoist recognizing keyphrases like \"today\" to add formal date information to todos, or Gmail recognizing potential formal event data from an email when you create a calendar event while an email is open.\n\nIncremental formalization addresses the cognitive overhead problem by spreading it throughout the task a bit more evenly, as well as removing it mostly from the earlier parts of the task, where minimal friction is needed to maximize exploration. It also helps with the premature and situational structure problems, since you don't have to commit early on to a structure that may not serve you well (or even hurt your performance) later on.\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-The-responsibilities-required-to-produce-synthesis-can-be-split-up-among-many-people":{"title":"C- The responsibilities required to produce synthesis can be split up among many people","content":"Authored By:: [[P- Rob Haisfield]], [[P- Brendan Langen]]\n\n[[Q- What synthesis behaviors must be done by an individual and what responsibilities can be distributed to many people]]?\n\n[[Q- What user behavior is required to grow a decentralized discourse graph|As a discourse graph grows]], the process of maintenance becomes more difficult. Synthesis also needs to occur in an ongoing manner. \n\nHere, we can call on prior art like [[Hypothesis]] [[LiquidText]] and [[Readwise|Readwise]] as examples of tools that enable [[C- Synthesis is supported by Active Reading|active reading]]. \n\n[[C- Social tagging is a key user behavior to managing a decentralized knowledge graph]]","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-There-is-a-trend-of-platforms-that-support-user-generated-content-winning":{"title":"C- There is a trend of platforms that support user-generated content winning","content":"[[Q- What user behavior is required to maintain a decentralized knowledge graph]]\n\n[[Q- How might we create a decentralized knowledge graph that people want to use]]\n\n[[Q- How do explorer communities grow around software tools]]\n\n[[Q- How do explorer communities attract a hacker community]]? Hacker communities can really expand the use cases of an app. [[C- End-user programming enables the developers to be lazy about their backlog of feature requests]]","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-There-is-a-wealth-of-creative-exhaust-generated-by-researchers-that-is-going-to-waste":{"title":"C- There is a wealth of creative exhaust generated by researchers that is going to waste","content":"","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-There-needs-to-be-an-excellent-workflow-for-refactoring-in-a-tool-for-thought":{"title":"C- There needs to be an excellent workflow for refactoring in a tool for thought","content":"","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-User-behavior-within-a-well-designed-choice-architecture-can-be-a-signal-of-preferences":{"title":"C- User behavior within a well-designed choice architecture can be a signal of preferences","content":"\n\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/C-Wikum-allows-you-to-summarize-groups-of-comments-on-a-Hacker-News-style-forum":{"title":"C- Wikum allows you to summarize groups of comments on a Hacker News style forum","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n[[Wikum|Wikum]] is a tool for recursive summarization in internet forums, with an aim to help readers see a summary tree of the main points of discussion. \n\nThis was first published by [[Amy Zhang]], [[Lea Verou]], and [[David Karger]] at CSCW 2017. \n\nThis paper has made significant traction in the para-academic HCI / tools for thought community. \n\nExample: [[P- Max Krieger]] in [[R- Chatting with Glue]] references [[Wikum|Wikum]], and this piece has made significant waves in this community outside of academic HCI (unclear to me how much impact Krieger's piece has had within academic HCI)\n\nSupports:: [[I- a primitive for composing pages to encapsulate meaning]]\n\n[[C- Retroactive funding incentivizes support because it's easier to agree on what was useful than what will be useful in the future]]\n\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-1":{"title":"CG-comment-1","content":"[[P- Chris Granger]]: This is one of those things we actually feel strongly about too. Every decision the system makes is one you should be able to make yourself directly. As a concrete example, I don't think you should really deal with the physical layout of your data, but that doesn't mean you shouldn't be able to specify it yourself. The big thing here though is that specifying it shouldn't mean you need to describe every detail like you would in C. Most tools are either fully implicit or fully explicit and the few that allow you to go from implicit to explicit throw you down into a hole from a simple world where everything was decided for you to a world where you have to define everything explicitly to make progress.\n\n[[P- Chris Granger]]: One principle that we didn't talk about is the idea that you ought to be able to work at the level of abstraction that makes the most sense for the task at hand. E.g. if I were working with a designer, I might say \"let's try something more red\" but if I¬†_were_¬†the designer, I'd want a color wheel to explicitly set the color. We should be able to talk very high level about the parts of the system that don't really matter to us, but be explicitly about the parts that do. Those statements about the world should be able to coexist and I should always be able to not only see what Jump has come up with, but use that as a starting point to be more explicit about how I want some of it to function.\n\n[[P- Rob Haisfield]]: Multiplicity of possible layouts on top of the same underlying data structure (or a shifting underlying¬†structure, like¬† the stairs in Hogwarts) 100%\n\n[[P- Rob Haisfield]]: Being able to do everything explicitly that happens implicitly is a fascinating principle. I agree that underlying code instructions can be highly abstracted. I've been learning Clojure off and on out of a love for DSLs and its brand of functional programming is my starting set of mental models for how code should work. I like writing all of the code and its definitions to the point where it's as though I'm writing sentences describing what the program should do. I imagine a natural language DSL for talking with Jump that can make the whole process more natural.\n\n[[P- Rob Haisfield]]: The levels of abstraction thing is interesting. [I wrote this](https://robhaisfield.com/notes/learn-by-going-up-and-down-the-ladder-of-abstraction) before I ever saw the [Bret Victor piece](http://worrydream.com/LadderOfAbstraction/) hence the name similarity, but it was referring to a different concept. It was a practice I first started in Notion, moved over to Roam, and want to instantiate into the data structure of a thought processor and facilitate the behavior.\n \n [[P- Rob Haisfield]]: Higher and lower level abstractions of concepts are not instantiated into any thought processor I can¬†think of, huh. Attributes sort of get at it, but crudely?","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-10":{"title":"CG-comment-10","content":"[[P- Chris Granger]]: If we do our job, my assumption is that the folks who are likely to change behavior will have more than enough incentive to do so in this¬†system. The question is then how do we either provide some of the value for those that won't or create interesting dynamics that might get them there by accident.\n\n[[P- Rob Haisfield]]: Right, I'm also pointing¬†out that identifying concretely what those behaviors¬†are is still important. In the very least, your behavioral statements during our conversation were good but generally abstract. I'll have more questions about it when I can see a UI. I have little doubt that people will want to use Jump on their own in excellent ways! I'm sure people will be itching to get their hands on dirty when they see more demos.","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-2":{"title":"CG-comment-2","content":"[[P- Chris Granger]]: FWIW, I definitely don't think it can and when it comes to things like goals, I'm actually not sure the computer can figure that much out, but context is an interesting and useful proxy for goals. In the Looker prototypes, you explicitly wrote them out in English: \"Increase revenue in EMEA by 10% this quarter\"\n\n[[P- Rob Haisfield]]: Proxy is a good way to phrase it.","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-3":{"title":"CG-comment-3","content":"[[P- Chris Granger]]: This sounds a bit like what we did in WikiEve¬†[https://incidentalcomplexity.com/2016/06/10/jan-feb/](https://www.google.com/url?q=https://incidentalcomplexity.com/2016/06/10/jan-feb/\u0026sa=D\u0026source=docs\u0026ust=1652846974268448\u0026usg=AOvVaw2eDPWHoUBXTGlU4Rau2S15)\n\n[[P- Rob Haisfield]]: Just read it, fascinating. As I was reading your Eve queries, I can see how you were itching to turn it into natural language. I was picturing something like inline clojure data structures, Specter, and a bunch of composable query operators. Maybe inline structural editors too.\n\n[[P- Rob Haisfield]]: What sorts of queries do you believe are the most important to serve the needs of interdisciplinary synthesis?\n\n[[P- Rob Haisfield]]: Our goal: Web3 people write smart contracts and behavioral economists immediately realize that they have time series data of environmentally constrained choices. They start talking to each other and combining their notes, and they synthesize new frameworks for positively influencing financial behavior. The behavioral economists realize that influence on the web has as broad or broader effects on society than governmental actions, and decentralized finance encourages systemically healthy personal finance. **I want that type of synthesis to arise from people interacting with whatever decentralized synthesis infrastructure ends up being built.**","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-4":{"title":"CG-comment-4","content":"[[P- Chris Granger]]: Yeah, I probably didn't do a good job explaining this, but my stance is that there's a sort of layered dictionary approach where your personal dictionary is always the highest priority and failing to find it there cascades down into a hierarchy of definitions and synonyms. It's ridiculous to me though that I have to explain what \"annual contract value\" is to every tool. Within a domain, things often have accepted definitions and they should just be available to me to use directly or tweak to my liking without me having to systematically describe every aspect of them. Similarly though, there will be lots of words that are unique to me and I should have just as much power to define them.","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-5":{"title":"CG-comment-5","content":"\u003e the choice architecture can present choices that reveal user preferences\n\n[[P- Chris Granger]]: Have you ever looked at decision support/making software? I always thought the hyper-rational binary partitioning they do is an interesting idea. Never been able to bring myself to actually use it though (it's also usually very expensive).\n\n\u003e control over the algorithms\n\n[[P- Chris Granger]]: In principle, the more of Jump that is implemented in Jump, you would have the ability to completely change this stuff if you wanted.","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-6":{"title":"CG-comment-6","content":"[[P- Chris Granger]]: yeah, working on these types of projects for a while I've come to be very optimistic about the possibilities and very conservative on what people will actually do with it.","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-7":{"title":"CG-comment-7","content":"[[P- Chris Granger]]: I was thinking about this after our conversation, when you mentioned search being similar to selection and I realized the reason I focused on selection is that search is about showing you places to go whereas selection is about gathering things. They're otherwise the same I think, it's just what the system lets you do with the answer that distinguishes them.\n\n[[P- Rob Haisfield]]: Ah, that clarifies things. Searching, selection, composing, refactoring, copying? Gathering is a good way to frame it. I suppose you have excellent tools for refactoring what you gather as you go and need?","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-8":{"title":"CG-comment-8","content":"[[P- Chris Granger]] I really like the idea of make a quick copy, clear out parts of it and then sharing that as a conceptual model for partial sharing of concrete things. Gigjam had something similar, though without the copying part which I think is important because it allows for controlled change.","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/CG-comment-9":{"title":"CG-comment-9","content":"[[P- Chris Granger]]: Yeah this is something I'm very interested in. Our belief is that we can do it step-wise though. Due to Jump's maleability, we first try to make it something a person can think with and then see what needs to change to support more and more distributed thinking.\n\n[[P- Rob Haisfield]]: Yup, not a bad approach to start with individuals. People will inevitably have their own styles and it'll be interesting to see what emerges. With idiosyncratic styles enabled by an infinitely flexible tool like Jump, it raises the¬†question: how do all of these unique Jump instances come together? You may need to wait and see. I suppose that's just a selection and refactoring question though?","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/Clarity":{"title":"Clarity","content":"Authored by:: [[P- Brendan Langen]]\n\nClarity is a shared workspace for remote teams. Clarity is pre-built with templates, workflows and project management structures that create an out-of-the-box productivity suite.\n\nIn the same way that [[C- Many products are simply specialized interfaces on top of Excel]]\n\nClarity uses [[Roam Research]]-like block referencing set in a freeform document environment. \n\nFounded by [[P- Richie Bonilla]]\n\nAssigning a task.\n![[Pasted image 20210918111022.png]]\nVisualizing the task assignment. \n![[Pasted image 20210918111207.png]]\nAssigned work in one place.\n![[Pasted image 20210918111321.png]]\nBlock-based transclusion. \n![[Pasted image 20210918111456.png]]\n\n","lastmodified":"2022-05-20T00:31:25.875404911Z","tags":null},"/Codex-OS":{"title":"Codex OS","content":"Authored by:: [[P- Rob Haisfield]]\n\n- #[[Twitter thread]] [[Robert Haisfield (Recently moved to Berkeley)]]: Yesterday I was given a demo of @codexeditor. Absolutely wacky, in all of the right ways. Everything is an entity on a graph that can have labeled relationships with other entities. The UX is basically creating a new desktop OS in your browser for managing all info on that graph. [*](https://twitter.com/RobertHaisfield/status/1326185315380310016)\n  - When I say everything can be an entity, I mean it... text, images, video, audio, websites. Iian is leaning into workspace model, allowing saved arrangements of what you have on the screen to revisit later. Those are also graph entities you can link to. https://twitter.com/RobertHaisfield/status/1267599197412286464?s=20 [*](https://twitter.com/RobertHaisfield/status/1326186101053140992)\n  - Whether it will see broad success or not, I have no idea. It's the sort of thing where you can only wrap your head around it so much without using it. It's insanely ambitious in doing a million things, so we'll see what people will actually do with it https://twitter.com/RobertHaisfield/status/1254554242183917568 [*](https://twitter.com/RobertHaisfield/status/1326187083736649730)\n  - But make no mistake - this is shaping out to be a professional research tool.\n    - Tired: @Superhuman of X\n    - Wired: @Photoshop of X\n    - The sort of thing where some knowledge workers simply accept the usage of this tool as a given for their job, and derive great pleasure in using it. [*](https://twitter.com/RobertHaisfield/status/1326187085296934912)\n  - Codex is rethinking so many core concepts to UX and data structure and operating systems in novel ways it's hard to keep up with. It's a powerful demo, but I'd have to play around with it myself to know if the ideas are actually good. Keep an eye on Codex! Beta upcoming. [*](https://twitter.com/RobertHaisfield/status/1326187977349869568)\n  - Regardless of its outcomes as a product, it will be a valuable research contribution to the tools for thought space. [*](https://twitter.com/RobertHaisfield/status/1326189522531491841)\n  - The last demo he showed me was in March. It's insane how much progress has been made both conceptually and technically in that time period. Just waiting to see what happens next when he opens it up to more users and is able to see what people do (or don't do) with the tools. [*](https://twitter.com/RobertHaisfield/status/1326190050124595201)\n  - I'd say my main concern has to do with how much user involvement is necessary in order for Codex to transform the user's effort into meaningful outcomes. I could annotate everything and name all of the relationships. Is it worthwhile to do that? https://robhaisfield.com/notes/user-involvement [*](https://twitter.com/RobertHaisfield/status/1326197127903506432)\n  - To that, Iian responded that when he releases the beta, one of the main things that he'll be looking out for is when things are more complicated than they need to be. If users are trying to do something cool that requires 5 steps, he'll ask how he could make it 2 or 3. [*](https://twitter.com/RobertHaisfield/status/1326197129950359552)\n  - Additionally, he raised the concept of \"Tacit annotation.\" Effortful annotation is powerful, but you could annotate such that \"it just happens\" as you go. He wants both to be valuable. This is a solid approach! Low floor, wide walls, high ceiling. http://gordonbrander.com/pattern/low-floor-wide-walls-high-ceiling/ [*](https://twitter.com/RobertHaisfield/status/1326197133985288194)\n  - Codex could become an absolute power tool for qualitative researchers https://twitter.com/RobertHaisfield/status/1383434842696683523 [*](https://twitter.com/RobertHaisfield/status/1383443781752549379)\n","lastmodified":"2022-05-20T00:31:25.883404919Z","tags":null},"/DEVONThink":{"title":"DEVONThink","content":"Authored by:: [[P- Brendan Langen]]\n\nDEVONthink is a networked tool for thought on Mac that allows users to collect and organize their documents while affording annotation across any document type. An interesting affordance is an automated categorization system that is trained by past user behavior.   \n\n\u003e DEVONthink keeps all your documents in easy-to-backup databases and presents them to you in a variety of ways. Many documents can be viewed and edited without opening them in another application. Read web pages as if they were local documents.\n\u003e [DEVONthink Pro](https://www.devontechnologies.com/apps/devonthink/editions) scans paper documents and makes them searchable, imports email, and even downloads complete web sites.\n![[Pasted image 20210916165512.png]]","lastmodified":"2022-05-20T00:31:25.891404927Z","tags":null},"/DSL":{"title":"DSL","content":"Authored by:: [[P- Brendan Langen]]\n\nA domain-specific language, or (DSL), is defined by [Wikipedia](https://en.wikipedia.org/wiki/Domain-specific_language) as such: \"A domain-specific language (DSL) is a computer language specialized to a particular application domain.\" \n\nThrough a DSL for decentralized discourse graphs, we would enable people to communicate information in a machine/human readable way. This enables [[end-user programming]]","lastmodified":"2022-05-20T00:31:25.891404927Z","tags":null},"/Dendron":{"title":"Dendron","content":"Authored by:: [[P- Brendan Langen]]\n\nDendron is a networked hypertext tool for thought with a local-first, flexible hierarchy. \n\n\u003e Dendron finds the usable center between unstructured backlinks and rigid file hierarchies. We help users maintain a canonical hierarchy for every note while also allowing for arbitrary backlinks to any other note.\n\nThis flexible data hierarchy aims to solve the growing problem of digital notebooks. \n\nFounded by [[P- Kevin Lin]]\n\n![[Pasted image 20210916145843.png]]","lastmodified":"2022-05-20T00:31:25.891404927Z","tags":null},"/Discourse-Graph-Plugin":{"title":"Discourse Graph Plugin","content":"\u003e The Discourse Graph extension enables [Roam](https://roamresearch.com/) users to seamlessly add additional semantic structure to their notes, including specified page types and link types that [model scientific discourse](/roamresearch-discourse-graph-extension/fundamentals/what-is-a-discourse-graph), to enable more complex and structured [knowledge synthesis work](https://oasislab.pubpub.org/pub/54t0y9mk/release/3), such as a complex interdisciplinary literature review, and enhanced collaboration with others on this work.\n\nFor more details, see the [documentation](https://oasis-lab.gitbook.io/roamresearch-discourse-graph-extension/) and a [demo](https://www.loom.com/share/2ec80422301c451b888b65ee1d283b40). \n","lastmodified":"2022-05-20T00:31:25.891404927Z","tags":null},"/EDN":{"title":"EDN","content":"Authored by:: [[P- Brendan Langen]]\n\nExtensible Data Notation (EDN) is a subset of the syntax in Clojure, similar to JSON. This allows for richer backend data structures alongside Datomic, and represents the data structures as text. \n\nEDN is extensible, which neither JSON or YAML can say. This enables text to act as functions to transform the data, similar to a map function. \n","lastmodified":"2022-05-20T00:31:25.891404927Z","tags":null},"/Elicit":{"title":"Elicit","content":"Authored by:: [[P- Brendan Langen]]\n\nAI research assistant that provides assistance with Lit Reviews, abstract summarization, claim validation, and many more features. \n\nElicit is built for academics and research professionals using GPT-3. \n\nElicit performs a large number of tasks, to which some are shown below. \n![[Pasted image 20211220155223.png]]\n\n\nOne prominent feature is to perform a lit review as it relates to a question, seen here.\n![[Pasted image 20211220155342.png]]\nResponses are generated that fit the natural language query. \n![[Pasted image 20211220155413.png]]\n\nThe model can then be trained to specify answers that match your search, bringing the human into the loop. \n\nhttps://twitter.com/i/status/1440380523180818440","lastmodified":"2022-05-20T00:31:25.891404927Z","tags":null},"/Feedback-for-Chris-Granger-after-we-discussed-Jump":{"title":"Feedback for Chris Granger after we discussed Jump","content":"Authored by:: [[P- Rob Haisfield]]\n\nHey Chris, thank you again for taking the time to interview! Most of the Roam and Notion forks are dedicated to making faster horses, meanwhile you're creating a self-driving, KITT augmented hovercraft. It's sort of like the holy grail OS of end-user computing, and I wish you the best of luck with it. Presuming successful execution, I fully expect this to be a paradigm shift. Based on your experience and career leading up to Jump, you're one of the best people to work on it.\n\nYou asked for my perspective on what you shared and what you might be missing, and I said I would reflect and get back to you. Your work pre-Jump has given you a complete conceptual framework and thesis. In many ways, [[Q- Does sufficiently advanced natural language processing invalidate the need for a structured DSL|Jump is one possible set of logical conclusions for the questions we've been asking]], and, if you were to look over my knowledge graph, you would likely find it hilarious in its similarity.\n\nIt feels almost audacious to say you're missing anything with Jump, as what you revealed in our conversation and the demo are a subset of all of your research. Regardless, I leave this for your consideration. When we have more to share of our own research, I'll appreciate your analysis of what we're missing.\n\nNot making a user tell you something twice is a powerful design principle. I'm sure you will be able to infer a large amount of structure for free. However, I can't shake the feeling that [[CG-comment-1|automatic inferences from my language would deprive me the fine grained control I want]], particularly in creating data structures. I imagine there will have to be some balance. Your molecular schemata approach may be enough. The philosophical difference is that I do not believe revealed preferences are everything. [[CG-comment-2|If a computer can't infer my goals perfectly from my behavior]], then what more powerful tool¬† do I have than manual controls to tell the computer what exactly my goals are? There is something to a manual tool that gives me 100% legibility over how the computer will interpret my inputs.\n\nI have been exploring the idea of a [[I- A DSL for a discourse graph with information entry, visualization, and retrieval|DSL for synthesis and decision making]] (where the data at the base is not necessarily a text file), and one notion I'm playing with is writing sentences with custom inline data structures. [[CG-comment-3|Nesting blocks within sentences as a replacement for words]]. One could synthesize our approaches ‚Äî Jump makes the data structures in the background and the user confirms that they are correct.\n\nGenerally, I've been exploring what I refer to as \"semantic self-expression\" in software. For example, Roam infers indentation in an outline to mean branching or nested thought, and encodes that relationship for the sake of querying and filtering linked references. To me, this felt like Roam gave me a way to communicate what was related to what so it could excel at answering my questions. You can find some design suggestions for visualizing those relationships and supporting exploratory browsing in my [onboarding Roamgames submission](https://www.figma.com/file/5shwLdUCHxSaPNEO7pazbe/Dhrumil%26Robert---RoamGames-Challenge-2?node-id=0%3A1). Other mechanisms that fall within your selection and refactoring framework include conditional formatting, possibly where the conditions are synthesized by the computer after the user has manually formatted information, possibly through something like Codex Editor's standoff annotation.\n\nI want to be sure that those schemata are not just what's formed from the dictionary and thesaurus, but also that I am [[CG-comment-4|appending custom phrases and coinages as I go]]. The dictionary and thesaurus provide information, but the phrases for how I compose labeled ideas is more important. I do not expect most interdisciplinary conceptual links to share words. (See Andy Matuschak's [note titles as APIs](https://notes.andymatuschak.org/z3XP5GRmd9z1D2qCE7pxUvbeSVeQuMiqz9x1C) and arguments for [concept oriented notes](https://notes.andymatuschak.org/z6bci25mVUBNFdVWSrQNKr6u7AZ1jFzfTVbMF))\n\nYou chose to organize exploratory browsing such that it would maximize surprise in the information theory sense of the word ‚Äî it must communicate something new that does not feel trivial to the perceiver. I'm not sure¬† the implications on user behavior would be to this. If your system is actively guiding people in their exploration, then you have a responsibility to guide appropriately. We've seen downstream implications of social media newsfeed algorithm optimizations so it's an important design space.\n\nHowever, computer augmented synthesis as a series of scaffolded surprises is certainly a compelling idea. It‚Äôs rare to see a behavioral perspective this refined. As such, I believe that the process of synthesis can be made intensely fun for participants through explicit choices.\n\nAlong those lines, I've been playing with two things. One is that the [[CG-comment-5|the choice architecture can present choices]] that [[C- User behavior within a well-designed choice architecture can be a signal of revealed preferences|reveal user preferences]], similar to how we were talking about how the most useful thing a user can do is reveal their goals and have a conversation with Jump. Another is that there are fascinating interface possibilities with respect to giving users direct feedback loops and [[CG-comment-5|control over the algorithms]] serving them content.\n\nTo reveal user preferences on an exploratory browsing/search interface, one approach would be allowing people to [[C- Highlighted and lowlighted search results map to how well results map to intentions|emphasize and de-emphasize information]], and encode the searches and their results accordingly. If queries are statements of intent, result prioritization is a confirmation that the computer has an accurate model.\n\nYour notion of building out a molecular schema that filters down the overall knowledge graph based on only the information it has and local context radiating outward is powerful. When people say they like an app because it works like their brain does, they don't know what‚Äôs coming. In your position, I would strongly consider how you can infer what is the most contextually relevant information based on how people write, lay out information, and relate it. I expect your refactoring tools will enable people to adjust structure top down as well.\n\nI think you are underestimating the sheer amount of information that can be gathered from crowdsourced user behavior from many people performing varied roles on the same content. Leaving questions for others to answer, emphasizing and de-emphasizing content, curating trails, curating curated trails, etc. Additionally, given how much you have reduced the friction to add metadata to content, a crowdsourced semantic web may become more feasible than it was before.\n\nIt also appears that you have not fully considered social coordination design to support lofty augmented collective thinking goals. As we discussed, this is a behavioral influence question, and as you mentioned, some percentage of the difficulty here has to do with current incentives around sharing. However, [[CG-comment-6|your goal seems more neutral about what people will do with Jump]], so that may not be important.\n\nTokens‚Äîfinancial, reputational, or otherwise, can support overall social coordination. For example, compensating users for supporting other users (answering questions, building plugins, leading onboarding sessions, etc.). However, crypto Web3 people haven't fully considered how software design influences user behavior and overweight tokens.\n\nYou mentioned that people don't always know what questions they are asking. The best thing to provide them with is an exploratory browsing experience. I wonder if, in those situations, there might be opportunities to prompt awareness and help them formulate a question. Your hypothesis that browsing with expanded context will lead them to generate a question is valid.\n\n[[I- Search as a part of the primitive design|Selection]], refactoring, and copies are a solid set of primitive concepts. [[CG-comment-7|The workflows to search]] / browse and work with that information as users go are crucial. For example, in Roam, one of their expectations is that as you're browsing backlinks or query results, you block reference them into an outline in the Daily Notes and annotating what you have determined as important, or editing results in place. Obsidian's search is arguably more complete, but incorporating the results into your current thinking is more difficult due to the UX because it shows all of the results but requires you to click and navigate. If you look at [part three of my Roamgames submission,](https://www.figma.com/file/5shwLdUCHxSaPNEO7pazbe/Dhrumil%26Robert---RoamGames-Challenge-2?node-id=0%3A1) you will see an interface that shows relevant notes, updates legibly based on new information, and enables the user to manually emphasize and de-emphasize relationships. That's all based on indentation, the slider, and filters. What's missing is an intelligent backend that's able to interpret the user inputs and log those _in conjunction with_ the searches as a meaningful input for the future.\n\nI would keep pulling at the Gigjam string. People bringing their knowledge graphs together [[CG-comment-8|without sharing too much of the total graph]] is not an easy problem at all and [keeps Joel Chan up at night](https://twitter.com/JoelChan86/status/1309521782806847490?s=20\u0026t=Y2_Y5xPm7X6NJAfkfO2_0A).\n\nA few miscellaneous thoughts that came to mind as we were going:\n\n-   I would prompt you to think more about [[CG-comment-9|how multiple people think together in Jump.]] Apps like Roam make the implicit assumption that synthesis happens in one mind. A computer augmenting a person is powerful, but so is a computer/other people. I believe that scaffolding built for decentralized synthesis looks a lot like a social network (with different goals than social media).\n-   You often said things like \"there ought to be a way for the system to recognize the conceptual similarity\" and my first thought each time was to replace \"system\" with \"multi-human/computer system.\" [[CG-comment-10|In your thought experiment, everything that requires difficult behavior change seems to be left to the computer]]. I'll be pondering that for a while.\n-   The insight that the most useful thing a user can do is reveal their goals explicitly and have a conversation with the tool is powerful. Additionally, that common goals for a domain can be prebuilt, it's just that nobody does it.\n-   Creating programs in the background for users is a powerful notion. A key problem we‚Äôve noticed in a lot of our user interviews is aspirational systems that people can‚Äôt seem to keep up with. This may be a way to decrease entropy over the entire system as quantity increases.\n-   Apart from knowledge management, I want to use Jump to handle my DeFi taxes. I'm imagining uploading CSVs, making rules about how I want interactions with certain smart contracts to be treated. Many DeFi users are feeling the pain of waiting for tax software developers to update and include new protocols given the rapid pace of innovation.\n-   I would love to chat with you about your user research in this space. It's clear how zooming out on your career, your work on Microsoft's assisted coding, Light Table, Eve, and Looker all played into your product-instantiated insights for Jump, but it's also clear that you studied your users more than most.\n-   I'd love to hear you elaborate on, \"Decentralized apps with a P2P backend feel different than cloud based apps in that you build a more direct relationship with it.\"\n","lastmodified":"2022-05-20T00:31:25.891404927Z","tags":null},"/Figma":{"title":"Figma","content":"Authored by:: [[P- Brendan Langen]]\n\nFigma is a collaborative design tool that enables rapid brainstorming, prototyping, and iteration. Figma's ecosystem is packed with design systems, components, and plugins from major companies and independent designers. This [[C- End-user scripting enables creative workarounds|end-user scripting enables creative workarounds]] for design teams around the world. \n\n[[C- We can add additional visual information on top of existing metadata]]\n\nLive board for related UI - https://www.figma.com/file/kHLDEecCMvZAIDnfe95tmp/graph-research-visual-inspo?node-id=0%3A1","lastmodified":"2022-05-20T00:31:25.895404931Z","tags":null},"/GUI":{"title":"GUI","content":"Authored by:: [[P- Brendan Langen]]\n\nA graphical user interface, or GUI, is a presentation layer atop an application. Most user interfaces today are GUIs, although a variety of other options exist, including Command Line Interfaces and Command Line User Interfaces. \n\nGUIs enable constructed additions, which allows affordances like [[ZUI]]s to exist atop them. ","lastmodified":"2022-05-20T00:31:25.903404939Z","tags":null},"/Gigjam":{"title":"Gigjam","content":"Authored by:: [[P- Brendan Langen]]\n\nMicrosoft tool from 2016-2017 aimed at on-the-fly collaboration in virtual spaces, referenced by [[P- Chris Granger]] during his time there. \n\nGigjam made a few interesting design decisions that prefaced the decentralized networking boom. \n1. Users could build their own mini groups for different gigs, giving them spaces to share different sets of information depending on who was in the gig with them.\n2. This occurred through API plug-ins where users could select what would be shown/redacted within a given card - an amazing level of privacy and choice afforded to the user.\n3. Beyond just mouse and keyboard control, gigs used touch and voice input, as well.\n\n![[Pasted image 20211212125337.png]]\nIn the image above, you could draw a circle around the data you wanted to be included, and an X over anything you want to redact. \n\n\u003e GigJam combines data from a variety of services including Microsoft‚Äôs own Office 365, Trello, Dropbox, and Salesforce. Users can then bring that information into a shared workspace, or ‚Äúgig,‚Äù allowing them to quickly work together.\n   \n\u003e Users can easily redact part of the information they‚Äôre sharing with other people, meaning they can selectively share only what needs to be seen in order to get a job done.¬†\n\n\u003e Here‚Äôs how it works: One user starts a ‚ÄúGig,‚Äù and then pulls in information from whatever services they need, like email, Salesforce, Office documents, and Asana tasks. That information shows up as a card inside GigJam, where users can highlight some information inside a card, redact other information, and then send the whole bundle off to another user for review or editing.\n\n\u003e It‚Äôs a good way to both keep focused on the task at hand (like editing only one slide out of a PowerPoint presentation) and also enables workers to more easily team up with people outside of their organization, like suppliers and contractors who shouldn‚Äôt be privy to some information.¬†\n\n\u003e The GigJam interface also combines a bunch of interesting input methods. Users can work entirely with the keyboard and mouse, but they can also interact with Gigs using touch and voice input. The service is a crazy bundle of different modern capabilities and looks in demos like something out of the future.\n\n\nhttps://www.pcworld.com/article/414816/microsofts-fascinating-gigjam-service-is-open-to-anyone-who-wants-an-invite.html","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Hode":{"title":"Hode","content":"Authored by:: [[P- Brendan Langen]]\n\nHode (Higher Order Data Editor) is a Haskell program for reading, writing and searching what might be loosely termed a hypergraph. It's like a knowledge graph, but more expressive -- atoms and relationships can both be members of other relationships, which are labeled and can have any number of members.\n\nCreated by [[P- Jeffrey Benjamin Brown]] https://github.com/JeffreyBenjaminBrown\n\n\u003e There are three branches to Hode: The RSLT data structure, the Hash language for describing subsets of an RSLT, and the UI, which lets you use the previous two things.\n\u003e A Rslt (Reflexive Set of Labeled Tuples) is a generalization of a knowledge graph. It lets a user easily represent any natural language expression. (A `Rslt` is isomorphic to what some programmers call a \"hypergraph\" -- but mathematicians claimed that term first, and in math it means something much less expressive.)\n\u003e A `Rslt`is a collection of expressions, each of which is either a phrase (like \"cats\"), or a relationship (like \"cats have noses\") or a template (like \"_ have _\") shared by many relationships.\n\u003e What distinguishes a `Rslt` from a `graph` is that relationships can involve any (positive) number of members, and a relationship can itself belong to other relationships.\n\u003e Hash is a language, close to ordinary natural language, for talking about expressions in a `Rslt`. It offers a concise representation, both for individual `Expr`s (expressions) in a `Rslt`, and for queries to retrieve sets of `Expr`s.\n\u003e The UI displays expressions from your graph using [the Hash language](https://github.com/JeffreyBenjaminBrown/hode/blob/master/docs/hash/the-hash-language.md).\n\nExamples in action.\n```\n/a (mammals #need calcium) #because (mammals #build bones)\n```\n\n```query\nmammals\n\t#need _\n\t7: \"mammals #need calcium\"\n```\n![[Pasted image 20210912173246.png]]\n\n```\n`/find /eval bob #likes pizza ##with pineapple ###because /it)` -- Returns only the reason, not the full \"because\" relationship.\n```\n\n```\n`/find bob #likes /_ /|| bob #dislikes /_` -- Every #likes and every #dislikes statement with bob on the left.\n```\n\n\n![[Pasted image 20210915183241.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Hypercard":{"title":"Hypercard","content":"Authored by:: [[P- Brendan Langen]]\n\nHyperCard is an example of early [[end-user programming]] in the wild. Released in 1987 by Apple on the Mac, HyperCard included its own [[DSL]] - HyperTalk - that enabled users to create hypermedia systems. HyperTalk was written in a syntax that resembled the English language, which enabled many non-programmers to interact with the cardstack structure expressed by HyperCard. \n\nHyperCard was deprecated in 2004. \n\n\u003e The beauty of HyperCard is that it lets people program without having to learn how to write code ‚Äî what I call \"programming for the rest of us\". HyperCard has made it possible for people to do things they wouldn't have ever thought of doing in the past without a lot of heavy-duty programming. It's let a lot of non-programmers, like me, into that loop.\n\nDavid Lingwood, APDA[4](https://en.wikipedia.org/wiki/HyperCard#cite_note-APDA_Pinpoints-4)","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Hypernote":{"title":"Hypernote","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\nGraph database based outliner, Roam predecessor, displays powerful use case for attributes and triples well. https://www.loom.com/share/7115476e01c342a3bd58a2734489dced","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Hypothesis":{"title":"Hypothesis","content":"Authored by:: [[P- Brendan Langen]]\n\nHypothes.is allows users to annotate webpages or documents in their margins, providing the option to further enrich a snippet with context. Sharing is built in to Hypothes.is, affording added context and active reading across groups. \n\nThis enables social tagging, which helps users find related content and build community. [[C- Social tagging is a key user behavior to managing a decentralized knowledge graph|Social tagging is a key user behavior to managing a decentralized knowledge graph]]. This can come in the form of text, likes or [[C- Emoji reactions are a form of social tagging|emoji reactions]]. \n\n\n![[Pasted image 20210915173128.png]]\n![[Pasted image 20210915182506.png]]\n\nIn an exciting development, the W3C Web Annotation recommends expanding upon their functionality by binding annotations to specific subregions of PDFs, which would enable [[interoperability]] beyond the tool alone.","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-A-DSL-for-a-discourse-graph-with-information-entry-visualization-and-retrieval":{"title":"I- A DSL for a discourse graph with information entry, visualization, and retrieval","content":"Authored By:: [[P- Rob Haisfield]]\n\n[Wikipedia](https://en.wikipedia.org/wiki/Domain-specific_language) defines a DSL as such: \"A domain-specific language (DSL) is a computer language specialized to a particular application domain.\" Through a DSL for decentralized discourse graphs, we would enable people to communicate information in a machine/human readable way.\n\nWhen thinking through [[Q- What are powerful interfaces for entering information into a discourse graph]], one of our biggest hypotheses is that a domain-specific languages can enable a high degree of expressivity and efficiency, and that GUI affordances can serve as scaffolding. Since my work with GuidedTrack, [I have come to believe that this is a powerful new design paradigm.](https://robhaisfield.com/notes/domain-specific-languages-as-end-user-software). \n\nWe should note that a DSL as we define it does not need to interface through a plain text file. They could feasibly go through spreadsheets, canvases, and block based editors as well. The point is that the syntax will allow people to communicate more information and metadata than simply writing a document with natural english would. People will have prebuilt, high level, declarative functions, like `query`, `sort`, `group`, `view-as-map`, `create-table` etc. that are unintimidating to use, and they will be able to create custom functions. \n\nThese functions could be simple to both create and call. In [[Notion|Notion]] and [[Roam Research]], they use slash commands to call functions on a block. This can more or less be thought of as [[autocomplete]]. In [[Emacs]], they use the infamous M-X. [[Nextjournal]] implements [[R- clojureD 2021- Command and Conquer- Learnings from Decades of Code Editing by Philippa Markovics]], where it only shows you the functions that are possible given your context. \n\n![[Pasted image 20210930175757.png]]\n\nAdditionally, not all information that is entered needs to be code. For example, a [[Literate programming interface]] might have a block based editor where the text in each block is understood as writing unless otherwise specified as a code block. The default assumption for a DSL for a discourse graph would likely be the same. One can also look at [[Pollen]] as an example for inline coding. There, you write prose normally, but when you want to apply a function to text within your prose you use a unique identifier, like using the following ‚óäem{to bold} what is within the curly brackets. This syntactical form is referred to as an X-Expression.\n\n[[Q- What are powerful primitives for a user of a decentralized knowledge graph]]? [[Roam Research]] is able to infer which page and block references are related to each other from collocation in a block or in a branch of an indentation tree. Indentation can be considered a core part of their syntax. Each page and block has a unique ID, and whenever those IDs are referenced in another block, those IDs are related to the block through a children-nodes attribute. [[Q- What are powerful interfaces for entering information into a discourse graph]]?\n\nWhile text is an interface for information entry, the end visualizations need not be limited to text. [Flowchart.fun](https://flowchart.fun/) is able to infer a flowchart structure from indentation, attributes, and references. This serves as a useful expansion to the meaning inferred from indentation.\n\n![[Pasted image 20210913182317.png]]\n\nWe do not yet know what the syntax for our discourse graph will be. This will certainly be informed by our explorations of [[Q- What is the data structure of a graph built to facilitate decentralized knowledge synthesis]] and user interviews.\n\nHowever, typed edges and nodes are an expressive backbone, as can be seen with [[Hode]]. Hode allows you to create edge and node relationships between items fluidly as you write. It then allows you treat a set of items with a relationship as an entity that can have relationships with other ideas.\n\nOn the extreme end, one might ask, \"Do we need to have defined ontologies, or can AI or neural networks replace databases?\" We might also ask, [[Q- Does sufficiently advanced natural language processing invalidate the need for a structured DSL]]? *See [[R- Neural Databases]].* \n\n[[Programmable text interfaces are the future]]. Text is familiar and fast. When you replace buttons with functions and add control flow / abstraction, it unlocks powerful custom use cases. Structural editors and other means can facilitate the growth of newcomers, who are able to gain value from the beginning, towards expertise, where they will never want to use anything else.\n\nBy allowing people to write data structures and queries inline, we allow for [[Literate programming interface|literate programming]]. When people include a number, they can also include the formula or query that results in displaying that value. In that way, readers would be able to verify the evidence. \n![[Pasted image 20211117170352.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Enable-composable-queries-to-facilitate-structure-in-hindsight":{"title":"I- Enable composable queries to facilitate structure in hindsight","content":"Authored By:: [[P- Rob Haisfield]]\n\n[[I- Populate the related items section through a search term|If a page's related items section is defined through a search term]], then we can think of the page title and its query as a key-value pair. The data structure would look something like this:\n\n``` clojure\n;; Here you'll notice a general pattern in the queries, where there's an \"any\" clause that always includes the base title in the search results. This is essentially a smart default - references to the page title are almost always going to be relevant. \n;; Then you'll see additional terms. In the first example, I'm saying that a result has to include either \"structure\" OR \"structuring\" as well as \"hindsight.\"\n;; Any time something is within parentheses, think of it as one term.\n\n\"PCT\" means\n  (any: \"PCT\"\n        \"Perceptual Control Theory\")\n\n\"SDT\" means\n  (any: \"SDT\"\n        \"Self-Determination Theory\")\n\n;; \"Behavioral Science Theories\" = pages whose titles match Behavioral Science Theories or either SDT or PCT\n\"Behavioral Science Theories\" means\n  (any: \"Behavioral Science Theories\"\n        (any: \"SDT\"\n              \"PCT\"))\n              \n\"Theories\" means\n  (any: \"Behavioral Science Theories\"\n        \"Political Science Theories\"\n        \"Biology Theories\")\n\n\"structure in hindsight\" means \n  (any: \"structure in hindsight\" \n        (all: (any: \"structure\" \"structuring\")\n              \"hindsight\")\n              \n\"structure in foresight\" means \n  (any: \"structure in foresight\" \n        (all: (any: \"structure\" \"structuring\")\n              \"foresight\")\n              \n\"structuring across time\" means\n        (any: \"structure in hindsight\"\n              \"structure in foresight\")\n\n```\n\nOver time, composable queries fight entropy. We would frequently see people create multiple pages that mean the same thing simply because their body of notes had grown so large that they didn't remember they had already created other pages to refer to the same concept. \n\nThis is a huge challenge of adding structure! In our private research notebook, we refer to this problem of entropy from structure in multiple ways (for example, `[[Q- How might we prevent workflow entropy]]`and `[[I- Enable composable queries to reduce system entropy]]`). With the solution we're proposing, we could adjust the query for the idea page to bring in references to the question page, or we could create a new, encompassing page with a query that brings in references to either.\n\nWe aim to [[work at the speed of thought]], but too much structure creates paralyzing cognitive overhead. [[C- An increasing amount of structure leads to entropy]], pointing to the importance of [[smart default|smart defaults]].\n\n[[Roam Research]] enables users to merge pages together. This affordance aims to solve issues where users have created multiple pages for similar ideas, but the results are different in practice. When two pages represent conceptually \"close\" concepts, users consistently link blocks to both pages to ensure linked references show up on each page. Merging is not always appropriate.\n\nOver time, these duplicate links pile up, and users feel the need to link to a large vocabulary of pages, which brings us back to square one with [[Repeat work]]. Their system's surface area has greatly increased in size, and they're reliant on their own memory to recall each new link they add. Ultimately, this leads to system collapse or abandonment.  \n\nMultiplayer amplifies the problem, [[Q- How do we solve the problem of different people referring to the same concept with different language|as different people refer to similar concepts with different language]].\n\nComposable queries enable people to create both aliases and hierarchies. Aided by the full expressiveness of a query language, composable searches empower people to build up their own semantic dictionary and thesaurus over time. As the queries can be adjusted at any point, this is a promising solution to the question: [[Q- How might we allow people to adapt their past system and notes to current needs]]?\n\nOf note: users who have created many layers of abstraction through these mechanisms may eventually end up with a system that is difficult to reason about. For example, someone may not be able to understand at first glance why a related items section or query contains certain results due to all of the propagation. At this point, we don't know if that would actually be a problem or not, but to go further down the [[I- A DSL for a discourse graph with information entry, visualization, and retrieval]] metaphor, there may be a requirement for \"debugging\" tools, enabling users to peer into their layers of abstraction.","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Populate-the-related-items-section-through-a-search-term":{"title":"I- Populate the related items section through a search term","content":"\nAuthored By:: [[P- Rob Haisfield]], [[P- Brendan Langen]]\n\nIn the [[structure now vs. later (uncertain payoff, regret)|structure now vs. later]] tradeoff, people tend to [[[Why people prefer to structure later or not at all|defer structuring until later]]) or not structure at all. Search is the primary tool people have available for resurfacing unstructured information. However, search can be extended to be the basis for creating structure in hindsight. As explored in [[I- Search as a part of the primitive design]] within [[I- A DSL for a discourse graph with information entry, visualization, and retrieval]].\n\nFor example, a page titled `[[Behavioral science theories]]` could have a \"related items\" section defined by a search for `[[Behavioral science theories]]` or any time any specific theory (e.g. `[[Perceptual Control Theory]]` or `[[Self-Determination Theory]]`) is mentioned.\n\nThe search might look like: `any: [[Behavioral science theories]] [[Perceptual Control Theory]] [[Self-Determination Theory]]` \n\nThese searches could then be composable, enabling more efficient categorization. For example, I might have a page titled `[[Theories]]` with an \"associated items\" section defined by the \"associated items\" sections for `[[Behavioral science theories]]` and `[[Biology theories]]`. \n\nAlternatively, a page titled `[[Intersection points in user experiences]]` could have a related items section defined as a search for any block or page that mentions multiple user names at a time.\n\nThere are few things that are more expressive than a search. [[C- Search terms express intentions]]. Through logical operators like ANY, ALL, NOT, IF, and custom relational rules people are able to express precisely what they consider relevant and in which contexts. *Logical operators and query syntax are subject to future debate.* There could be graph query operators, for example, that indicate \"show me anything within 2 degrees of connection to this node.\"\n\nNow let's get fancy with it, to show more interesting examples of what is possible for [[structure in hindsight]] and [[structure in foresight]].\n\nSee, for example, how I map Roam queries to English questions so I'm able to find an answer [in minutes 20-30 of this video](https://youtu.be/47A0gK7Vo8E?t=1200). Those questions did not have a set of backlinks because there was no single page to backlink to! If I created a new page for each question, the backlinks would be empty until after I had searched through my notes and manually added linked references to the question page. By creating a query, I was able to say: \"All blocks that match these rules are relevant to this question, even if there is not an explicit reference to this question.\"\n\nEffectively, this enabled me to process past notes in bulk. [[C- A decentralized discourse graph is dynamic]], and wrangling past data into new slots through search is one way to answer the question: [[Q- How might we allow people to adapt their past system and notes to current needs]]? Queries like this reduce work going forward, as they mean I don't need to remember to reference a specific page every time if there are other mechanisms for inclusion into a related items section.  \n\nThe above example points towards another powerful use for queries: Smart folders. \n\nMost file systems require you to manually add files into different folders. Keeping these folders up to date can be a challenging job. The same problem holds true for note taking apps where you need to manually place a tag or bidirectional link in order to put ideas into buckets. [In Apple's Finder](https://www.howtogeek.com/403077/how-to-automate-your-mac-with-smart-folders/), you have the ability to create smart folders. These smart folders are populated by the results of a search, so they autoupdate when new content is added to your file system that matches certain criteria. In essence, this enables you to create organization systems now to capture knowledge in the future. \n\nI remember how my mind was blown when I first saw [[P- Allen Wilsonn]] link a tweet [to a search](https://twitter.com/AGWilsonn/status/1265760007414579206). This was just an early example of a powerful new behavior.\n\n![[Pasted image 20210916173736.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Provide-Push-and-Pull-as-inline-syntax-to-affect-the-related-items-section-for-a-page":{"title":"I- Provide Push and Pull as inline syntax to affect the related items section for a page","content":"Authored By:: [[P- Rob Haisfield]]\n\nAs mentioned on [[I- Replace the backlinks section of a page with a related items section]], [[C- Linked references are a smart default for related items]] but are not a complete description of what is related. The exclusive use of backlinks to define a related items section leads to the problems outlined on [[Q- How do we solve the problem of different people referring to the same concept with different language]]\n\nWhile [[I- Populate the related items section through a search term|populating the related items section through a search term]] solves that problem with precision, editing the search term would likely require a decent amount of effort. [[Agora]] provides an ergonomic solution with their Push and Pull mechanisms.\n\nWithin [[Agora]], if you are writing on a page and believe the backlinks for another page would be relevant, you can write `[[pull]] [[page2]]`. Push does the opposite, pushing backlinks from the page you are writing on to another page's backlinks section.\n\nFor example, imagine you have a page called `[[Graph Core Protocol Development Teams]]`. On that page, you could `pull` references from each team into the related items section of development teams page. The search term that populates the related items section would adjust in response to uses of `push` and `pull`, looking something like *(syntax isn't final)* `(any: [[Graph Core Protocol Development Team]] [[Edge \u0026 Node]] [[Figment]] [[Semiotic.ai]] [[StreamingFast]] [[The Guild]])`\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Replace-the-backlinks-section-of-a-page-with-a-related-items-section":{"title":"I- Replace the backlinks section of a page with a related items section","content":"Authored By:: [[P- Rob Haisfield]], [[P- Brendan Langen]]\n\nWe can [[I- Populate the related items section through a search term]]. Since [[C- Linked references are a smart default for related items]], linked references should be the default search term so users don't have to do additional manual work to make the related items section useful. To give users a low friction way to edit the search term for a page's related items, we can [[I- Provide Push and Pull as inline syntax to affect the related items section for a page]].\n\nBecause users struggle with the dilemma to [[structure now vs. later (uncertain payoff, regret)]], it is useful to lower the cost of adding structure. A related items query that can be edited over time could accomplish this by enabling [[I- Enable composable queries to facilitate structure in hindsight|structure in hindsight]], or affording a user the chance to add structure indirectly, and in the moment of ambient review. We suspect these queries will reduce system entropy over time.\n\nGiven that search terms can be defined to bring in a broader array of stuff than a simple list of backlinks, users will require a way to filter their results. As described in [[Interfaces for adjusting a query]], this does not all need to occur through text entry.\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Search-as-a-part-of-the-primitive-design":{"title":"I- Search as a part of the primitive design","content":"Authored By:: [[P- Rob Haisfield]]\n\nThis page serves as an index page for a clump of ideas.\n\n-   [[I- Replace the backlinks section of a page with a related items section]]. This one gets at the core idea, which is that if backlinks can be expressed as a query, we might as well allow the user to edit those queries, as [[C- Search terms express intentions]]. \n-   [[I- Use search terms as the basis for other features]]\n    -   [[I- Utilize search terms as the basis for user-defined notifications]]\n    -   [[I- Utilize search terms as the basis for user-defined subscriptions]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Use-search-terms-as-the-basis-for-other-features":{"title":"I- Use search terms as the basis for other features","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n\nA sufficiently detailed search term can be the basis for other features. For example:\n    - [[I- Utilize search terms as the basis for user-defined subscriptions]]\n    - [[I- Utilize search terms as the basis for user-defined notifications]]\n    - [[I- Utilize search terms as the basis for conditional formatting]]\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Utilize-search-terms-as-the-basis-for-conditional-formatting":{"title":"I- Utilize search terms as the basis for conditional formatting","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n\nThis one is simple - if we can use search as the basis for other features, and we can [[Q- How might we apply map filter reduce to notes and what other primitives are relevant to this domain|map some action onto all of the search results]]. For example, we might search for all claims and then make all wikilinks green.\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Utilize-search-terms-as-the-basis-for-user-defined-notifications":{"title":"I- Utilize search terms as the basis for user-defined notifications","content":"Authored By:: [[P- Rob Haisfield]]\n\nIn multiplayer applications that do not have native notifications like [[Roam Research]], we noticed a compensatory pattern in user behavior. Essentially, they would come up with a convention for a special page link to indicate a mention (like `[[@ Rob Haisfield]]` or `[[mention: Rob Haisfield]]`), and then write two queries for seen and unseen mentions. Unseen mentions would be filtered out through the addition of a link like `[[seen]]`.\n\n![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwrite-hypertext-notebook-graph-research%2Fdri1S1LNrf.png?alt=media\u0026token=90e1df22-759e-4bed-a401-6b967d7198dd)\n\nThis actually isn't far off from a proper notifications system. Its primary problem is that it requires the user to manually visit the location of the queries. The main adjustment would be to [ping the user when a new item enters the unseen query](https://davidbieber.com/snippets/2021-01-25-notifications-in-roam-research/). Roam users have hacked around this by writing plugins that would ping them on Slack whenever there was a new mention.\n\nFrom a [[primitive design]] perspective, users would need the ability to set up automations, to include \"new result enters query\" as a trigger, and \"send notification\" as an action. From there, the user could define what would notify them by adjusting search terms. This is further enabled by [[I- Enable composable queries to facilitate structure in hindsight|composable queries]]. \n\nIt is important to note that not everyone wants to go through the effort to create their own notifications system, hence the importance of smart defaults. Notification through mentions is unlikely to be controversial as a starting point.\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Utilize-search-terms-as-the-basis-for-user-defined-subscriptions":{"title":"I- Utilize search terms as the basis for user-defined subscriptions","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n\nPiggybacking on [[I- Enable composable queries to facilitate structure in hindsight|enabling composable queries to facilitate structure in hindsight]], people will be able to define a search term that outlines their interests, such that they will be able to see any time new items are added to the query. From there, they could easily define [[I- Utilize search terms as the basis for user-defined notifications|whether they would like to receive notifications]] or have a more passive way for keeping up to date, similar to how Gmail will show you the number of unread items in a label.\n![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwrite-hypertext-notebook-graph-research%2FONhwQVp40E.png?alt=media\u0026token=06082a7a-f7c9-4c2c-b021-509caf216d8b)\n\nFor example, I might be interested in everything that [[P- Joel Chan]]writes. The query might look like:\n ```javascript\n(all: (written-by:\"Joel Chan\"))```\n\nBut maybe I'm only interested in his writing that pertains to synthesis. Then I might adjust my query:\n ```javascript\n(all: (written-by:\"Joel Chan\")\n      \"synthesis\")```\n\nMaybe I want to follow everything written by a politician that pertains to my industry so I can stay up to date on potential regulations. Since we enable composable queries, the query could look as simple as this:\n\n```clojure\n(all: (property: \"written-by\" list-of-politicians)\n      (any: \"Web3\"\n            \"crypto\"\n            \"cryptocurrency\"))```\n\nCompare this to the black box algorithms people traditionally see in social media newsfeeds. On Twitter, it is unclear what determines whether a tweet is displayed in your feed. It often features \"suggestions\" outside of who you follow based on its perception of your interests.\n\nAt its simplest, a newsfeed could be considered an aggregate of your saved queries. Since queries are composable, that might look like:\n```clojure\n(def \"followed people\"\n  (any: \"John Doe\"\n        \"Jane Buck\"))\n(def \"followed interests\"\n  (any: \"Web3 thoughts from politicians\"))\n(def \"blocked list\"\n  (all: \"Joe Rogan\"))\n(def \"newsfeed\"\n  (any: \"followed people\"\n        \"followed interests\")\n  (not: (any: \"muted authors list\"\n              \"muted topics list\"\n              \"muted items list\")))\n```\n\n ## [[Interfaces for adjusting a query]]\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-Workspaces-as-a-primitive":{"title":"I- Workspaces as a primitive","content":"Authored By:: [[P- Rob Haisfield]]\n\n[[Q- What are powerful primitives for a user of a decentralized knowledge graph]]\n\n[[C- Workspaces enable users to pick up their old context and investigation where they left off]]\n\n- #[[Twitter thread]] [[P- Rob Haisfield]]: ü§ØÔ∏è Freeze by @jasonyuandesign @tylerangert  @szymon_k seems incredible! I've been talking for a while in snippets about saved workspaces as a primitive in tools for thought, but with Freeze bringing the concept to desktop, it's time for a thread! üßµÔ∏è https://freeze.app/ [*](https://twitter.com/RobertHaisfield/status/1359542159838502913)\n  - Imagine you're writing a paper or a technical spec in a Word Doc. You work until you're halfway done, and then save it to finish later. You also have a few notes open as reference material. You can re-open the Word Doc later, but you reopen the ref material separately. [*](https://twitter.com/RobertHaisfield/status/1359542160794730496)\n  - What if you wanted to have your reference material open every time you reopen the Doc? If you're only saving one file at a time, then the best you could do is maybe write down your ref material in a checklist for next time. [*](https://twitter.com/RobertHaisfield/status/1359542161767886857)\n  - I posit that most knowledge work is not contained in a single document. You don't just want to re-immerse yourself in the work, but also the context of your work. [*](https://twitter.com/RobertHaisfield/status/1359542163021914115)\n  - On my hypertext garden, every page is interconnected with others, so it's genuinely hard to edit and release one note at a time. Everything is autosaved, but [[Obsidian|Obsidian]] also lets me save and reload arrangements of pages for later. This helps me manage my mental stack. Obsidian's workspace model does feel clunky though - ideally, we would be able to create and destroy workspaces in an instant, form relationships between workspaces as independent entities, [[Q- What implicit metadata can be gathered from the structure of a workspace|form relationships based on coexistence within a workspace]], and search within the contents of a workspace. https://t.co/rLVzjc7Zku [*](https://twitter.com/RobertHaisfield/status/1359542180264747008)\n    - ![](https://pbs.twimg.com/media/Et4QZVxXcAcEmWD.jpg)\n    - ![](https://pbs.twimg.com/media/Et4QaAKXEAsa6UQ.jpg)\n  - Of course, [[Org-mode|Emacs]] did it first (tm). Emacs lets you save your current session to a file, which you can search for and restore later. https://t.co/PFMdBMw6M5 [*](https://twitter.com/RobertHaisfield/status/1359542200397365252)\n    - ![](https://pbs.twimg.com/media/Et4QbL7WQAc7wFp.jpg)\n  - [[P- Iian Neill]] has an interesting variation where each workspace can contain multiple file types (text, media, websites...). The workspace is an entity on a graph, as well as all of its contents. This turns workspaces into a core organizational structure. https://twitter.com/RobertHaisfield/status/1326186101053140992?s=20 [*](https://twitter.com/RobertHaisfield/status/1359542202343583749)\n  - [[Codex OS]] is also doing something really cool where you can see a little preview of the contents of each workspace. Codex has the most powerful implementation of workspaces I have seen so far. https://twitter.com/codexeditor/status/1341374670331756547?s=20 [*](https://twitter.com/RobertHaisfield/status/1359542203685732356)\n  - ![[Pasted image 20210923193416.png]]\n  - [[WorldBrain Memex]] lets you save all of your browser tabs to a collection and then you can full text search their contents. Collections aren't restricted to this purpose, but they can be used like this. https://t.co/YdOURtdK6U [*](https://twitter.com/RobertHaisfield/status/1359542237722533888)\n    - ![](https://pbs.twimg.com/media/Et4Qc_oWQAA80Pk.jpg)\n    - ![](https://pbs.twimg.com/media/Et4QdacWgAQdXEC.jpg)\n  - What makes Freeze cool is that it can save your workspace in a way that spans multiple applications and allows you to search their contents. It's not out yet, but I'm really curious to follow its development üëÄÔ∏è [*](https://twitter.com/RobertHaisfield/status/1359542239811280903)","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/I-dont-want-my-tool-to-tell-me-how-to-think":{"title":"I don't want my tool to tell me how to think","content":"\nAuthored By:: [[P- Brendan Langen]], [[P- Rob Haisfield]]\n\n\n### Advanced tools can be seen as too opinionated.\n\nOpinionated tools for thought can feel restrictive at the point of entry, limiting the user's ability to work at the speed of thought.\n\nOne participant told us that Roam Research was too opinionated when she tried it. You need a course to use it properly and end up indoctrinated into the Roam way of thinking. Her way of thinking is different, so there was a consistent mismatch. In her words, she just wants to inundate her notes with crap but have it all searchable.\n\nAnother participant told us that too much inferred structure from his tools slows him down because that would force him to think about structure. For example, Roam derives its structure from indentation and wikilinks. If he had to think about that while he was brainstorming, it would disrupt his flow. He far prefers the agnostic structure of Miro and its infinite canvas because he can just structure by drawing without worrying about if it will fit properly into his \"system\" later.\n\nInferred structure necessarily encodes assumptions about what user behavior means. Not every user shares those assumptions.\n\nThe learning curve of most advanced tools adds friction to adoption. [Roam Research‚Äôs Daily Notes paradigm](https://forum.obsidian.md/t/using-daily-notes-as-a-convert-from-roam/15393) feels significantly different from Obsidian‚Äôs blank page format. Further, if someone has a system already in place in another tool, the opportunity cost to migrate their structure is high.\n\nUltimately, this comes down to the fact that people are different, and the stage of work we‚Äôre in drives the tools we want to use. Some of us want a zoomable canvas. Others want an outliner. Even more of us want to move between these interfaces and a blank page structure, depending on what the current task demands of us. We operate in different contexts over different stages of work, and while our tools are moving towards flexible interfaces, there is still too much of a one-size-fits-all approach in current user interfaces.\n\nExpert users tend to prefer tools that either share their opinion or are neutral enough to enable them to impose their opinion onto the tool, and not vice versa.\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/If-Im-thinking-too-hard-about-structure-before-I-write-the-note-I-forget-what-I-was-going-to-write-down":{"title":"If I‚Äôm thinking too hard about structure before I write the note, I forget what I was going to write down","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n\nThe title says it all. We spoke with multiple participants who generate ideas constantly, and if they don't write them down immediately, the idea disappears. This points towards the necessity to design low friction [[quick capture]] and overall reduce the requirement to structure information upfront.\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Interfaces-for-adjusting-a-query":{"title":"Interfaces for adjusting a query","content":"\nAuthored By:: [[P- Rob Haisfield]]\n\n\ntags: [[Q- What are powerful interfaces for entering information into a discourse graph]]\n\nNot all edits to the query have to be within the query editor. In [[I- Provide Push and Pull as inline syntax to affect the related items section for a page]], we outlined how `push` and `pull` work as an inline DSL to add to a related items query's `any` clause. \n\n[[C- User behavior within a well-designed choice architecture can be a signal of preferences]]. We can see how this happens with [[C- Newsfeed management can enable users to express their preferences through a combination of revealed preferences and declared preferences|newsfeed management mechanisms]]. As people are looking through the results of a query, they might right click then select the option: \"remove from query.\" The menu path might look like the following, where each of the topics could be toggled:\n - ```clojure\n{:remove {:itemID sladfkji4af4sdkf\n          :author \"John Doe\"\n          :topic-mentioned [\"topic1\"\n                            \"topic2\"\n                            \"topic3\"]}}```\n\nWhen people click on an \"x\" on the query result's item, the [[smart default]] could be to remove that block's UID from the query through `(not (any: uid1 uid2))`.\n\nImagine \"economic inflation\" were a topic mentioned in an item within your newsfeed query and you right clicked to remove that topic. This might translate into the following:\n```clojure\n(def muted-topics-list\n  (any: \"economic inflation\"\n        \"veganism\"))\n(def muted-items-list\n  (any: uid1\n        uid2))\n(def newsfeed\n  (any: followed-people\n        followed-interests)\n  (not: (any: muted-authors-list\n              muted-topics-list\n              muted-items-list)))```\n\nAlternatively, users might process the list of query results through highlighting and lowlighting. [[C- Highlighted and lowlighted search results map to how well results map to intentions]], so the default assumption could be that lowlighting a block would remove that block from the query results. A block id would then have a `queries-where-highlighted` and `queries-where-lowlighted` property, listing the relevant queries.\n\nIt's possible that users don't always want to reify an entirely new query as an entity, and sometimes would like to simply filter a query. Given the compositional nature of the language I'm proposing, it should come as no surprise that we can apply a filter function to any list of query results.\n ```clojure\n(query:\n  newsfeed\n  (any: \"behavioral economics\"\n        \"web3\")\n  (not: \"regulation\"))```\n\n[[Roam]] has a GUI for manipulating these filters that can go beyond the language interface described above. For example, below, I'm filtering all linked references to `[Rob Haisfield](Rob%20Haisfield)` for `[CLM](CLM)` (claims) and `[EVD](EVD)` (evidence) that are connected.\n\n![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fwrite-hypertext-notebook-graph-research%2FRwZUCu20fg.png?alt=media\u0026token=314cf155-c8ec-4904-a051-8cb6b4496269)\n\nIf written in my proposed query syntax, this would look like the query below. An `all:` operator is assumed at the beginning of the query.\n ```clojure\n(query: \"Rob Haisfield\"\n        \"CLM\"\n        \"EVD\")```\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/It-takes-too-much-work-to-create-structure":{"title":"It takes too much work to create structure","content":"In current tools, structuring ideas in real-time is an extraneous task that _takes you out of the flow of the work_ you're doing. Often, people don't even know how to predict how they will reuse the note in the future!\n\n-   Deciding what to name something in the moment has a cost; people are removed from the current train of thought.\n-   In a file-folder format, deciding where to place a note adds another layer of friction.\n-   This occurs again when trying to retrieve past thoughts through search or block references.\n\nFor some experts, the primary concern is speed. As described in [[C- Synthesis is hard to do with people who don‚Äôt share context with you]], experts prefer to work on their own or with people who already share context **because** their ability to move fast depends on omitting the basics.","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Jump":{"title":"Jump","content":"Authored by:: [[P- Rob Haisfield]]\n\n[[Feedback for Chris Granger after we discussed Jump]]\n\n![[Pasted image 20211222123954.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Kanopi":{"title":"Kanopi","content":"Authored by:: [[P- Brendan Langen]]\n\nKanopi is a multimodal tool for knowledge management that provides different views for thought through linked notes, maps, and trees. Kanopi allows users to define relationships between nodes to create meaning, speaking to the idea of [[I- A structural editor for composing relationships between files or blocks]]. Kanopi has pre-loaded web integrations with Wikidata and Semantic Scholar that appear in search results.  \n\nSolo built by [[P- Brian James Rubinton]]. \n\n![[Pasted image 20210916113356.png]] - linked notes\n![[Pasted image 20210916113423.png]] - maps\n![[Pasted image 20210916113506.png]] - trees","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Knovigator":{"title":"Knovigator","content":"Authored by:: [[P- Brendan Langen]]\n\nKnovigator allows users to build atop other comments and threads in the form of quests. \nTheir system uses quadratic voting on each block in order to indicate how important or relevant it is. \n\nKnovigator uses the vocabulary of threads as quests. Threads are lists of short, related posts around a topic (i.e. bitcoin mining). They refer to these short posts as messages, which can be re-used as blocks in other threads. This also allows the ability to add evidence to reinforce or refute claims. \n\nKnovigator also enables user queries to turn into quests, which can later be linked to related threads. \n[[Q- How might you allow people to query information without explicit knowledge of how that information is structured]]\n\n[[C- An ideal decentralized knowledge graph would map a social graph and a knowledge graph]]\n\n\n![[Pasted image 20210916104840.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Kosmik":{"title":"Kosmik","content":"Authored by:: [[P- Brendan Langen]]\n\nKosmik positions itself as a visual representation for data, following along with other recent networked tools for thought. Kosmik draws personal parallels to whiteboards and art boards as a note-taking app. \n\n[[R- Kosmik App a visual tool for thought with an IPFS database]]. A principle of Kosmik is visual-first that gives the user an ability to zoom in, out, and across their digital space - which they call the Kosmik Universe. This suggests [[I- Workspaces as a primitive]] as a future design element to the discourse graph. \n\nBuilt by [[P- Paul Rony]]\n\n![[Pasted image 20210916111811.png]]\n![[Pasted image 20210916112255.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Learning-Loop-Complex":{"title":"Learning Loop Complex","content":"Authored by:: [[P- Brendan Langen]] and [[P- Joel Chan]]\n\nAs seen in [[R- The Cost Structure of Sensemaking]] -0\nIntroduces the [[Learning Loop Complex]] and idea of encodon as bridge between data and representations (schemas) that are then used for downstream tasks during sensemaking.\n\n![[Pasted image 20210928155633.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/LiquidText":{"title":"LiquidText","content":"Authored by:: [[P- Brendan Langen]] and [[P- Joel Chan]]\n\nLiquidText is an annotation tool alongside PDFs that enables [[active reading]]. LiquidText's simple interface has made it a favorite among Mac and iPad users, including collaboration for synthesis in larger groups. \n\nThe tool was described by its founder, [[P- Craig Tashman]] following his PhD research at Georgia Tech in [[R- LiquidText A Flexible Multitouch Environment to Support Active Reading]]. LiquidText allows to \"pull out\" excerpts and make pointers to context, and use these units on a canvas to weave together a larger understanding, albeit in a less formal fashion\n\n-   [https://www.liquidtext.net/](https://www.liquidtext.net/)\n\u003e Every project in LiquidText creates a document pane and infinite workspace. The document pane imports and contains all relevant documents, including live user annotations, highlites and connections. Notes, ideas, excerpts, images and connections are written, typed and/or placed on the workspace.\n\n\u003e LiquidText makes gathering notes and excerpts frictionless; users do it naturally, pulling out references as they read and dropping them on the workspace. LiquidText allows the user to organize the workspace as they prefer making notes and connecting them to others in the form of a list of notes and/or as mind maps.\n\n![[Pasted image 20210927161814.png]]\n\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Literate-programming-interface":{"title":"Literate programming interface","content":"","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/LogSeq":{"title":"LogSeq","content":"Authored by:: [[P- Brendan Langen]]\n\nLogseq is a hypertext enabled tool for knowledge workers that allows block-based referencing, a la [[Roam Research|Roam Research]]. Their aim is to be a privacy-first, open-source knowledge base, built in outliner form. \n\nLogseq visually looks like Roam, as well. A community of developers has begun building on advanced features into the ecosystem.\n\n![[Pasted image 20210916105759.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Mermaid-diagrams":{"title":"Mermaid diagrams","content":"Authored by:: [[P- Brendan Langen]]\n\nMermaid is a markdown syntax to generate diagrams, flowcharts, and other visualizations. This is a form of [[end-user programming]], as users can form their thoughts as a variety of visual processes. \n\nhttps://mermaid-js.github.io/mermaid/#/\n\n![[Pasted image 20210928164047.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Muse":{"title":"Muse","content":"Authored by:: [[P- Brendan Langen]]\n\niPad tool for thought created by [[Ink and Switch]]. Muse is a spatial canvas, with interactions specifically designed on the iPad. \n\nMuse allows annotations (or ink) on anything within a given board, and affords benefits of [[ZUI|zoomable interfaces]].\n\nhttps://museapp.com/\n\n![[Pasted image 20211007153804.png]]\n\n## A critique written by Rob from November 14th, 2020\n\n-   Okay, so I spent some time to git gud. I thought that I couldn't accurately judge it until I did that.\n    -   There's probably still a good amount of stuff that I'm not doing yet, especially stylistically, but they do onboarding through an instruction manual so I think I get it functionally at least.\n    -   Overall, it's cool, but it's just an interesting UX veneer on established concepts. Excerpts is cool though. Really, I just feel as though their main innovation is on stuff that doesn't matter to me.\n    -   The biggest problem is that it's essentially a file system with no good way to cross folder paths. It mixes drawing and personal knowledge management, but I don't think it's great for PKM.\n        -   Each board is both a folder and a file, and you can place boards within boards.\n        -   Each board is only as big as your iPad screen size. You can't zoom in or out at all to fit more information on the board.\n            -   Due to this limitation, if you want a board to contain more contents, you need to make new boards and refactor the information on your current boards into those smaller nested boards.\n                -   **The problem is that this just buries information.** Instead of having everything visible, it requires multiple taps to find what you're looking for.\n                -   Additionally, each board can only exist in one place and there's no way to embed pointers in other areas.\n                    -   For example, I was reading a chapter in a book about experiment design in political psychology:\n                        -   An observation that is reinforced at multiple points is that \"Awareness of being observed can change behavior.\" I have a board with that title. That claim originally came up in the context of me making arguments in a board called \"Against Lab Experiments,\" so it's nested within there.\n                        -   I have another board called \"Field Experiments.\" The book made the argument that a unique strength of field experiments can be that the participants are unaware that they are being observed by researchers. I want to be able to transclude my \"Awareness of being observed can change behavior\" board there so I can play around with those ideas in a different context.\n                        -   All that is allowed to me by Muse is one single path to \"Awareness of being observed can change behavior\" when already there should be two. I have to enter an exact sequence of boards to find the specific one that I'm looking for. I have to organize my boards according to a taxonomy rather than trains of thought.\n                        -   Roam is another Tool for Thought application that has shown possible solutions to these problems.\n                            -   Hyperlinks allow you to cross folder trees. They are wormholes between locations.\n                            -   block reference allow you to move ideas around nondestructively.\n                                -   \"Essentially, a block reference is a visual clone of the original block that you can move into new locations. If you change the text in the original block, it updates all of the block references as well.\" \"Block references link back to the original context. This allows you to cross folder trees.\"\n                                    -   Essentially, a block reference is a visual clone of the original block that you can move into new locations. If you change the text in the original block, it updates all of the block references as well.\n                                    -   Block references link back to the original context. This allows you to cross folder trees.\n                                    -   In Muse, I would love to be able to \"block reference\" the \"Awareness of being observed can change behavior\" into multiple locations, allowing the idea to proliferate and be discoverable in its appropriate contexts.\n                                        -   These block references would also allow me to nondestructively play with my boards. If each board is making a specific claim, then I want to be able to spread them out like notecards on a table and move them around. If I wanted to do this currently, I would need to remove the board from its original context.\n                                -   You can compare block references to copy paste. If you copy paste one idea into multiple locations and update the original idea, then every pasted instance stays the same unless you manually update it.\n    -   No map.\n        -   We've established that a person's Muse instance is going to be a whole set of nested boards. The only way to navigate between boards is to zoom in on one board, zoom in on another, etc,. For any serious amount of quantity, this creates a ton of steps to navigation. I have no idea how somebody would do it.\n            -   Just look at this example of me trying to navigate my thoughts around this one paper!\n            -   If we're going to go with nesting folders as the main information architecture, then we at least need to be able to visualize it.\n                -   Board titles could show a file path to give you a hint of what would happen if you zoom out.\n                -   We could have 2D Navigation like Mac Finder has\n                    -   ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2FRobAndHisNotes%2Fs9R0yTlpt1.png?alt=media\u0026token=fd4174e6-8a21-46e6-b4d0-b79e5e8fa400)\n                -   If we want spatial navigation to still be the primary way of moving around, then we could zoom out really far to see a bird's eye view of everything. The only level of zoom we can have at any point is at the level of a single board.\n        -   Alternatively, you can search for information, but focused search with a search bar is not enough.\n            -   \"Search Is Not Enough - Synergy Between Navigation and Search.¬†When websites prioritize search over navigation, users must invest cognitive effort to create queries and to deal with the weak implementations of site search. [https://www.nngroup.com/articles/search-not-enough/](https://www.nngroup.com/articles/search-not-enough/) search behavior Resource\"\n    -   Dumb UX things\n        -   I can't resize a selection, only boards and cards\n        -   I can't select and resize handwritten text\n        -   There's no duplicate or copy paste\n        -   Certain interactions are just weird. Pinch to zoom in and out is an established interaction that makes sense to people. Right now you do that to zoom in and out on boards, revealing how they are nested. But if I want to resize a card, I can't? I have to pull from the corner.\n        -   I have to manually select whether I'm inking or highlighting. There should be a gesture for this, right now it's inconsistent with the general modelessness of \"I'm touching the screen with one hand and writing with another to determine what I'm doing.\"\n        -   I just think that Concepts App has a little bit better of a UX for manipulation! I would base a Muse UX on the following\n            -   Use your finger to make a selection\n                -   Could be modified for Muse so if you tap a specific card then it automatically selects it, letting you move it around with the same ease that Muse allows currently\n            -   Use your Apple Pencil to draw\n                -   Still can use the same two handed operations for modelessness","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/My-tooling-gets-in-the-way-of-working-at-the-speed-of-thought":{"title":"My tooling gets in the way of working at the speed of thought","content":"\nAuthored By:: [[P- Rob Haisfield]], [[P- Brendan Langen]]\n\n\nStructural affordances in current tools conflict with our desire to work at the speed of thought.\n\nPeople want to work at the **speed of thought**, but current tools don‚Äôt allow for this to occur when one‚Äôs aim is to reuse and build off past thoughts. \n\nIn current tools, structuring ideas in real-time is an extraneous task that __takes you out of the flow of the work__ you're doing. Often, people don't even know how to predict how they will reuse the note in the future! Deciding what to name something in the moment has a cost; people are removed from the current train of thought. In a file-folder format, deciding where to place a note adds another layer of friction. This occurs again when trying to retrieve past thoughts through search or block references.\n\nIf people have already developed elaborate systems, then the requirement to structure as you go can feel restrictive and stressful. For example, if I'm on my phone and want to capture an idea, but I have a highly structured system, then it will simply take longer to navigate to the correct locations and link to the correct pages because I went from 10 fingers to 2 thumbs.\n\nIf a tool doesn't have a great mobile version, this problem is exacerbated. We frequently heard from our interviews that quick capture of an idea on the go was vital to their process. If a tool doesn't have a great mobile version, this problem is exacerbated. We frequently heard from our interviews that quick capture of an idea on the go was vital to their process.\n\nBecause [[It takes too much work to create structure|structuring notes in the moment is costly]], the design implications are that thought processors should either lower the cost of structuring or give users the ability to structure in hindsight.\n\nThose who avoid explicitly collating and naming ideas can work at the speed of thought and avoid premature convergence. They may prefer to specify relationships implicitly. Yet, if the tool is unable to recognize those relationships, this makes it harder to retrieve relevant materials to think through later, either with yourself or others. In these cases, our tools don‚Äôt let us easily refactor, leading to high friction in reifying our ideas. There also may be an upper limit on the complexity of unintentional structure people can create. \n\nSome thought processors attempt to give their users **structure for free** by implicitly structuring the data from user behavior. These methods promise to make structuring ideas an intrinsic task. For example, outliner tools like Roam and Logseq embed implicit AND/OR relationships between blocks through indentation. Obsidian and other networked notebooks with wikilink autocompletion allow users to name and categorize ideas on the fly. Voiceliner allows you to signify the relative importance of notes by dragging your finger up and down the screen as you record. Jump creates a semantic graph in the background for you and is able to infer relationships and metadata to lighten the user's information entry requirements. \n\nHowever, some users feel restricted by tools that implicitly infer structure from the way that users write because the design is necessarily opinionated. For example, one participant told us that Roam Research was too opinionated when she tried it. You need a course to use it properly and end up indoctrinated into the Roam way of thinking. Her way of thinking is different, so there was a consistent mismatch. In her words, she just wants to inundate her notes with crap but have it all searchable.\n\n Another participant told us that too much inferred structure from his tools slows him down because that would force him to think about structure. For example, Roam derives its structure from indentation and wikilinks. If he had to think about that while he was brainstorming, it would disrupt his flow. He far prefers the agnostic structure of Miro and its infinite canvas because he can \"structure\" by drawing without worrying about if it will fit properly into his \"database\" later.\n\nWe have yet to see affordances for easily structuring unstructured content in hindsight, but we can look to user behavior for ideas to solve this problem. For more, read [[I- Enable composable queries to facilitate structure in hindsight]] and explore its general context, as well as [[I- Replace the backlinks section of a page with a related items section]]. Generally, users can write search expressions that semantically describe what they're looking for, label those searches, and then functionally compose them in order to enable efficient aggregation going forward.\n\nOf course, there are problems of app performance, as well. Certain data structures have proven to be more resilient as scale increases. We won't touch extensively on this topic, but as users want to [[work at the speed of thought]], software that slows us down is a major drawback.\n\nTools for thought performance test at different graph sizes. \n\thttps://www.goedel.io/p/tft-performance-interim-results?r=14n5r4\u0026utm_campaign=post\u0026utm_medium=web\u0026s=r\n","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/NVivo":{"title":"NVivo","content":"Authored by:: [[P- Brendan Langen]] and [[P- Joel Chan]]\n\nNVivo assists with qualitative data analysis by identifying patterns in data and returning insights to the user. [[C- Scholars repurpose qualitative data analysis software to facilitate synthesis]].\n\n\n![[Pasted image 20210921165447.png]]","lastmodified":"2022-05-20T00:31:25.907404943Z","tags":null},"/Napkin":{"title":"Napkin","content":"Authored by:: [[P- Brendan Langen]]\n\nNapkin is built by Fabian Wittel and David Felsmann, and it employs a spatial canvas to help users traverse their thoughts and make spontaneous connections. \n\nWorking tour below, using the example of a public Napkin they built, working through Farnam Street podcasts. \n\n\n\n![[Napkin - Main Screen.png]]\n\nFocusing on a given tag shows any related ideas:\n![[Napkin - Tag Focus with Related Ideas.png]]\n\nCollection happens spatially, with all product use building around swarms of ideas. \n![[Napkin - Swarms of ideas forming a collection.png]]\n\nNapkin promotes retrieval of your ideas by surfacing related concepts and reminding you what you haven't seen recently. \n![[Napkin - Surfacing Related Concepts.png]]\n\n[[trace provenance]] in Napkin by hovering the source, which is required for any card - ![[Napkin - Source Provenance.png]]\n\nNavigation is entirely spatial, and while it doesn't allow for [[ZUI]], the experience feels like flying through a canvas. Napkin promotes exploration by showing notes like the one below: \n\n![[Napkin - Spatial Browsing + Islands.png]]","lastmodified":"2022-05-20T00:31:25.919404955Z","tags":null},"/Nextjournal":{"title":"Nextjournal","content":"Authored by:: [[P- Brendan Langen]]\n\nNextjournal is an example of a [[Literate programming interface]], as seen in [[R- clojureD 2021- Command and Conquer- Learnings from Decades of Code Editing by Philippa Markovics|this demo]]. Nextjournal uses a context-driven model that enables users around context-first thinking, via convenient UI abstraction, context-aware glossary of features, and a balance between easy to discover (mouse usage) and expert usage (keybindings, flows).\n\n\nContext is critical for displaying UI elements at the correct time, so Nextjournal provides users with a command bar at the bottom of each page, like this. \n![[Pasted image 20211217121037.png]]\n[[C- Tools for thought are popular when it feels like they get to know you better over time]], and Nextjournal aims to enable and foster an environment of power users. Below are the different contextual keybindings that are highlighted in a command bar, located at the bottom of each screen. \n![[Pasted image 20211217121152.png]]\n![[Pasted image 20211217121225.png]]\n![[Pasted image 20211217121241.png]]\n\nInformation is gathered across all of the states a user may be operating in.\n![[Pasted image 20211217120935.png]]\n\nOnce a keyboard shortcut is used, a pallette is generated with specific context on where you are: \nIn a rich text context, the focus:\n![[Pasted image 20211217121348.png]]\n\nIn a code editing context, keybindings will tie to the specific language you're writing in (i.e. Clojure will operate differently than Typescript).\n![[Pasted image 20211217121328.png]]\n\nThe overall feel is a tool that allows novices to gain a deeper understanding of their environment, while affording expert users with advanced functionality to improve their programming experience. \n","lastmodified":"2022-05-20T00:31:25.919404955Z","tags":null},"/Notation3":{"title":"Notation3","content":"Authored by:: [[P- Brendan Langen]]\n\nNotation3 is a shorthand RDF model, designed for human readability, developed by [[P- Tim Berners-Lee]] and [[P- Dan Connolly]].\n\nhttps://www.w3.org/TeamSubmission/n3/\n\n\u003e The Resource Description Framework (RDF) is a general-purpose language for representing information in the Web.\n\u003e This document defines _Notation 3_ (also known as _N3_), an assertion and logic language which is a superset of RDF. N3 extends the RDF datamodel by adding formulae (literals which are graphs themselves), variables, logical implication, and functional predicates, as well as providing an textual syntax alternative to RDF/XML.","lastmodified":"2022-05-20T00:31:25.919404955Z","tags":null},"/Notational-Velocity":{"title":"Notational Velocity","content":"Authored by:: [[P- Brendan Langen]]\n\nNotational Velocity is a Mac OS [[Open source]] note storage and retrieval app based around advanced search functionality. In their model, the same interface is used for creating notes and searching, which enables related past thoughts to generate alongside typed content. \n\nNotational Velocity's structure implies an evergreen or permanent note taking structure similar to [[Zettelkasten]] or [[P- Andy Matuschak]]'s approaches.\n\nCreated by: [[P- Zachary Schneirov]]\n\n![[Pasted image 20210918112435.png]]","lastmodified":"2022-05-20T00:31:25.919404955Z","tags":null},"/Notion":{"title":"Notion","content":"Authored by:: [[P- Brendan Langen]]\n\nNotion is a block based DSL for file and relational database information entry and visualization.\n\nThe keywords and functions of Notion's language are executed through slash commands, which can essentially be thought of as autocomplete for common functions.\n\nNotion has the ability to embed other websites directly into blocks. As an application, it does not need to do everything because it is interoperable with other web applications. Here we see that people are able to access their REPLs from Replit directly from within Notion.\n\n![[Notion-Replit-Integration.gif]]\n\nhttps://www.notion.so/Intro-to-databases-fd8cd2d212f74c50954c11086d85997e#344dc1cf493b4c8aa16ade3f40e24f5a\n\nIn Notion, you have many different views of the same underlying relational databases. You can edit or view them as Tables, Calendars, Boards, Lists, Galleries, and Gantt Charts. This means Notion provides multiple structural editors for relational databases. \n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-vLeXjO3aKU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/Notional-Model-of-Sensemaking":{"title":"Notional Model of Sensemaking","content":"Authored by:: [[P- Brendan Langen]] and [[P- Joel Chan]]\n\nAs seen in [[R- The sensemaking process and leverage points for analyst technology]], the\n[[Notional Model of Sensemaking]] in intelligence analysis has a looping structure that loops both between and within foraging and sensemaking loops, and progressively increases in both structure and effort, starting from raw data sources and culminating in a synthesized set of hypotheses. \n\n![[Pasted image 20210928155724.png]]","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/Obsidian":{"title":"Obsidian","content":"Authored by:: [[P- Brendan Langen]]\n\nObsidian is a networked hypertext notebook, in which this literature review is published. Obsidian has a strong community, growing rapidly from interested PKM folks, [[Roam Research| Roam Research]] expats, and . Obsidian focuses on local first storage, enabling a higher level of privacy/security than competitors like Roam. \n\nThe base form of files is .MD, which affords accelerated community development. [[C- End-user programming enables the developers to be lazy about their backlog of feature requests]]\n\n![[Pasted image 20210916112351.png]]\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/Org-mode":{"title":"Org-mode","content":"Authored by:: [[P- Brendan Langen]]\n\nOrg-mode is a [GNU Emacs](https://www.gnu.org/software/emacs/ \"A Lisp Machine which masquerades as an editor\") major mode for convenient plain text markup ‚Äî and much more. It's designed for PKM and task management, as an open source system. \n\nhttps://orgmode.org/\n\nDeveloped in early 2000's, with focus on enabling reproducibility with literate programming patterns. Full system described in [[R- A Multi-Language Computing Environment]]\n\n![[Pasted image 20210916110008.png]]\n![[Pasted image 20210916110038.png]]\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/Org-roam":{"title":"Org-roam","content":"Authored by:: [[P- Brendan Langen]]\n\nOrg-roam is a major mod on top of [[Org-mode|Org-mode]], with a goal of creating a Zettelkasten, but finding the vanilla version wasn't enough. Org-roam attempts to replicate [[Roam Research]] in the Org-mode format.\n\nThis speaks to the [[Hackers]] culture of modifying tools to fit personal uses, but it's expanded beyond, with over 3800 'stars' on Github as of 2021-09-15.\n\nhttps://blog.jethro.dev/posts/introducing_org_roam/\n\n![[Pasted image 20210915134459.png]]","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Alexander-Griekspoor":{"title":"P- Alexander Griekspoor","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Allen-Wilsonn":{"title":"P- Allen Wilsonn","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Andy-Matuschak":{"title":"\u003c% tp.file.title %\u003e","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Anne-Laure-Le-Cunff":{"title":"P- Anne-Laure Le Cunff","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Aosheng-Ran":{"title":"P- Aosheng Ran","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Brendan-Langen":{"title":"P- Brendan Langen","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Brian-James-Rubinton":{"title":"P- Brian James Rubinton","content":"\ncontext: building https://kanopi.io/\n\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Chris-Granger":{"title":"P- Chris Granger","content":"","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Conor-White-Sullivan":{"title":"P- Conor White-Sullivan","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Craig-Tashman":{"title":"P- Craig Tashman","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-David-Holz":{"title":"P- David Holz","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Elzr":{"title":"P- Elzr","content":"\nhttps://twitter.com/elzr/status/1378821500246065154\n\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Filip-Stanev":{"title":"P- Filip Stanev","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Flancian":{"title":"P- Flancian","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Geoffrey-Litt":{"title":"P- Geoffrey Litt","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Gordon-Brander":{"title":"P- Gordon Brander","content":"","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Iian-Neill":{"title":"P- Iian Neill","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Joel-Chan":{"title":"P- Joel Chan","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Kevin-Lin":{"title":"P- Kevin Lin","content":"","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Matt-Goldenberg":{"title":"P- Matt Goldenberg","content":"\n\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Max-Krieger":{"title":"P- Max Krieger","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Maxim-Leyzerovich":{"title":"P- Maxim Leyzerovich","content":"","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Oliver-Sauter":{"title":"P- Oliver Sauter","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Paul-Roney":{"title":"","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Paul-Rony":{"title":"P- Paul Rony","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Rich-Hickey":{"title":"P- Rich Hickey","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Richie-Bonilla":{"title":"P- Richie Bonilla","content":"\ncontext: Working on [[Clarity]] [clarity.so](http://clarity.so/) A ready-to-use productivity suite for remote teams\n\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Rob-Haisfield":{"title":"P- Rob Haisfield","content":"","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Ryan-Murphy":{"title":"P- Ryan Murphy","content":"Twitter: https://mobile.twitter.com/ryanjamurphy/with_replies","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Ryan-Singer":{"title":"P- Ryan Singer","content":"","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Stian-H%C3%A5klev":{"title":"P- Stian H√•klev","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Tiago-Forte":{"title":"P- Tiago Forte","content":"\n\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Tom-Critchlow":{"title":"P- Tom Critchlow","content":"","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Tudor-Girba":{"title":"P- Tudor Girba","content":"","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Vitalik-Buterin":{"title":"P- Vitalik Buterin","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Yochai-Benkler":{"title":"P- Yochai Benkler","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/P-Zachary-Schneirov":{"title":"P- Zachary Schneirov","content":"\n","lastmodified":"2022-05-20T00:31:25.931404967Z","tags":null},"/Pollen":{"title":"Pollen","content":"\nAuthored By:: \n\nPollen is a domain-specific language written in Racket that lets you [tag information with functions](https://docs.racket-lang.org/pollen/third-tutorial.html#%28part._.Tags___tag_functions%29). So, ‚óäem{text} will apply a function that applies the \"italicize\" to \"text\" as an argument. For a basic overview, [see this presentation](https://www.youtube.com/watch?v=IMz09jYOgoc).","lastmodified":"2022-05-20T00:31:26.507405549Z","tags":null},"/Programmable-text-interfaces-are-the-future":{"title":"Programmable text interfaces are the future","content":"Authored By:: [[P- Rob Haisfield]]            \n\n# **Programmable text interfaces are the future, not GUIs**\n\nPeople who don‚Äôt code are accustomed to interacting with apps with Graphical User Interfaces (GUI) [^1]. In order to give instructions to a GUI app, users need to click on buttons/menu items, click-and-drag blocks, and write in text boxes. GUIs are the dominant interaction design paradigm for non-coders [^2].\n\nWithout code, people need to be comfortable with whatever pre-existing features they can find, and if they want something custom, they will need to hire a coder or use a graphical no-code tool that will ultimately require them to use the same coding concepts.\n\nCoders will open a text file with an Integrated Development Environment (IDE) [^3]. Their IDE will provide helpful features (graphical or textual) to debug, refactor, and generally support writing more functional code. Coders give software instructions by writing in text what they want. Generally, code is flexible enough that they can make it do anything they want.\n\nWhile working on the onboarding for GuidedTrack [^4], I began to think‚Ä¶ What if people could interact with apps like coders interact with their programs? What is the difference between clicking a button to call a function and writing code to call a function? **In the future of text, I foresee programmable text interfaces with custom IDEs replacing graphical applications.**\n\nGuidedTrack [^5] is a simple low-code application that allows you to make surveys, experiments, web applications, online courses, signup forms, and more. While the use cases are broad, here I will primarily focus on the form builder side of it for simplicity. As one of my projects with Spark Wave [^6], GuidedTrack is the best example I know of a textual interface, so I‚Äôll dive into it and extract takeaways as I go.\n\n![](file:////Users/roberthaisfield/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image002.jpg)\n\n**The keywords are direct and domain-specific.** If you were to describe a survey in an email that you send to a collaborator, how would you do it? That‚Äôs not going to be far off from how it looks in GuidedTrack. This makes it feel like a **_textual interface_** for an app people are already familiar with. **Users write their instructions with text at the level of abstraction they care about.**\n\nStraightforward and direct language design is crucial. This quote from A Small Matter of Programming [^7] stuck out to me: \"A mathematician, in a broad sense, already knows Mathematica.\" Wolfram Mathematica formally encodes pre-existing mental models / notation into a syntax that can be used for computing.\n\nEach line represents a function (marked by the keyword at the beginning of the line), and relationships are conveyed through indentation. In order to add a caption to an *image, you indent a caption under the *image. In order to ask a follow-up *question, you indent the follow-up underneath specific answers, which are indented under the question. Saving responses to use for later is also done through indentation. **The syntax is simple.**\n\nIn user research, we found that non-coders often believe that in order to write code, they need to spend significant time reading documentation upfront. However, in GuidedTrack we have a toolbar filled with forms that enable you to write code through a graphical interface. All the user needs to do is fill in the blanks, toggle options on or off, and select options from menus. **The toolbar enables users to learn as they go and write code before they know how.** This is more or less equivalent to the graphical interfaces people are used to in Google Forms, but the difference is that once you get used to it you can just type.\n\nFor example, in this image, we see a form for writing a question. This multiselect checkbox type question will save the answers to a variable labeled `participantSubscriptions.` Later, these saved answers can be referenced in a **conditional** (e.g. \"if Netflix in participantSubscriptions, ask this question\") or a **loop** (e.g. \"Ask the same followup question about each of the answers they selected\"). There are forms for those as well!\n\n![](file:////Users/roberthaisfield/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image004.jpg)\n\n**_Loops_** **and _conditionals_ fall under the broader category of ‚Äúcontrol flow.‚Äù** By default, content (text, questions, charts, images, etc.) are read as instructions, flowing top to bottom. If that feels restrictive, loops and conditionals can declare other rules. A new user can learn how to replicate Google Forms‚Äô functionality in 10-15 minutes without requiring control flow. However, upon learning to use control flow, they will not want to return to the limitations of GUI alternatives.\n\n**Simply adding variables and control flow to a form builder gives people the ability to create their own functionality. Users don‚Äôt need to wait for developers to implement a specific feature.**\n\nOur competitors will advertise that they can do calculations as though that is a noteworthy feature! These calculations are used to score users' responses (in a quiz, for example). In GuidedTrack, calculations are emergent from the way saved variables work. In the case of a quiz, it would look something like: `totalScore = totalScore + lastAnswerScore` after each question.\n\nAnother example: an alternative form builder, Tripetto, is able to take the above question and ask how long the respondent was subscribed to each of the streaming services. The developers of Tripetto needed to specifically design that feature. It takes 20+ clicks (besides typing) and ~3 minutes to do it. That will never get faster.\n\nGuidedTrack doesn‚Äôt have specific functionality for this. We don‚Äôt have a long set of steps. However, control flow makes it possible in 15 lines of code. Most of that code is the same text Tripetto users would have to write anyway. Although it is more advanced than what beginners will need or know how to do [^8], many serious knowledge workers, like course creators or PhD researchers, will need a tool that can scale with their changing goals over the course of a whole career. **GuidedTrack, as programmable text, can handle the increasing demands of users with changing goals** [^9]**. The feedback loops** [^10] **built into the design of its editor enable users to increase their skills as they go** [^11]**.**\n\n**Textual interfaces are as fast as typing.** If writing a question, its answers, and the loop with the follow-up question is 60 words, then it will take you 1-1.5 minutes. **When the syntax feels natural, creating feels like writing.** Most people who use computers know how to type, and have had to use Google or Word documents. The keyboard shortcuts people are familiar with (command-a to select all, command-c to copy, command-v to paste, etc.) all work in a text editor because text editors have worked this all out already! If you try to copy/paste multiple questions at a time in a graphical application like Typeform, you can‚Äôt. If you try to undo an accidental change with command-z, you can‚Äôt. **Graphical applications have to specifically build keyboard shortcuts that text editors can do by default.**\n\nOftentimes, writing and reading is simply more pleasant than dragging and dropping. **Text is incredibly information dense.** Look at the difference between the same survey in GuidedTrack vs. Tripetto. With Tripetto, you would need a massive screen to work with a long survey!\n\n![](file:////Users/roberthaisfield/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image006.jpg)\n\n_![](file:////Users/roberthaisfield/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image008.jpg)_\n\nA key problem that confronts domain specific languages attempting to replace standard graphical applications is that they can be intimidating to non-coders. **People who don‚Äôt know how to code will look at anything that looks remotely like code as something that is ‚Äúfor coders.‚Äù** They assume that programming anything requires years of learning and prior experience. For the most part, they are not wrong. The sort of work you hire developers to do often does require specific knowledge.\n\nHowever, we are not trying to be a general purpose programming language like Python or JavaScript that can write anything. We‚Äôre just trying to replace a creation-oriented application with programmable text and a custom IDE. If you have to make a programming language that only makes surveys and experiments, it does not need to be complex. Let‚Äôs say that again more broadly: **If you have to make a programming language that only does ______, it does not need to be complex.**\n\n**Non-coders don‚Äôt need to think about deployment or backend boilerplate.** In order to send the program to someone, they only need to send a link. The program doesn‚Äôt require any code to save all of the responses to a spreadsheet, because why should someone trying to write research have to think of that? All of this complexity is abstracted away for our end users.\n\n**User research clearly demonstrated that novice users felt more comfortable when they could look at code and predict what it would do.** Obviously, that meant we needed users to see simple programs upfront. Unfortunately, we also learned that too many code comments to explain the code actually made code scarier! It is as though that sent the signal that there was more to explain.\n\nOur solution here was to implement a ‚Äúsplit view.‚Äù In the split view, we show users what code _(on the left)_ is actively producing what is displayed in the preview _(on the right)_. Originally, you would need to read code first and then view the preview. With the split view, instead of requiring users to read code and then preview it, they can do both at the same time. This tightened the feedback loop dramatically and enabled us to remove most code comments as they became unnecessary [^12]. Further usability tests were like night and day - we were far less likely to hear the ‚ÄúI‚Äôm not a coder‚Äù protest. **Legibility is key, and the simple design of the language is complemented by the IDE‚Äôs design.**\n\nIn HCI research, the split view is sort of like a live programming environment, in Bret Victor‚Äôs terms [^13]. We still have substantial work here to improve the experience, but for more, Sketch-n-Sketch [^14] and their research represents the state of the art.\n\n![](file:////Users/roberthaisfield/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image010.jpg)\n\nTo sum up, here is what we have so far that can be generalized beyond GuidedTrack:\n\n‚óè The familiarity and speed of text\n\n‚óè The user delivers instructions through text instead of buttons\n\n‚óè Powerful programming concepts like variables and control flows to unlock customized use cases\n\n‚óè Boilerplate code is abstracted away so the user can focus on creation within their domain\n\n‚óè Strong cognitive scaffolding (via graphics or other means) to make sure that it is approachable and people learn as they go\n\n# What other graphical applications could have a programmable text as an interface?\n\nGuidedTrack is awesome, but the point of this piece is to show a new kind of application design. There is no shortage of possibilities here. In no particular order:\n\n1.  **Task managers and schedules.** What better way to express that starting one task depends on finishing another than writing a _conditional_ if-then statement? Isn't a repeating event just an event on a _loop_ until it‚Äôs canceled? Isn‚Äôt it nice to be able to write your todos as though they were on a notepad as opposed to _navigating through a GUI_? With a text editor, it would be easy to copy/paste a selection of tasks to move them into another project. With conditionals, you could create an event that only occurs if half the invitees RSVP yes. This would be more about primitive design than feature design.\n2.  **Diagrams and flowcharts.** See Flowchart.fun [^15] to see how indentation and notation can make flowcharts easy. In order to be more aligned with how I'm looking at it, it would require some level of programmatic control on top of the boxes, lines, and sets of boxes and lines.\n\n![](file:////Users/roberthaisfield/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image012.jpg)\n\n3.  **Configuration files.** With most graphical applications, if I‚Äôm using them on multiple computers or across multiple accounts, I will need to manually change the settings each time. If settings were run through text, it's easy to transport through copy/paste.\n4.  **Personal finance.** Many people do this with custom spreadsheets [^16]. You could also do this with a domain-specific language, it might look something like the image from Andrew Blinn [^17] or the pseudocode below. Notice how specific the functions are - there‚Äôs no boilerplate for users, so they can focus on providing instructions.\n\n```clojure\n(when (true? (unpaidInvoice? today))\n\n (if (\u003e paymentFund (value unpaidInvoice))\n\n (sendFund (value unpaidInvoice) USDC John Doe)\n\n :else (sendMessage Me (str \"check the payment fund, there isn't enough in it for John Doe\"))))\n ```\n\n5.  **Discourse/knowledge graph database entry, retrieval, and visualization.** This is one subject of my research with The Graph [^18].\n6.  **Slideshows.** The Racket lang Slides [^19] demonstrate this. With programmability, an editor could change the background image and font on multiple slides at once, or turn it into a choose your own adventure, HyperCard style [^20].\n\n![](file:////Users/roberthaisfield/Library/Group%20Containers/UBF8T346G9.Office/TemporaryItems/msohtmlclip/clip_image014.jpg)\n\n# The future is programmable text, not graphical applications\n\nMost people are accustomed to graphical interfaces. Those are fine for basic use cases, but as user goals become more advanced, graphics alone will not be enough. This is where programmable text comes in.\n\nText is fast, as fast as we can type. Text can express complicated control flow concepts and abstract repetitive processes to create functionality that the app's designer would never conceive of on their own. Thus, it‚Äôs more about the design of primitives than features. Our job as designers is to make sure people can predict the outcome of their code through communicative feedback loops [^21] and examples they can build on. Clear documentation and graphical scaffolding can help them begin. In order to create a powerful application with a low floor, wide walls, and high ceiling [^22], it is important to think about how people can learn as they go through. Graphics can help people get started, but text will take applications and their users to the next level. If the language is simple, creation will feel like writing. \n\nReferences\n\n[^1]: Wikimedia Foundation. (2021, September 26). _Graphical user interface_. Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Graphical_user_interface.\n\n[^2]: Angert, T., \u0026 Elkammash, M. (2020, March 5). _CLUI: Building a graphical command line_. Replit Blog. Retrieved from https://blog.replit.com/clui.\n\n[^3]: Wikimedia Foundation. (2021, September 29). _Integrated development environment_. Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Integrated_development_environment#:~:text=An%20integrated%20development%20environment%20(IDE,automation%20tools%20and%20a%20debugger.\n\n[^4]: Haisfield, R. (2020). _GuidedTrack_. Rob's Hypertext Notebook. Retrieved from https://robhaisfield.com/notes/guidedtrack.\n\n[^5]: GuidedTrack. (2020). Retrieved from https://www.guidedtrack.com/.\n\n[^6]: _We create software companies to solve important problems._ Spark Wave. (2021). Retrieved from https://www.sparkwave.tech/.\n\n[^7]: Nardi, B. (1995) _A Small Matter of Programming: Perspectives on End User Computing_. Retrieved from https://scattered-thoughts.net/blog/2016/06/17/notes-on-a-small-matter-of-programming-perspectives-on-end-user-computing/\n\n[^8]: Haisfield, R. (2020). _New users do not yet have the vocabulary to understand the app_. Rob's Hypertext Notebook. Retrieved from https://robhaisfield.com/notes/new-users-do-not-yet-have-the-vocabulary-to-understand-the-app.\n\n[^9]: Haisfield, R. (2020). _User goals change over time_. Rob's Hypertext Notebook. Retrieved from https://robhaisfield.com/notes/user-goals-change-over-time.\n\n[^10]: Haisfield, R. (2020). _Feedback loops are a more efficient method of communication_. Rob's Hypertext Notebook. Retrieved from https://robhaisfield.com/notes/feedback-loops-are-a-more-efficient-method-of-communication.\n\n[^11]: Haisfield, R. (2020). _User skill level increases over time_. Rob's Hypertext Notebook. Retrieved from https://robhaisfield.com/notes/user-skill-level-increases-over-time.\n\n[^12]: Haisfield, R. (2020). _Feedback loops are a more efficient method of communication_. Rob's Hypertext Notebook. Retrieved from https://robhaisfield.com/notes/feedback-loops-are-a-more-efficient-method-of-communication.\n\n[^13]: Victor, B. (2012, September). _Learnable Programming - Designing a programming system for understanding programs_. Learnable Programming. Retrieved from http://worrydream.com/LearnableProgramming/.\n\n[^14]: Ravi Chugh. (2021). _Fun, Funky, Functional: The Pursuit of Better User Interfaces for Programming_. Retrieved from https://www.youtube.com/watch?v=1gGd7pKSpRM.\n\n[^15]: _flowchart.fun_. Flowchart Fun - Online text to flowchart creator. (2021). Retrieved from https://flowchart.fun/.\n\n[^16]: Kryptik. (2021). _Portfolio tracker by Kryptik_. Portfolio Tracking Template. Retrieved from https://docs.google.com/spreadsheets/d/1qYLOAjzaIIcFLFw_j-P4yH0oOhYdy-CcmBItEc6--50/edit#gid=464576501.\n\n[^17]: Blinn, A. (2021, September 16). _Twitter Reply re: Programmable Transactions_. Twitter. Retrieved from https://twitter.com/disconcision/status/1438302534649667590.\n\n[^18]: Haisfield, R. (2021, July 13). _Twitter Announcement - Research with The Graph_. Twitter. Retrieved from https://twitter.com/RobertHaisfield/status/1415041842064756737?s=20.\n\n[^19]: Takikawa, A. (2018, March 31). _Making the Most of #lang Slideshow_. Asumu. Retrieved from https://www.asumu.xyz/blog/2018/03/31/making-the-most-of-lang-slideshow/.\n\n[^20]: Wikimedia Foundation. (2021, September 27). _HyperCard_. Wikipedia. Retrieved from https://en.wikipedia.org/wiki/HyperCard#:~:text=HyperCard%20is%20a%20software%20application,flexible%2C%20user%2Dmodifiable%20interface.\n\n[^21]: Haisfield, R. (2020). _Continuous onboarding_. Rob's Hypertext Notebook. Retrieved from https://robhaisfield.com/notes/continuous-onboarding.\n\n[^22]: Brander, G. (2021). _Low floor, wide walls, high ceiling_. Gordon Brander. Retrieved from https://gordonbrander.com/pattern/low-floor-wide-walls-high-ceiling/.","lastmodified":"2022-05-20T00:31:26.507405549Z","tags":null},"/Project-Description":{"title":"Project Description","content":"\nWe‚Äôre looking to answer a few key questions in our research:\n\n-   Which data structures would allow individuals to incorporate their personal knowledge into a decentralized knowledge graph?\n-   Which behaviors would the interface need to facilitate in order to integrate and maximize the accessibility and iterability of knowledge?\n-   How can we build upon implicit metadata for a frictionless user experience?\n\nMuch of the data on the web is unstructured or poorly structured. While there may be agreed-upon schemas under which people operate, it‚Äôs [a challenge to get users to input metadata](https://people.well.com/user/doctorow/metacrap.htm) in a consistent way. These pre-existing structures are implicit metadata ‚Äî the significance of which is revealed through user behavior. For example, the number of times a paper has been cited acts as an implicit measure of support for that particular paper. However, these citation counts miss critical information about whether citing authors supported or opposed the findings, or how claims in the paper play into the area of the discourse graph that authors are to contribute to. If we can identify and understand the rationale behind these agreed-upon structures, we can build upon them in a way that does not feel intrusive to the user and allows them to maximize their potential.\n\nCould the interface to the knowledge graph support positive user behaviors? For example, Twitter's 280-character limit promotes knowledge compression and unqualified hot takes. We first hope to understand how this would impact an individual user, but a more interesting question comes from the potential of this strategy to disrupt how we conceptualize data and iterate on a global knowledge base. The current body of research is [limited by its inability to be queried and updated](https://drive.google.com/file/d/1yjTjcIVqttXEA2NXKQ75D8iHGTOxiEj4/view). Nearly every graduate student has completed a review of the literature at some point, but their labor will likely go unrecognized unless it is published by a journal, leaving future researchers to start at the beginning. Can we create a decentralized knowledge graph that encourages synthesis through its design? Can this be developed while still maintaining an individual user‚Äôs privacy?\n\nOur success will be measured by our ability to accurately map out the space, utilize prior research, and understand the techniques and strategies employed by professional knowledge workers and those building tools for thought. We seek to gain an understanding of how implicit metadata already plays a role in these systems, and understand the strategies and searches that underlie these decisions. In the long term, we hope our research will be used in the creation of a collaborative, decentralized knowledge graph.\n\n[[Project success metrics]]","lastmodified":"2022-05-20T00:31:26.507405549Z","tags":null},"/Project-Mission-and-Impact":{"title":"Project Mission and Impact","content":"\n**\n\nThrough our research, we plan to generate insights and questions that enable the creation of decentralized tools for networked thought that improve the thinking of all participants. We hope to connect the research and knowledge of academics and practitioners across disciplines and professions, fueling interdisciplinary synthesis the likes of which the world has never seen before. In doing so, we will fight against the trend of technological discourse software that overwhelms us with noise we can't even trust and leaves us fried as we mindlessly scroll through it.\n\nOver the last three years of my (Rob Haisfield) career [in applied behavioral science, I have learned a ton](https://robhaisfield.com/notes/hyper-learning). Most of this learning came from reading papers and working with clients, and it has all gone into my personal notebooks. One of the most important realizations I have had as a practitioner is that the findings from academic research don't always map 1-1 with what works in practice. I have learned the boundary conditions of various theories and synthesized my own through attempting to apply the theory to real life, across multiple situations.\n\nHere's the thing: it's not just me. Every practitioner is learning a lot through their work, connecting the dots. Many of them use a personal knowledge management system in some form or another. Practitioners don't have a proper feedback loop to update the academic knowledge base, and barely even talk to each other. What would happen if we unleash the knowledge of individuals and groups into a decentralized knowledge graph built to facilitate synthesis? How can that technology and behavior be actualized? How can we [[R- Accelerating Scientific Discovery by Lowering Barriers to User-Generated Synthesis of Scientific Literature|lower the barrier to access, contribute, and arrange research?]] What structure can make knowledge graphs interoperable across fields of study and professions, and what will be necessarily domain-specific?\n\nThis is not a new mission. The semantic web has long eluded us, [largely due to problems of human behavior](https://people.well.com/user/doctorow/metacrap.htm). If we are to create a decentralized knowledge graph, we have to figure out a structure that doesn't break if individual people don't manually tag information consistently and honestly. In order for this to work, we need to figure out the right implicit metadata, and in order to do that we need to learn how people interact with knowledge graphs today. What behavioral conventions are necessary to create and maintain a decentralized knowledge graph at different scales so the edges and nodes are composable and able to propagate changes? We aim to learn what [[Q- What is the data structure of a graph built to facilitate decentralized knowledge synthesis|data structures and interfaces can facilitate]] people in this necessary knowledge work. ","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Project-success-metrics":{"title":"Project success metrics","content":"-   Success:\n    -   Research paper\n    -   Increased knowledge\n\n\n- What the project looks like:\n    - reading papers, desk research\n    - participant observation, by working with social knowledge management communities\n    - user interviews\n    - Synthesis and public hypertext wiki\n\n\n- Problem being solved:\n    - People need an interface to input information into the knowledge graph such that it is queryable\n    - People need to be able to develop their thoughts personally and collectively, at different scales\n    - The right balance between unstructured and structured knowledge must be discovered","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-Can-neural-networks-answer-queries-from-natural-language-without-a-predefined-schema":{"title":"Q- Can neural networks answer queries from natural language without a predefined schema","content":"[[Q- What is the role of AI in facilitating a decentralized discourse graph]]\n\nIn [[R- Neural Databases]], the authors present a tool, NeuralDB, that requires no schema and is queryable through natural language processing. \n\nPre-trained language models are performing at a high enough level of accuracy through [[NLP]] to begin experimenting with [[Q- What would a discourse graph without a predefined schema look like|a discourse graph without a predefined schema]]. If the field is specified, the model can improve even further by training on domain specific knowledge.\n\n\u003eThe first, and most important benefit is that a NeuralDB, by definition, has **no pre-defined schema**. Therefore, the scope of the database does not need to be defined in advance and any data that becomes relevant as the application is used can be stored and queried.\n\u003eThe second benefit is that updates and **queries can be posed in a variety of natural language forms**, as is convenient to any user. In contrast, a traditional database query needs to be based on the database schema.\n\u003eA third benefit comes from the fact that the NeuralDB is based on a pre-trained language model that already contains a lot of knowledge.\n\u003eFurthermore, using the same paradigm, we can endow the NeuralDB with more domain knowledge by extending the pre-training corpus to that domain.\n\nThis is best used in domains where knowledge is unclear. \n\nTo compare to other data structures - [[Q- What is the data structure of a graph built to facilitate decentralized knowledge synthesis]] - let's examine relational databases and graph databases. \n\nSince relational databases use predefined structures to store relationships, future changes to schema are more costly than in graph database structures, which store relationships at the individual level. \n\nIn fields where most knowledge is clear, or stable, changes to schema are less necessary. You can design a DB schema up front with greater confidence it will model the field. [[C- Structured writing is a method that can accelerate learning in stable fields]] \n\nIn contrast, if you have to account for changing information - in a field that's being forged, as you say - a graph DB is better because it can define relationships on each individual node. structured writing doesn't work as well in these fields.\n\nFurther, though, might a neural DB be a better model?\n\n\u003e Given its benefits, Neural Databases are well suited for emerging applications where the schema of the data cannot be determined in advance and data can be stated in a wide range of linguistic patterns.\n\nThe example of verifying the correctness of political claims is used, as information is constantly changing. \n\n\u003e Another class of applications is the modeling and querying of political claims [46] (with the goal of verifying their correctness). Here too, claims can be about a huge variety of topics and expressed in many ways. \n\nIt remains to be seen how effective neuralDBs are at updating information over time.\n\n\u003e Finally, another interesting challenge concerns developing semantic knowledge that helps in identifying which updates should replace previous facts and which should not.\n\nChallenges also exist around bias encoded in the model. \n\n\u003e A possible downside of using neural techniques in a database system is the potential for bias that might be encoded in the underlying language model.\n\u003e A possible approach to this important issue is to design a separate module that attacks the database with queries in order to discover hidden biases. ","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-Does-sufficiently-advanced-natural-language-processing-invalidate-the-need-for-a-structured-DSL":{"title":"Q- Does sufficiently advanced natural language processing invalidate the need for a structured DSL","content":"As made clear in [[I- A DSL for a discourse graph with information entry, visualization, and retrieval]], I believe syntax to be useful for creating a tool for thought. In my experience using [[Roam Research]], indentation and wikilinks essentially became my language for communicating with the computer that X relates to Y. Queries became my power tool, because I knew that given the way that I had written my outlines, the queries were a representation of my questions that Roam would understand, such that I knew it would pull up the correct answer.\n\nAt first glance, [[Jump]] might invalidate the idea of a domain specific language for thought. After all, why require the user to learn a specific syntax in order to communicate with the computer, when the computer can understand what you mean simply from your natural language?\n\nHowever, the computer will never have a perfect understanding of your natural language. The computer will make its best attempt and show you the outcome. There needs to be an [[intermediate interface]] that shows you what the computer understood so you know how it came to its conclusion. This is where I believe a domain-specific language comes into play, as it will be able to express with higher precision and fine grained control what the computer understood than natural language.\n\nJump does an impressive thing where it takes your natural language query, translates it into propositional logic, and then shows you the result. The incredible part is that it allows you to edit the logic so you end up with a query that more accurately reflects your intent. Jump derives [[smart default]] logical relationships from natural language that will get it right most of the time, but enables you to edit for the 20% of the time it's incorrect.\n\n![[CleanShot 2022-05-17 at 18.51.18.png]]","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-can-people-maintain-a-decentralized-discourse-graph-with-a-high-quantity-of-information-in-it":{"title":"Q- How can people maintain a decentralized discourse graph with a high quantity of information in it","content":"Authored By:: [[P- Rob Haisfield]]\n\nIf everybody comments in the discourse graph, then comments become useless. If everybody produces content, then it's harder to find the right content.\n\nAs discussed on [[Q- What community roles are necessary in a decentralized knowledge graph]], curators and educators will play a valuable role in reducing the sheer quantity of information people need to keep up with.\n\nExisting solutions like Twitter try to solve this problem through a newsfeed. However, [[C- Newsfeeds are a poor intervention for distributing and discovering relevant information]]. Hypertext is often better, but not the only solution.\n\nProgressive summarization is one response to this as a way to [[C- It will be important to capture the potential energy of information consumption|capture potential energy of information consumption]].\n\n[[C- An ideal decentralized discourse graph would enable people to view information at different levels of granularity through a ZUI]].\n\nAlternatively, search operators could be helpful, such as a way to only view things between certain dates. See [[I- Search as a part of the primitive design]] for more discussion.\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-do-explorer-communities-attract-a-hacker-community":{"title":"Q- How do explorer communities attract a hacker community","content":"[[Q- How might we facilitate healthy social dynamics with hacker communities and typical end users]]\n\n[[C- End-user programming enables the developers to be lazy about their backlog of feature requests]]","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-do-explorer-communities-grow-around-software-tools":{"title":"Q- How do explorer communities grow around software tools","content":"","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-do-we-increase-the-frequency-of-social-review":{"title":"Q- How do we increase the frequency of social review","content":"Authored By:: [[P- Brendan Langen]], [[P- Rob Haisfield]]\n\nA challenge in [[Q- How can people maintain a decentralized discourse graph with a high quantity of information in it|maintaining a decentralized discourse graph with a high quantity of information in it]] is the amount of time required, without a strong incentive system in place. This is an open space to explore.\n\nAs [[C- Curation is an important role in maintaining a decentralized discourse graph|curation is an important role in maintaining a decentralized discourse graph]], \n[[Q- How do we increase the frequency of social tagging behaviors|how do we increase the frequency of social tagging behaviors]]?\n\nChallenges also exist because [[C- It is easier for people to cite their own work than the work of others due to limitations of human memory|it is easier for people to cite their own work than the work of others due to limitations of human memory]]. \n\nTools like [[Hypothesis]] enable active reading through progressive summarization, which is a constructive behavior in discourse graphs. [[Extended Universe/C- Incrementally processing notes is a key user behavior to promote synthesis|Incrementally processing notes is a key user behavior to promote synthesis]]. [[C- Synthesis is supported by Active Reading|Synthesis is supported by active reading]]. ","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-do-we-increase-the-frequency-of-social-tagging-behaviors":{"title":"Q- How do we increase the frequency of social tagging behaviors","content":"[[C- Social tagging is a key user behavior to managing a decentralized knowledge graph]]\n\n[[C- Emoji reactions are a form of social tagging]].\n\nAt the same time we don't want to overload the discourse graph with a poor signal to noise ratio. If everybody comments, then comments are useless.","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-do-we-solve-the-problem-of-different-people-referring-to-the-same-concept-with-different-language":{"title":"Q- How do we solve the problem of different people referring to the same concept with different language","content":"Authored By:: [[P- Rob Haisfield]]\n\nOne time I was in [a conversation](https://twitter.com/andy_matuschak/status/1256294495919828992?s=20) with [[P- Andy Matuschak]] on Twitter, where we ended up having a conversation through our notes. He linked to a note about what we can learn from game design, and then I linked to a note about the learning curve that's commonly seen in puzzle games, and then he linked to a note about cognitive scaffolding. It was interesting because I had actually encountered his thought about cognitive scaffolding before, but I hadn't drawn the connection that we were talking about the same thing with different language! A knowledge graph that naively uses linkages to the same term wouldn't capture this connection.","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-do-you-allow-people-to-query-nth-degree-and-fuzzy-connections-without-overwhelming-the-signal-to-noise-ratio":{"title":"Q- How do you allow people to query nth degree and fuzzy connections without overwhelming the signal to noise ratio","content":"## Queries\n\n### Query for signal-to-noise\n\n```query\nblock:(signal noise)\n``````\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-might-we-allow-people-to-adapt-their-past-system-and-notes-to-current-needs":{"title":"Q- How might we allow people to adapt their past system and notes to current needs","content":"Authored By:: [[P- Rob Haisfield]]\n\nThis is closely related to the question: [[Q- How might we make it so it doesn't matter if user behavior is inconsistent]]. There is a fundamental tension with notes that are intended for your future self where it's impossible to know exactly what your future self will need all of the time... so you need to be able to refactor and [[structure in hindsight]].\n\nFor example, let's say that we suddenly decide \"Let's not categorize our notes with prefixes in the titles. Instead, let's do it with tags.\" How could someone refactor everything to current needs without manually going through everything one by one? [[Q- How can people maintain a decentralized discourse graph with a high quantity of information in it]]\n\nThere can be an intense amount of FOMO and friction that comes from thinking that you need to plan everything ahead of time. This is one of the problems with [[Notion]] - it's really hard to refactor systems so people feel like they can't start before they have it all figured out. It is a big friction for [[new users]].\n\nOne can reduce this friction through:\n\t- [[C- Incrementally processing notes is a key user behavior to promote synthesis]]\n\t- [[C- Reviewing past notes in the process of creating new notes is a key user behavior to promote synthesis]]\n\n[[C- End user programming enables people to bulk process notes]]","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-might-we-apply-map-filter-reduce-to-notes-and-what-other-primitives-are-relevant-to-this-domain":{"title":"Q- How might we apply map filter reduce to notes and what other primitives are relevant to this domain","content":"Authored By:: [[P- Rob Haisfield]]\n\n[[Q- What are powerful primitives for a user of a decentralized knowledge graph]]\n\nSome ideas: Write a query, save the results to a list. Process that list with map, filter, and reduce. Filter it down. Map some function onto each note in the list. Reduce the results into another page, or into the \"backlinks\" for a page. [[C- Bulk refactors are a necessary primitive to maintaining a decentralized discourse graph]].\n\nAs can be seen in [[I- Search as a part of the primitive design]], search is a powerful way to populate a list. One could map the append function onto a list of the search results in order to append a new tag onto each item. Alternatively, one could map syntax highlighting rules based on the results of a query.\n\n[[P- Conor White-Sullivan]] has an [interesting thread](https://twitter.com/Conaw/status/1134173307878629376?s=20) on this:\n\n\u003e In practice -- for notes Filter your notes for questions Map over your open questions and say -- how could I reframe this question to make it easier to answer -- what smaller or bigger questions help me answer this one -- how will I know I've found an answer\n\u003e Filter your notes for beliefs Map over them Ask -- How surprised would I be if this were false (\"weight\") -- what other beliefs do I think are true because this one is *what does this imply* -- What other beliefs, if I changed my mind about them, would change this one\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-might-we-create-a-decentralized-knowledge-graph-that-people-want-to-use":{"title":"Q- How might we create a decentralized knowledge graph that people want to use","content":"As a general rule for influencing behavior, [it is easier to facilitate people in doing something they already wanted to do but struggled with than it is to compel them to do something they did not want to do](https://robhaisfield.com/notes/behavioral-product-strategy). Many systems break down at the level of desire - if RDF already solves knowledge graph modeling questions, why hasn't the semantic web taken off? Many posit that RDF is awkward for people / developers to use.","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-How-might-we-facilitate-healthy-social-dynamics-with-hacker-communities-and-typical-end-users":{"title":"Q- How might we facilitate healthy social dynamics with hacker communities and typical end users","content":"Briefly - plugin ecosystems are awesome for end-users insofar as it allows users to create features that the core developers don't have time for. However, the plugin developers will often do this out of the goodness of their heart or out of a personal need for a specific plugin, leaving many users dry, especially if a plugin developer loses the energy or will to maintain the plugin over time. A plugin marketplace is an obvious answer, but this needs to be balanced against a desire not to prevent the community vibes coming from an intrinsically motivated system.","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-are-powerful-interfaces-for-entering-information-into-a-discourse-graph":{"title":"Q- What are powerful interfaces for entering information into a discourse graph","content":"","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-are-powerful-primitives-for-a-user-of-a-decentralized-knowledge-graph":{"title":"Q- What are powerful primitives for a user of a decentralized knowledge graph","content":"Authored By:: [[P- Rob Haisfield]]\n\n[The design of a tool for thought's primitives and its UX will influence the behavior of its users](https://robhaisfield.com/notes/every-app-is-designed-for-behavior-change,-intentionally-or-unintentionally). For example, [[Roam Research|Roam]]  designed its indentation and bidirectional link filtration to facilitate a Zettlekasten.\n\n\u003e When I realized that the way that filtering/queries work with indentation can lead users to nest blocks in basically the same way as a folgezettel I was like ‚Äú[@Conaw](https://twitter.com/Conaw/) you dog.‚Äù Tangential to what [@vgr](https://twitter.com/vgr/) is describing, but just shows how Roam excels at embedding philosophy in design [twitter.com/vgr/status/134‚Ä¶](https://t.co/BKE4DPfctY \"https://twitter.com/vgr/status/1346191600590864384\")\n\u003e Tweeted on [Jan 4, 2021](https://twitter.com/RobertHaisfield/status/1346197182383177729)\n![[Pasted image 20210916180319.png]]\n\n\nBacklinks are displayed by as a collapsible list. The backlinks do not just display the titles of the pages where a backlink is present: they also show breadcrumbs of context from what came before the relevant block. *see [[I- Search as a part of the primitive design]]*\n\nWhile you are looking through backlinks, you can filter them down by including and excluding items, which are sorted by their frequency.\n\n![[Pasted image 20210922151354.png]]\n\nOne could also argue that outliners push people towards divergence. In Roam, you can branch your thoughts, but it can be difficult to group them together. Sibling nodes are unrelated to each other, so it can be difficult to query for a block that contains a list of items nested beneath it. In its current form, it can only query one branch at a time.\n\nTake a look at the backlinks for [[Roam Research]] and [[Obsidian|Obsidian]]. You can expand and collapse the list of results in each.\n\n![[expand-collapse-roam.gif]]\n\nWhen there are too many results, you can filter to narrow the results\n\n![[Pasted image 20210914182735.png]]\n\n![[Pasted image 20210914182805.png]]\n\nUsers will have a primitive to write [[I- Inline data structures|inline data structures]]\n\nMy current thinking is that the most valuable set of primitives are basically: typed concepts, typed relationships, expressive user-generated data structures, and rules. I want to be able to write in blocks with inline data structures, arbitrary relationships between blocks, block referencing, inline querying, and inline coding.\n\nNLP, similar to [[Jump]]'s NLU, will enable users to implicitly instantiate complex mental models into the data structure. Expressive data structures (like triplets and nested tables) will enable people to manually specify structure when necessary. They should be expressive enough that many different visualizations can be made on top.\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-changes-in-a-discourse-graph-as-quantity-of-content-increases":{"title":"Q- What changes in a discourse graph as quantity of content increases","content":"Authored By:: [[P- Rob Haisfield]]\n\n#questions/core \n[[Q- What user behavior is required to grow a decentralized discourse graph]]\n\n[[Q- What user behavior is required to maintain a decentralized knowledge graph]]\n\n[[Q- How can people maintain a decentralized discourse graph with a high quantity of information in it]]\n\n[[Q- What community roles are necessary in a decentralized knowledge graph]]\n\n[[Q- What community values are necessary in a decentralized knowledge graph]]","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-community-roles-are-necessary-in-a-decentralized-knowledge-graph":{"title":"Q- What community roles are necessary in a decentralized knowledge graph","content":"Authored By:: [[P- Rob Haisfield]]\n\nAs discussed elsewhere, one of the most important questions to creating a decentralized discourse graph is the following: [[Q- What workflows and behaviors facilitate synthesis]]? We also acknowledge that [[C- People are lazy]], so if we can split up the work of synthesis, we should.\n\nIf we were to divide up the work of synthesis by roles instead of jobs, below are some possible roles:\n- Curator\n\t- Curates the fire hose of information down for people to keep up with the discourse in less effortful ways\n- Connector\n\t- Consumes information across many domains and forms new connections, addressing the question of [[Q- How do we solve the problem of different people referring to the same concept with different language]]\n- Educator\n\t- Understands the discourse and educates others so they can more meaningfully participate\n- New knowledge provider / creator\n\t- Contributes new information and beliefs to push the discourse forward\n- Debater / contrarian\n\t- Makes sure people don't get too caught up in a consensus that is unsubstantiated\n\nThis is one of our open questions going into [[Interview Guide|user interviews]]. #questions/core","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-existing-systems-facilitate-individual-synthesis":{"title":"Q- What (existing) systems facilitate individual synthesis","content":"","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-is-a-decentralized-discourse-graph":{"title":"Q- What is a decentralized discourse graph","content":"Authored By:: [[P- Joel Chan]], [[P- Rob Haisfield]], [[P- Brendan Langen]]\n\nOur research draws from a long line of information models, such as the Semantic Web Applications in Neuromedicine (SWAN) ontology [[R- The SWAN biomedical discourse ontology]], the Micropublications model [[R- Micropublications a semantic model]], the ScholOnto ontology for modeling scientific discourse [[R- ScholOnto an ontology-based digital library server for research documents and discourse]], the nanopublication model [[R- The anatomy of a nanopublication]], and the Hypotheses, Evidence, and Relationships (HypER) model [[R- Hypotheses Evidence and Relationships]]. These models share a common underlying model for representing scientific discourse: they distill traditional forms of publication down into more granular, formalized knowledge **claims**, linked to supporting evidence and **context** through a network or **graph** model. \n\nWe use the term **discourse graph** to refer to this information model, to evoke the core concepts of representing and relating knowledge claims (rather than concepts) as the central unit, and emphasizing linking and relating these claims (rather than categorizing or filing them). Standardizing the representation of scientific claims and evidence in a graph model can support machine reasoning ([[R- Genuine semantic publishing]]). We are particularly interested in the [[C- Discourse graphs could significantly accelerate human synthesis work | potential of discourse graphs to accelerate human synthesis work]]. \n\nSo what does it mean for a discourse graph to be decentralized?\n\nAs seen in [[R- Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking|R- Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking]], we see that there are many component parts to synthesis. Given that, [[C- The responsibilities required to produce synthesis can be split up among many people]]. [[P- Michael Karpeles]] refers to this concept as [human computation](https://twitter.com/mekarpeles/status/1440886235917164546?s=20)\n- Some people will contribute primary research. \n- Some will formalize the research contributed by others into frameworks and theories. [[C- Incrementally processing notes is a key user behavior to promote synthesis]]\n- Some will help determine what directions are meaningful to explore, ranking the utility of the outputs of others.\n- Some will simply read everyone else's outputs and annotate them, all the while meaningfully connecting the conversation between the various fields they explore. [[C- It will be important to capture the potential energy of information consumption]]. \n- Some will rate the contributions of those who annotate to improve the signal to noise ratio. As such, a key question as we go into the interview phase of our research will be: [[Q- What community roles are necessary in a decentralized knowledge graph]]?\n\nDividing the responsibilities of synthesis is one of the core strengths of decentralizing the process. When we look at prior attempts at building a semantic web, we find that the primary reasons have to do with human behavior and dishonesty.\n\n[[C- People are lazy]] and [[C- Most people will primarily consume information]], so we can't expect them to do all of the work necessary to index information themselves. While tools like [[Roam Research]] and [[Obsidian|Obsidian]] enable people to develop advanced discourse graphs for themselves, over time they may end up with a system that is so complicated that maintaining it becomes work. Why not split up the effort so the ones who share information aren't responsible for 100% of the processing?\n\nAnd if people can be dishonest, isn't consensus over a shared state of truth one of the key innovations of the blockchain?\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-is-a-hypertext-notebook":{"title":"Q- What is a hypertext notebook","content":"Authored By:: [[P- Rob Haisfield]]\n\nA hypertext notebook can be thought of as a mini-Wikipedia. It is a deeply entangled web of ideas that are changing constantly. Readers will take a different journey every time they visit, and pages that they have already read may take on new meaning when placed into new contexts.\n\nA key advantage of writing in hypertext is that it allows the writer to express their thinking and research in a way that is simultaneously more in depth and more concise. [[C- Hypertext enables communication with high information density]]. If you have already read a page and internalized its core idea, then all you need to do is see the title of that page to bring to mind the entire argument. At the same time, if something doesn't make sense, you can explore its context by navigating to the ideas that it is built on.\n\nAs such, writing a hypertext notebook is an ideal way to perform a literature review.\n\n[[R- Where the semantic publishing rubber meets the scholarly practice road]]\n\u003e Another example of Explorers‚Äô niche tools is a growing ecosystem of ‚Äúnetworked notebooks‚Äù, which are a particularly interesting category with deep intellectual roots in hypertext [7, 20]. A prominent example is RoamResearch ; others include TiddlyWiki and Obsidian.md. Scholars who use these tools create and maintain relatively atomic notes on concepts or some kind of focused claim (Compression). These notes are densely linked to each other (Composability), typically bi directionally: every time a link is made from one source note to a target note, both the source and target notes record the link (see Fig. 5). In this way, links between notes are more accessible, since links can be followed from either source or target notes. This, together with other affordances like autocompletion of links during text editing, enables easier tending to connections between notes (Composability). The links also enable users to compress quite complex ideas into a single statement (e.g., \"knowledge is contextual\") while retaining links to the less compressed ideas that \"unpack different aspects and subtleties of the more complex idea. In this way, tending to the notes and links also enhances the Contextualizability of each entry.\n\u003e Finally, since notes that collect bi-directional links can be as small as a single concept, the act of deliberately linking notes partially accomplishes the work of developing folksonomies (Composability). A key affordance in these networked notebooks is that it is quite easy to renam note titles, with changes automatically propagating throughout the database. This enables more agile and evolving folksonomies.","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-is-an-interface-for-going-up-and-down-the-ladder-of-abstraction":{"title":"Q- What is an interface for going up and down the ladder of abstraction","content":"Authored By:: [[P- Rob Haisfield]]\n\nSee how, over the next few minutes, I'm able to **model a thought process** of connecting the generic principles of Roguelikes to the specific context of using flashcards. What would be an interface that specifically facilitates creating these connections?\n\nhttps://youtu.be/HO_qvM-URHY?t=1352","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-is-synthesis":{"title":"Q- What is synthesis","content":"\nAuthored By:: [[P- Joel Chan]]\n\nWe use the term \"synthesis\" in this project to refer to [[C- Synthesis as a process is usefully modeled as a specialized form of sensemaking| a particular form of sensemaking]], where the *inputs* are practical, scientific, or scholarly theories, findings, and data, and the *output* is a coherent intellectual whole that can guide decision-making for complex problems with no clear playbook to follow, and/or spur conceptual innovation to advance the state of knowledge on a problem.\n\nSynthesis may be supported by and manifested in a variety of forms, such as a theory, an effective systematic or integrative literature review, a causal model, a cogent research proposal or problem formulation, or model of a design space, among others. The core commmon intuition is the **creation of a novel conceptual whole that is greater than the sum of its parts** [[R- Types of synthesis and their criteria]].\n\nTo illustrate the difference between a \"mere assemblage\" and a true synthesis, consider the difference between these two hypothetical 'summaries\" of a set of findings:\n\n| Example                        | Mere assemblage                                                                                                                                                                              | Synthesis                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n| ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Related work for systems paper | There are a few ways to solve problem X. One is [1,2]. People have also tried [3,4,5]. Finally, others have tried [6,7,8]. In this paper, we introduce our solution Y that solves problem X. | The earliest attempts to solve problem X took the angle of [1,2]. But it has limitations ABC. For example, [1] showed that... To address limitations ABC, others have explored the possibility of [3,4,5], which makes key modifications XYZ to . This yields some gains under settings DD, but doesn't work in settings EE. The overarching open problem seems to be the tradeoff between XXX (common in settings EE) and the side effect BB of . To address this open problem, researchers are exploring [6,7,8]. has shown some promise, yielding X% gains in settings EE; but a remaining open problem is YY. In this paper, we build on these efforts by addressing this key limitation. |\n| Darwin's theory of evolution   | Species vary: some variations are bad, and some help with survival. Species struggle to survive. Some, but not all, organisms pass on new offspring.                                         | Species struggle to survive. Species also vary, and some variations are good and some are bad for survival. Therefore, one precondition for species to survive and pass on offspring is by having or inheriting beneficial variations. This variation and selection process explains how we get the diversity of species we see today.                                                                                                                                                                                                                                                                                                                                                        |\n  \nIn both examples, mere assemblage simply lists an undifferentiated series of findings in succession. In contrast, a synthesis of the hypothetical set of findings in both cases, weaves them together into something greater than the sum of the parts: in the system paper example, a theory of change for what the most fruitful angle of attack is for making progress on a problem; in the latter example, a full-blown causal theory of the origin of species!\n\nYou would be surprised at how common this \"phone book\" mere assemblage pattern is in academic literature reviews, from doctoral dissertations, even ones that pass! ([[R- Making the Implicit Explicit]], [[R- Investigating PhD thesis examination reports]], [[R- Scholars Before Researchers]]), and even many published papers ([[R- A Troubleshooters Checklist for Prospective Authors]], [[R- Generating research questions through problematization]], [[R- Theory Before the Test]], [[R- A decade of theory as reflected in Psychological Science]], [[R- Using systematic reviews to inform NIHR HTA trial planning and design]], [[R- Cochrane and non-Cochrane systematic reviews]]). This is why we use the term synthesis instead of the more familiar \"literature review\": in today's conventions, only effective literature reviews are actually synthesis! We think this is [[C- Effective synthesis is necessary for innovation and scientific progress | harmful for scientific progress and innovation]], and are therefore motivated to build new knowledge infrastructures that could make effective synthesis less painful and more commonplace, such as a [[Q- What is a decentralized discourse graph|decentralized discourse graph]].\n\n- Some examples of interdisciplinary synthesis include:\n\t- Daniel Kahneman and Amos Tversky recognized a claim made by economists that people behaved rationally and questioned it. The rational actor model had long been assumed to be true in economics. They then studied the question through the lens of judgment and decision. Amos Tversky, always the militant debater, would challenge economists to rationalize their findings. This led to the beginning of a new field: **Behavioral Economics**\n\t\t- Much of the discourse in DisciplineA rests on AssumptionA. Participants in the discourse graph for DisciplineB call AssumptionA into question through the evidence and claims of GraphDisciplineB. \n\t\t- In the effort of refuting AssumptionA, they found the areas where DisciplineA and DisciplineB were [[Q- How do we solve the problem of different people referring to the same concept with different language|discussing the same questions with different language.]]\n- [[P- Vitalik Buterin]] et al. recognizing that the blockchain could support full programmability and innovating smart contracts through Ethereum. There were further acts of synthesis when specific futures were envisioned, like an alternative to DNS. \n- The scaling solutions in the plan for Ethereum 2.0, with Proof of Stake, Sharding, and Layer 2, is an act of synthesis. They combined many sources of information to solve a problem, emphasizing only the most important bits, where the solutions work together synergistically.\n- Cryptoeconomics is the combination of cryptography and economics for the goal of social coordination towards desirable outcomes. The realization that decentralization needs to come with an incentive layer was a key insight.\n\nSynthesis is necessary for innovation, and as outlined in [[C- Discourse graphs could significantly accelerate human synthesis work]], a decentralized discourse graph can dramatically reduce the work required to achieve synthesis.","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-is-the-data-structure-of-a-graph-built-to-facilitate-decentralized-knowledge-synthesis":{"title":"Q- What is the data structure of a graph built to facilitate decentralized knowledge synthesis","content":"Authored By:: [[P- Rob Haisfield]]\n\n- Logical\n\t- Claims\n\t\t- Subquestions\n\t\t- Subclaims\n\t\t- Supporting Evidence\n\t- Problems\n\t\t- Subproblems\n\t\t- Subclaims\n\t\t- Subquestions \n\t- Questions\n\t\t- Subquestions\n\t- Evidence\n\t\t- In academic literature, they will organize papers by their name, journal, authors, and year. This data structure is not directly designed for the purpose of promoting synthesis. Titles are often nondescriptive, and the paper itself will raise many claims and questions. If you find a citation, it is not entirely clear what the citation is referring to within the paper.\n- Technical\n\t-  Typed Relationships\n\t\t- What are common types of relationships?\n\t\t\t- Causality `caused-by` or `because of` or `due to`\n\t\t\t- Opposition or Support\n\t- Alternatives\n\t\t- [[version control]], or maybe [[I- I should be able to leave a hole to fill in the blanks for an idea or domain]] where I can specify what the characteristics are of a viable alternative\n\nA primary goal of this research is to uncover a data structure that facilitates synthesis. Synthesis is not always within the academic context. Here we see it in product development:\n\nLet's say that I'm trying to figure out the onboarding for a tricky app like GuidedTrack. I need to bring together a ton of potentially conflicting information! User interviews, papers I've read, stakeholder beliefs, emails... By default, this is difficult to synthesize because there is simply too much to read.\n\nA helpful architecture might enable me figure out what the main questions are, find claims related to those questions, follow the evidence supporting and opposing the claims I identify as interesting, and then rearrange those claims to form a fitting answer.\n\nThen, 2 months later when the onboarding plan is built and it does not perform well in usability tests, I would be able to track down the reasoning that led to the incorrect decision, re-evaluate the pillars with new evidence, and finally update the decision.\n\nIn order to facilitate synthesis, the data structure of the discourse graph needs:\n- **Composability.**\n\t- As seen in [[Q- What is synthesis]], people need to compose and remix existing information to form a new whole.\n- **Compression.** \n\t- Compression can be thought of as abstraction. A core thing that abstraction does is remove \"extraneous\" details, retaining only what is \"necessary.\" If one is trying to reuse old thoughts, reading too much information can be overwhelming. \n\t- Compression is about increasing the information density as much as possible. [[P- Andy Matuschak]] recommends using [[I- Search as a part of the primitive design|note titles as APIs to the whole idea]].\n- **Context.**\n\t- It can often be difficult to synthesize from the work of others if their thoughts are too compressed because you don't know they really mean. [[C- Compression and contextualizability are in tension]]. Without proper context, it is impossible to know what people base their conclusions on, or why people see some piece of information as important.\n\nFor more challenges awaiting synthesizers, see [[R- Knowledge Synthesis- a conceptual model and practical guide]].\n\nWe certainly do not have all of the answers to this question yet. It will be an active area of discovery over the coming months as we learn more about [[Q- What workflows and behaviors facilitate synthesis]] and [[Q- What user behaviors are people doing already that imply structure that is not being instantiated into a literal structure]].\n\nFor now, we believe a decentralized discourse graph will require support for:\n\n- Blocks of information\n\t- Claims\n\t- Subjective probabilities of truth, with people able to qualify the probabilities they assign\n\t- Boundary conditions\n\t- Questions\n- Inline coding\n\t- This would be able to indicate the logical relationships between blocks and to operate on blocks. End-user programming is crucial, as can be seen in [[I- A DSL for a discourse graph with information entry, visualization, and retrieval]]. \n\t\t- let me write sentences and indicate logical relationships between items. Equals, greater than, follows from, also, is a subcomponent of, etc. Maybe provide a logic for people to define their own relationships through a [parser](app://obsidian.md/parser) like [Instaparse](app://obsidian.md/Instaparse)\n- People\n- Typed Relationships\n\t- Ability to express:\n\t\t- Causality\n\t\t- Opposition or support\n- [[I- Search as a part of the primitive design]]\n- Breadcrumbs\n\t- Queues\n[[Standoff Annotation]], as seen in [[Codex OS]], is very appealing as an option. It seems as though it could be an alternative to HTML for tools for thought, as it enables users to annotate overlapping pieces of information and assign metadata to the annotations.\n\n- The data structure needs to contend with a few big problems:\n\t- [[Q- How can people maintain a decentralized discourse graph with a high quantity of information in it]]\n\t\t- [[C- Bulk refactors are a necessary primitive to maintaining a decentralized discourse graph]]. With a massive amount of information coming from many people, it would simply be too much work to update processes or curate content otherwise.\n\nSome initial beliefs:\n\n- Subquestions\n-\n\t- What are common types of relationships?\n\t\t- Causality `caused-by` or `because of` or `due to`\n\t\t- Opposition or Support\n- Evidence\n\n- The data structure of a graph built to facilitate decentralized knowledge synthesis requires:\n\t- Contextualizability\n\t- Composability\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-is-the-role-of-AI-in-facilitating-a-decentralized-discourse-graph":{"title":"","content":"[[Q- What user behavior is required to maintain a decentralized knowledge graph]]? Presumably, there should some balance between what is required of user behavior, what is required of the design of the system, and what AI can do to reduce the effort from user behavior.\n\n2022-04-12 reflections:\n\n- Today in random things brought to my attention on Twitter ( @balOS @joelchan86 you'll both get a kick out of this), Tau is a blockchain decentralized social network that uses logic based AI (similar to [[Jump]]) and program synthesis from user discussions to make updates to its codebase, allowing all participants in the discussion forums to participate in [Tau's](https://tau.net/) governance.\n- Supposedly they've figured out a way to use logic programming also to identify points of consensus so it isn't updating its code all over the place.\n- When you listen to them talk, you can hear them make an implicit point: if web3 is in part about being able to participate in the governance of the products/platforms you use, unless you expand programming capability to everybody, it's ultimately governed by the developers who make decisions about what to build. End-user programming is necessary to self / community governance.\n- https://twitter.com/TauChainOrg/status/1513777139451142146?s=20\u0026t=gVmIRmtS-DT3Cw_pDhf7Aw\n- I asked on Twitter about the conceptual fit between query languages like SQL and Datalog with personal and shared knowledge graphs and they responded.\n- Makes me also think - is there more of a conceptual fit between different types of AI and discourse graphs? I.e. logical AI (Jump, Tau) or probabilistic AI (GPT-3, DALL-E 2)?\n- I would bet that logical AI is a better fit because it's fully explainable. In the context of a discourse graph, you want to know exactly how it built up its answer. Architectural transparency is key.\n- Also, a logical AI might be able to do more from the relatively small amount of data produced by individual researchers in their own graphs. From our interviews, people seem to want AI that is tuned to their personal knowledge graphs, not necessarily from a global body of knowledge.\n- On the other hand, a probabilistic AI might be a better fit if you consider the \"zettelkasten as a conversation partner\" to be most important.\n- Additionally, with the discourse graph generated by thousands or millions of participants, idk which would be more effective - a probabilistic or logical AI.\n- I know a guy who has built a library (can't find it) called Frankenstein that is trained to look at black box probabilistic AI results and fabricate propositional logic explanations. Spitballing, a cool model might be:\n\t- Probabilistic AI to identify logical relationships from natural language\n\t- Logical AI built on top of that\n\t- Probabilistic AI to answer questions from a global discourse graph\n\t- Frankenstein to interpret those answers in terms of logical relationships and check to see if they are correct or where they might have logical breakdowns\n\nThoughts from [[P- Ryan Murphy]]:\n\nIn language processing/AI-ish stuff, I want to see:\n\u003e [[P- Ryan Murphy]]:\n\u003e - Find similar/related documents (as per DEVONthink) \n\u003e - Identify missing links (e.g., highlight important words/tokens that seem to appear significant and relevant elsewhere in your notes, too) \n\u003e - Suggested tags ... I thought I had more, but I'm forgetting 'im\n\n\u003e [[P- Joel Chan]]: \n\u003e love the idea of missing links! maybe a more powerful/fuzzy variant of aliases. \n\u003e\n\u003e suggested aliases for the current note. Would be super valuable. Prevents people from talking about the same thing using slightly different terms in different places\n\n\u003e [[P- Ryan Murphy]]: actually, and that's a fifth (or maybe the same one): redundancy detection. \"These two notes are so similar that you might want to merge them\"-type use cases\n\u003e you are always in control, but the system can help you with your tasks: finding things you want to merge, propagating changes, detecting missing links, etc.","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-synthesis-behaviors-must-be-done-by-an-individual-and-what-responsibilities-can-be-distributed-to-many-people":{"title":"Q- What synthesis behaviors must be done by an individual and what responsibilities can be distributed to many people","content":"","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-user-behavior-is-required-to-grow-a-decentralized-discourse-graph":{"title":"Q- What user behavior is required to grow a decentralized discourse graph","content":"","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-user-behavior-is-required-to-maintain-a-decentralized-knowledge-graph":{"title":"Q- What user behavior is required to maintain a decentralized knowledge graph","content":"","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-user-behaviors-are-people-doing-already-that-imply-structure-that-is-not-being-instantiated-into-a-literal-structure":{"title":"Q- What user behaviors are people doing already that imply structure that is not being instantiated into a literal structure","content":"Authored By:: [[P- Rob Haisfield]]\n#questions/core\n\nThis will be a primary research question of the interviews, as it is the sort of thing you need to see in action. User behavior will often reveal a structure that they did not know they had. For example, [people will use multiple colors to highlight sections of books](https://twitter.com/elzr/status/1373492338207698944?s=20), where the colors each mean something different. Roam is able to infer that the nesting and branching people do thoughtlessly as they write in outliners [is meaningful information for queries](https://twitter.com/elzr/status/1373492338207698944?s=20). People will use shorthand to communicate structure to themselves. How can we leverage what researchers and decision makers are already doing as they manage their knowledge?\n\n[[Q- What implicit metadata can be gathered from the structure of a workspace]]?\n\n[[Q- How can we form an implication graph for knowledge based on search and navigation behavior]]?\n\n[[Q- How is knowledge crystallized and disseminated in discussion oriented explorer communities]]?\n\n[[C- There is a wealth of creative exhaust generated by researchers that is going to waste]]\n\n[[Q- What causes people to start new documents as opposed to updating existing ones]]?","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-user-behaviors-are-scholars-doing-already-that-specify-structure-for-synthesis":{"title":"Q- What user behaviors are scholars doing already that specify structure for synthesis","content":"Authored By:: [[P- Joel Chan]], [[P- Rob Haisfield]]\n\nReferences bricolage, using a slipbox, synthetic notes, framework of qualitative analysis to literature reviewing. \n[[C- Effective individual synthesis systems (seem to mostly) exist (for a select few)]]\n[[Q- What (existing) systems facilitate individual synthesis]]\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Q-What-workflows-and-behaviors-facilitate-synthesis":{"title":"Q- What workflows and behaviors facilitate synthesis","content":"Authored By:: [[P- Rob Haisfield]]\n\nAs seen in [[R- Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking|R- Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking]], we see that there are many component parts to synthesis. Given that, it makes sense that the responsibilities required to produce synthesis can be split up among many people. This is one of the key strengths of decentralized applications.\n\nWith this question, we aim to learn how people synthesize information. Answering this will inform the creation of data structures and functionality that facilitate synthesis in a decentralized way. If we want the default outcome of participating in this discourse graph to be synthesis, the data structure should support people in going through the process. After all, it is generally easier to facilitate closing the intention-behavior gap than it is to motivate an entirely new intention.\n\nThis is the primary question we will be exploring over the course of the [[Interview Guide|user interviews]].\n\nFrom our exploration, we have a few starting points. \n\n- [[Search Behavior]]\n\t- People need to find information when they aren't quite sure what they are looking for or how to articulate what they are looking for. Before you have solved a problem or articulated the right framing for a it, focused search is not sufficient.\n- Tidying/Gardening\n\t- As people review their content, they make small changes and updates. This helps to ensure [[Q- How might we propagate changing beliefs throughout a network|changing beliefs are propagated throughout the system]]. \n\t\t- See [this interview](https://www.gamedeveloper.com/design/q-a-dissecting-the-development-of-i-dwarf-fortress-i-with-creator-tarn-adams) with Tarn Adams: ![[Pasted image 20211118174735.png]]\n- Braindump\n\t- People will dump all of their thoughts onto a page in order to figure out what they want to say. At this point, they need to work with a data structure that is fast and does not get in the way. [[C- Some users feel restricted by tools for thought with overly strict data structures]]. However, there is a [[structure now vs. later (uncertain payoff, regret)]] tradeoff here, where they braindump so many ideas that it becomes hard to work with in the future. This is perhaps where there is an opportunity to support [[C- Incrementally processing notes is a key user behavior to promote synthesis|incremental processing]], as [[C- Loosely structured notes are not a useful knowledge artifact for others]].\n\t- It is often the case that people will go through multiple stages of braindumping in succession. [[C- People will iterate and rewrite the same ideas in order to crystallize their thoughts]].\n- Sensemaking behavior\n\t- See [[C- Synthesis as a process is usefully modeled as a specialized form of sensemaking]], where we discuss how there are two primary loops: foraging (finding information) and sensemaking. For the interviews, we are primarily interested in sensemaking, as there are more pre-established design patterns for foraging.\n\t- Designers [[Q- What workflows and behaviors facilitate synthesis|will often]] use [affinity diagrams](https://www.nngroup.com/articles/affinity-diagram/), and we see variations of this in other fields, where people will collect many thoughts and then attempt to chunk them together into meaningful groups.\n\t- - How and why [---] and existing implementations (sub in each of the bullet points below for the blank space between brackets, as though [---] represented a variable)\n\t\t- people compose, compress, and expand information\n\t\t- people reuse information / interpretations\n\t\t- people prioritize information / interpretations\n\t\t- people connect information / interpretations\n\t- Progression through stages\n\t\t- People like to move ideas through stages. From seedlings to evergreen notes, from premise to draft, from draft to deliverable. When an idea has moved through all of those stages, they end up with [[Multiplicity]], where they have referred to similar concepts in many different ways. While they don't want to delete it all, they do want the canonical, published version to be what they refer to. [[C- People want to maintain multiple copies or iterations of the same information while recognizing contextual canon]].\n\t\t- Some people will write and rewrite their ideas multiple times in order to crystallize their thoughts. There is a big opportunity in supporting the process of rewriting ideas and identifying the themes that are consistent across iterations.\n\t- Prioritization\n\t\t- When people have many tasks or idea premises, they need to be able to sift through all of them and prioritize/signal which are the most important. One way to do that is by marking a note as worthy of progressing to the next stage. [[Knovigator]] does it instead by enabling quadratic voting on individual notes.\n\nAny work that we hook into for them to author a discourse graph should line up with workflows that help them. People like putting ideas into clusters. We want to hook into in progress work. People take courses, download exercise apps, etc. because they want to change but are struggling to do it on their own. \n\n[[P- Ryan Singer]] instantiates his brainstorming and planning workflow in the video here.\n\n{{\u003c tweet user=\"rjs\" id=\"1434934749731098630\" \u003e}}","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/Quotebacks":{"title":"Quotebacks","content":"Authored by:: [[P- Brendan Langen]]\n\nQuotebacks is a browser plugin that allows anyone to manage quotes. Users can add  quotes from articles or documents online to their own website, or save and manage quotes while reading online. \n\nhttps://quotebacks.net/\n\nCreated by [[knowledge-graphs-ux/People/P- Toby Shorin]] and [[P- Tom Critchlow]].\n\nLooks like this: \n![[Pasted image 20211021174307.png]]\nUX Pattern for create-your-own.\n![[Pasted image 20211021174138.png]]","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-A-Multi-Language-Computing-Environment":{"title":"R- A Multi-Language Computing Environment","content":"\n-   Title: A Multi-Language Computing Environment for Literate Programming and Reproducible Research\n-   Meta:\n    -   Tags: #ref/Paper\n    -   Authored by:: Eric Schulte , Dan Davison , Thomas Dye , Carsten Dominik\n    -   Year: [[2012]]\n    -   Publication: Journal of Statistical Software\n    -   URL: [https://www.jstatsoft.org/index.php/jss/article/view/v046i03](https://www.jstatsoft.org/index.php/jss/article/view/v046i03)\n    -   Citekey: schulteMultiLanguageComputingEnvironment2012\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-A-Short-Introduction-to-the-Underlay":{"title":"R- A Short Introduction to the Underlay","content":"Source:: https://notes.knowledgefutures.org/pub/underlay-short-intro/release/1\n\n**The Opportunity** \n\nHuman knowledge is an ever-expanding resource. Unfortunately, more knowledge does not necessarily lead to more understanding. Our methods for sharing knowledge are collapsing under the weight of distrust of science and journalism, false beliefs, misinformation campaigns, and deliberate fraud ‚Äînot to mention the sheer quantity of information. Think of how overwhelming it would be to read everything that has been said on a given topic, much less decide what of it is true. Fortunately, the information technologies that helped create these problems may also help us solve them. Eventually, intelligent software will enable us to filter, judge, and connect all public information. Here, we describe how we are making this possible by sharing public assertions of knowledge in a form that can be more readily processed by such software. \n\n- ==[[Q- What user behavior is required to grow a decentralized discourse graph|Sharing human knowledge involves]] selecting what is important, analyzing what is true, vetting the reliability of the source, and presenting the information in a form that is intelligible to the intended audience.== In traditional media, such as textbooks and scientific papers, all of this work is done in advance of publication, and there is no method for adapting to new knowledge or new contexts. In contrast, consider technology-enabled _**dynamic presentation**_, which continuously filters and updates knowledge. For example, while you are driving, a dynamic map makes you aware of changing traffic conditions and suggests alternate routes based on your priorities; this software is already in use. Now imagine how dynamic presentations could change publishing in science, education, or journalism. A scientific paper could reflect the latest experimental evidence. A textbook could adapt to the language, interests and reading level of the student. A news article could reflect the latest developments and filter out information that is no longer credible.\n\t- [[Q- How might we propagate changing beliefs throughout a network]]? Obvious answer is we use [[GraphQL]] queries for live updating data\n\n- \u003e ==To enable this kind dynamic presentation of all knowledge, we propose the creation of an open, machine-readable collection of all public knowledge ‚Äî including science, law, commerce, news ‚Äî called the _**Underlay**_.== The Underlay will gather knowledge currently used to produce publications, databases, and dynamically generated displays. It will make each associated assertion available in a machine-readable form that can be dynamically searched, vetted, and combined, based on its provenance. ==By connecting multiple sources together, each asserted claim can be analyzed for relevance and veracity, recombined and re-presented for different purposes. The Underlay will allow intelligent software to help people find what is relevant and judge what is true.==\n\t\t- [[C- Composability facilitates synthesis]]\n\n**The Underlay Project** \n\n==The creation of richer collections of machine-readable knowledge is inevitable. What is not inevitable is that such knowledge will be connected in a meaningful way and be freely available as a public resource.== We are at a fork in the road. The transition to machine-mediated access¬† could consolidate our dependence on a few large commercial intermediaries, or alternatively, it could be built as open infrastructure, as a public good.¬†\n\nJust as it took decades of work to weave the World Wide Web, building the Underlay will be a massive worldwide effort. Like the web, it will be built primarily by those who connect their information to the shared system for their own purposes. Our initial effort is focused on building what is required to enable that.¬†\n\n- ==This work is underway at MIT‚Äôs not-for-profit Knowledge Futures Group (KFG), with collaboration and funding by grants from individuals, foundations, and commercial partners.== We have already solved many conceptual problems related to the Underlay, such as:¬†\n\t-   how to represent complex assertions\n\t-   how to represent the Underlay in a way that is independent of human language\n\t-   [[Q- Can the blockchain be used to improve citation chains|how to indicate and verify provenance]]\n\t-   how to recognize when separate sources are making assertions about the same entity\n\t-   the technical architecture required to store and disseminate the Underlay\n\t-   how to protect the Underlay from damage by imposters, spammers and other bad actors¬†\n\n\nWe are now just beginning to build the software tools that implements these ideas. We plan to test and refine these tools by applying them to use cases in a few specific areas of knowledge, such as scientific information about COVID-19, that are of interest to our early sponsors. Fields of coverage will expand over time, eventually linking many topics and formats of published knowledge. Our goal is to build a sound framework that can grow to a distributed, industrial-scale effort.¬†\n\n**How the Underlay works** \n\n- ==The Underlay is not truth, but assertions of truths attributed to their source. For example, the assertion ‚Äú_Nur-Sultan is the capital of Kazakhstan,_‚Äù might be linked to the provenance: ‚Äúfrom _Version 2011.1 of the UN Membership database_._.._‚Äù The provenance would also describe when and how the database was extracted and recorded into the Underlay. All assertions and provenances would be stored in easy-to-process machine-readable form, and through chains of relationships, interconnected into a web of knowledge.¬†== \n\t- [[Q- What is the data structure of a graph built to facilitate decentralized knowledge synthesis]]\n\n- \u003e Assertions in the Underlay are _**relationships**_ between _**entities**_. In the example, _Kazakhstan_ and _Nur-Sultan_ are entities, and _is-the-capital-of_ is a relationship[[RDF Triplets|+]]. Entities may be anything worth knowing about, real or fictional, permanent or ephemeral, current or historical. Assertions may be anything that can be said about them. The relationship _written-by_ might link an author and a book; _location_ might link a mountaintop to its geo-coordinates. ==Assertions may have other assertions made about them, such as opinions on their veracity, or limitations on their scope. For example, someone can assert that the example assertion about _Nur-Sultan ‚Äúbecame-true_‚Äù in 1997_._ And such assertions about assertions have provenances of their own. This uniform representation helps software to search and evaluate the Underlay‚Äôs web of knowledge.¬†\n\t\t- I'm really curious how this could relate to [[Hode]], where you can also have assertions about assertions to form higher order relationships.\n\t\t- The became-true relationship with a date type value is really interesting as a way to respond to: [[Q- How might we propagate changing beliefs throughout a network]]\n\n- \u003e ==Sometimes sources contradict one another. Comedian Gracie Allen claimed her birth year was 1906, but the US Census listed it as 1895. The Underlay will contain both assertions with their provenance, leaving it to other processes to decide which to trust.== It will also include assertions by others about the credibility of these alternate versions of the truth. Algorithms will use endorsements from organizations such as Merriam-Webster, the Wikimedia Foundation, and scientific journals, to judge what is true.¬†\n\t- [[Q- How do we make a data structure that can be queried through GraphQL]]? Does this assertion style work, or does it have to be a standard schema? Would we have to create certain sorts of assertions?\n\n- \u003e ==Assertions may be added to the Underlay by anyone; those who add an assertion are noted as part of its provenance. The Underlay allows additions, not edits, so that many copies (or partial copies with different subsets of knowledge) can exist simultaneously.== The Underlay may be used in many places at once, online and offline. It can be stored in a distributed network of registries, each storing different parts. No registry is required to store the entire Underlay, or to accept every new assertion.\n\t- [[comment]] Perhaps people could \"back\" the assertions of others, or there could be some sort of consensus mechanism for bringing multiple assertions together or [[F- Wikum allows you to summarize groups of comments on a Hacker News style forum|summarizing multiple assertions]]. We ask [[Q- What is the role of reputation in a global knowledge graph]], and perhaps that goes into the consensus mechanism. When asking [[Q- What community roles are necessary in a decentralized knowledge graph]], this is a rich source of inspiration.\n\nAny assertion can be updated or contradicted, perhaps even by the same source, but the updates will have different timestamps. [[Q- What does multiplayer look like with immutable state|Some registries may choose to stop storing an assertion while others retain it.]] ==Some will make an assertion about a statement‚Äôs veracity or value. Others will take these judgments into account in deciding what to store. This leads to many independent editorial judgments of what to believe and what to store. Consensus is not required, and not even expected.==¬†\n\n**How the Underlay is different from other public knowledge bases** \n\nOne way to understand the Underlay is to compare it to other open collections of knowledge.¬†\n\n_**Wikipedia**_ is a set of public knowledge about notable entities presented in an illustrated natural language format. Distinct versions of Wikipedia exist in different languages. It may be the most widely used reference publication ever created. Much of its knowledge is not yet easily interpretable by machines. Articles contain structured elements, including categories and often an ‚Äúinformation box‚Äù with roughly standardized fields. As in the Underlay, additions can be asserted by anyone. A prefilter for ‚Äúnotable‚Äù topics limits what is included in most language editions. Knowledge is attributed, usually to pseudonymous editors, and cited to a source roughly once a paragraph. ==What makes Wikipedia different than the Underlay is that its assertions are not machine-readable, and it covers only a few million topics, making it tiny by comparison with commercial knowledge graphs.==\n\n==Public databases such as the _**U.S. patent database**, **SEC filings**, **citation indexes**, **catalogs**, the **human genome**_, star atlases, linguistic lexicons, and zoological taxonomies are usually machine- readable, but not stored in any consistent format. Some, like the _**Allen Brain Atlas**_, are actively maintained, expanded, and made available for the public good.== The Underlay would both take advantage of these open efforts and expand their usefulness by making the knowledge within them available in a common format and connecting them with other types of knowledge.¬†\n\n==_**Blockchains**_ are distributed public ledgers of assertions. Like the Underlay, they are distributed, with no storage node having special status. Their primary purpose is to ensure agreement among users about what transactions took place between them, which is not a requirement of the Underlay. **Blockchains may have a role in implementing the Underlay, but because they have no¬†standard representation of knowledge and are difficult to scale, they are not in themselves a solution to the problem of sharing public knowledge.**¬†==\n\nSeveral projects have tried to represent vetted general knowledge in an open machine-readable format, such as _**DBpedia**_ and _**Opencyc.**_ These databases differ from the Underlay in attempting a consistent version of the truth, rather than all (possibly contradicting) assertions and their provenance. ==These are an excellent source of assertions for the Underlay, and the Underlay may be a source of provenance and reliability data for these vetted collections, as it stores attributed assertions about the validity of other assertions. Such curated databases could be represented as sources in the Underlay.¬†==\n\n==The public databases most similar to the Underlay are _**Freebase**_ and _**Wikidata**_, open entity-relationship databases with limited provenance. Like the Underlay, each preserves the history of assertions; sourced assertions can come from anyone; they can contradict other assertions; and relationships are language-independent.== (The recent launch of **Abstract Wikipedia** may improve further on the latter.) Both have assertions added by curators around the world. Like the Underlay, they include assertions automatically extracted from other published sources. They have some representation of authorship, although not as rich a representation of provenance as that envisioned for the Underlay.¬†\n\n==Freebase and its toolchain was acquired by Google and used to build what is now the _**Google Knowledge Graph**_. At the time Freebase was acquired, it had about 100 million assertions, many of which have now become part of Wikidata. The Google Knowledge Graph has grown steadily since then, adding other public and private data, now fully intermixed so that none of these updates are available to the public.== Today it has hundreds of billions of assertions, and is used widely for search, advertisement placement, generating dynamic presentations, and much more. It is the knowledge base that is most similar to the envisioned Underlay, except for the important difference that it is managed as a proprietary resource, rather than as a public good. The Underlay also supports richer presentations of provenance and improved methods of connecting independent collections of knowledge.¬†\n\n**Summary** \n\nThe Underlay is a long-term global project. It has the potential to become part of humanity‚Äôs basic infrastructure, enabling wider and more useful access to public knowledge. Eventually, if we are successful, it may change how we discover and understand what is known.¬†\n\n_For the original whitepaper, see: [The Future of Knowledge](https://www.underlay.org/pub/future/)_\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-A-Troubleshooters-Checklist-for-Prospective-Authors":{"title":"R- A Troubleshooters Checklist for Prospective Authors","content":"\n-   #references\n    -   Title: A Troubleshooter's Checklist for Prospective Authors Derived from Reviewers' Critical Feedback\n    -   Meta:\n        -   Tags: #ref/Paper\n        -   Authored by:: Adrienne Alton-Lee\n        -   Year: [[1998]]\n        -   Publication: Teaching and Teacher Education\n        -   URL:\n        -   Citekey: alton-leeTroubleshooterChecklistProspective1998\n    -   Content\n        -   Placeholder\n        -   Abstract\n            -   Presents, in order of frequency of occurrence, critical comments apparent in multiple reviews of 58 manuscripts received from April 1997 to April 1998 by Teaching and Teacher Education, thus identifying the breaches of canon most frequently identified in reviewers' critical comments. There are 13 broad categories of criticism. The one outstanding area of criticism relates to author approach or methodology. (SM)\n    -   #[[üìù lit-notes]]\n        -   out of 369 criticisms in 142 reviews for 58 manuscripts submitted for review to Teaching and Teacher education, approximately 33% ({{[[calc]]: (31+29+23+21+20)}} / 369) were directly related to inadequacies in [[synthesis]]\n            -   as cited in #[[@booteScholarsResearchersCentrality2005]]\n                -   ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FNbJN82GKeI.png?alt=media\u0026token=94bb3cf6-b064-4fe9-a64b-7b03468bd692)\n                -   a bit of a chicken-and-egg here maybe.. if [[Z: Effective synthesis is hard]] for everyone, then reviewers may not have adequate expertise to judge whether a [[synthesis]] is adequate. Probably doesn't change the numbers here that much though. and also remember that these aren't necessarily [[interdisciplinarity]] papers.\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-A-decade-of-theory-as-reflected-in-Psychological-Science":{"title":"R- A decade of theory as reflected in Psychological Science","content":"\n\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Accelerating-Scientific-Discovery-by-Lowering-Barriers-to-User-Generated-Synthesis-of-Scientific-Literature":{"title":"R- Accelerating Scientific Discovery by Lowering Barriers to User-Generated Synthesis of Scientific Literature","content":"\n\n\nAuthored By:: [[P- Joel Chan]]\nhttps://drive.google.com/file/d/1yjTjcIVqttXEA2NXKQ75D8iHGTOxiEj4/view\n\nOutcomes of synthesis: theory, integrative literature review, systematic literature review, systematic lit review, causal model, problem framing, problem formulation, model of a design space, etc.\n\nIt's really hard for science to move forward if a space is already largely discovered. Jones, 2009, \"Burden of knowledge\"\n\n\u003e The fundamental information models that underlie most scientists' everyday reading and communication practices are not readily amenable to integration, comparison, sharing, and translation across publications, researchers, or domains\n\nSynthesis is arduous and effortful. A significant amount of labor goes towards transforming raw unstructured texts into forms amenable to publication. \n\n\u003e The experience of synthesis work is often described as arduous and effortful (Ervin, 2008, Knight et al., 2019, Granello, 2001,) and estimates of the time taken to do synthesis in a rigorous manner, such as in a systematic review, corroborate these subjective experiences (Shojania et al., 2007, Petrosino, 1999, Ervin, 2008), with the labor of transforming the \"raw data\" of unstructured texts into forms amenable for analysis comprising a major portion of these time costs.\n\n\u003e One effort to address the difficulty of synthesis is a growing body of work on tools for augmenting systematic review work [O‚ÄôConnor et al., 2019]. While promising, these efforts are often framed as special-purpose tools disconnected from (and not interoperable with) routine scientific practices [O‚ÄôConnor et al., 2019].\n\nThis reminds me of how [Abstract](https://www.abstract.com/) struggled because [Git](https://en.wikipedia.org/wiki/Git) was pretty far outside of designer workflows\n\nOne of Joel's main goals is to lower the barrier for scientists to efficiently generate synthesis and incorporate it into their routine practices. [[Q- What user behaviors are people doing already that specify structure that are not being instantiated into a structure]]\n- Integrate into scientific practices\n- Improve synthesis quality\n- Lower overhead synthesis through reuse of the intermediate products of synthesis\n\nHis research uses in-depth [[user interview]]s, [[participant observation]], and [[co-design]]\n\n## State of the Art and Research objectives\n\n### The promise of discourse graph information models for augmenting synthesis\n\nThe best softwares for augmenting knowledge synthesis will distill publication down to claims, linked to supporting evidence and context through a network or graph model. Knowledge graph format coined as a discourse graph\n\n- A discourse graph will take knowledge claims, rather than concepts, as the core unit. Linking and relating claims is emphasized over categorizing or filing them. \n\t- It's not just connecting them, but also coding the distinctions\n\nIsn't this claim mapping more useful than a concept mapping?\n![[Pasted image 20210413213435.png]]\n*Would be nice to have a probability view*\n\n\u003e For example, which theories have the most empirical support in this particular setting? Are there conflicting theoretical predictions that might signal fruitful areas of inquiry? What are the key phenomena to keep in mind when designing an intervention (e.g., perceptions of human vs. automated action, procedural considerations, noise in judgments of wrongdoing, scale considerations for spread of harm)? What intervention patterns (whether technical, behavioral, or institutional) have been proposed that are both a) judged on theoretical and circumstantial grounds as likely to be effective in this setting, and b) lacking in direct evidence for efficacy?\nThese sorts of questions and claims help you actually map knowledge better\n\nYou need contextual details in order to understand how domain-general knowledge applies to domain-specific knowledge. [[Q- How do you handle the transition from private to public without sacrificing context and privacy]]\n\nDiscourse graphs allow for greater reuse and repurposing of synthesis over time by compressing knowledge into usable claims.\n\n\u003e In a discourse graph, claims have many-to-many relationships to support composition of more complex arguments and theories, or \"decompression\" into component supporting/opposing claims.\n\nI don't know if decompression is the right word or better, \"expansion\"\n\nHis model has synthesis claims, questions, context, and observations. I would love for it to have **conditions** *(under which to a claim or question applies)* and **parameters** *(how to understand the boundaries of a claim)*\n\n## 2.2 The empirical gap between conceptual models and scientific practice\nSkip, come back later\n\n##  2.3 Research objectives\n\n### 2.3.1 Research question 1: What enabling conditions are necessary for discourse graphs to be successfully adopted / implemented in scientific practice?\n\n[[user interview]]\n\n### 2.3.2 Research question 2: To what extent, and in what ways, might constructing and using discourse graphs enable scientists to do synthesis better\n\n[[participant observation]] and [[co-design]] addresses each research question.\n\nSome controlled experiments.\n\n### 2.3.3 Research question: To what extent, and in what ways, do discourse graphs enable scientists to do synthesis with less friction for reuse?\n\nThis is a really important thing. How can we actually reduce the amount of [[user effort investment]] necessary to do synthesis?\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Achieving-Both-Creativity-and-Rationale":{"title":"R- Achieving Both Creativity and Rationale","content":"-   Metadata\n    -   Title: Achieving Both Creativity and Rationale: Reuse in Design with Images and Claims\n    -   Authored by:: [[D. Scott McCrickard]] , Shahtab Wahid , Stacy M. Branham , Steve Harrison\n    -   Year: 2013\n    -   Publication: Human‚ÄìComputer Interaction Series\n-   Context\n-   Reading notes\n    -   The size/complexity of [[Design Patterns]] may make them difficult to understand\n        -   Users of the Damask storyboarding tool that leveraged the [[reuse]] of [[Design Patterns]] complained #secondary-claim from [[@linEmployingPatternsLayers2008]]\n            -   #primary-claim found here: some users struggled to understand large design patterns #[[üìù lit-notes]] #Atomicity #[[context]]ualizability (p. 1321)","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Ambiguity-and-Engagement":{"title":"R- Ambiguity and Engagement","content":"\n-   Title: Ambiguity and Engagement\n-   Meta:\n    -   Authored by:: [[Peter McMahan]] [[James A. Evans]]\n    -   Year: [[2018]]\n    -   Publication: American Journal of Sociology\n    -   URL: [McMahan \u0026 Evans (2018). Ambiguity and Engagement. American Journal of Sociology](https://www.journals.uchicago.edu/doi/10.1086/701298)\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Are-theoretical-results-Results":{"title":"R- Are theoretical results Results","content":"-   #[[references]]\n    -   Title: Are theoretical results ‚ÄòResults‚Äô?\n    -   Meta:\n        -   Authored by:: [[Raymond E Goldstein]]\n        -   Year: [[2018]]\n        -   Publication: eLife\n        -   URL: [Goldstein (2018). Are theoretical results ‚ÄòResults‚Äô?. eLife](https://doi.org/10.7554/eLife.40018)\n    -   Content\n        -   Abstract\n            -   Yes.","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Beyond-Boundary-Objects":{"title":"R- Beyond Boundary Objects","content":"- #references\n    - Title: Beyond Boundary Objects: Collaborative Reuse in Aircraft Technical Support\n    - Meta\n        - Tags:: #[[ref/Paper]] #[[D/Synthesis Infrastructure]]\n        - Publication: Computer Supported Cooperative Work (CSCW)\n        - Authored by::  [[Wayne Lutters]] ,  [[Mark Ackerman]]\n        - Year: [[2007]]\n        - Content\n            - #context-snippets\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FNYuuWA8u30.png?alt=media\u0026token=9373424d-e37a-4087-acb4-2a81b14b4c90)\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F1E7PICjFNA.png?alt=media\u0026token=eedfa6d7-109a-49f2-b5ee-a188621a56ee)\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FYYrLEASOci.png?alt=media\u0026token=620a5b60-4f92-4f2a-9268-64e4a4fab994)\n    - #lit-context\n        - is #seminal ref for thinking about [[flexible compression]] as a way to get at #context  based on empirical study of aircraft technical support\n    - #[[üìù lit-notes]]\n        - **How did the authors approach their questions/problems?** ((most of this is going to come from the methods sections, although some info about conceptualization of key concepts might be relevant to pull from intro/lit-review)) #lit-context\n        - **What did they find?** ((keep this as contextualized as possible; resist the urge to repeat claims or generalizations)) #[[observation-notes]]\n            - engineers sometimes included information about who they had talked to about the case in the ROC references section, to facilitate downstream judgments of reliability (p. 357)\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FdirZWk5ezS?alt=media\u0026token=f19f4505-7e37-4a9f-b177-599ed78e8869)\n            - In a field study of collaborative information reuse in aircraft technical support, engineers lamented reusing old records because information was missing, outdated, or not appropriate anymore because of procedural changes. Over the years if any changes to the records were not tagged, the context of those changes were lost.\n                ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FuEK3Mxh7LT.png?alt=media\u0026token=04134bdb-948a-4994-8203-92487faefbd1)\n                ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FCAuivC9ftx.png?alt=media\u0026token=20b3b6d2-5da8-4d72-8209-606df2a9f45b)\n                ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fof2_R7gYN5.png?alt=media\u0026token=0cf05b15-bf9a-44a7-bf41-acaae6c09864)\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Cochrane-and-non-Cochrane-systematic-reviews":{"title":"R- Cochrane and non-Cochrane systematic reviews","content":"\n-   Metadata::\n    -   Title: Cochrane and non-Cochrane systematic reviews in leading orthodontic journals: a quality paradigm?\n        -   Tags:: #references#ref/Paper\n    -   Authored by:: Padhraig S. Fleming , Jadbinder Seehra , Argy Polychronopoulou , Zbys Fedorowicz , Nikolaos Pandis\n    -   Year: 2013\n    -   Publication: European Journal of Orthodontics\n    -   URL: [https://academic.oup.com/ejo/article/35/2/244/490824](https://academic.oup.com/ejo/article/35/2/244/490824)\n    -   PDF\n        -   Placeholder\n-   #lit-context\n    -   Focused on domain of orthondontics\n    -   Finding that Quality of reviews is slightly better for more recent reviews compared to older reviews (standards of quality have risen over time) is replicated by a later study, per scite\n        -   [https://scite.ai/reports/10.1093/ejo/cjs016?page=1\u0026utm_campaign=badge_generic\u0026utm_medium=plugin\u0026utm_source=generic](https://scite.ai/reports/10.1093/ejo/cjs016?page=1\u0026utm_campaign=badge_generic\u0026utm_medium=plugin\u0026utm_source=generic)\n-   #[[üìù lit-notes]]\n    -   #Claim Only about 20% of published [[systematic review]]s in orthondotics are \"good\" by [[AMSTAR standards of review quality]]; 20% are considered \"poor\"! (Table 2, p.246) #synthesis\n        -   ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fb_73-bHxFp.png?alt=media\u0026token=f7c204cc-c570-4561-852e-62b83f9f4f11)\n    -   Quality of reviews is slightly better for more recent reviews compared to older reviews (standards of quality have risen over time)\n    -   [[Cochrane systematic reviews]] were substantially better than non-Cochrane reviews\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Collaborative-information-synthesis-1":{"title":"R- Collaborative information synthesis","content":"Authored By:: [[P- Joel Chan]]\n\n- #[References]\n\t- Title: Collaborative information synthesis I: A model of information behaviors of scientists in medicine and public health\n\t- Meta:\n\t\t- Authored by:: [Catherine Blake] [Wanda Pratt]\n\t\t- Year: [2006]\n\t\t- Publication: Journal of the American Society for Information Science and Technology  [JASIST]\n\t\t- URL: [Blake \u0026 Pratt (2006). Collaborative information synthesis I: A model of information behaviors of scientists in medicine and public health. Journal of the American Society for Information Science and Technology](https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.20487)\n\t\t- Citekey: blakeCollaborativeInformationSynthesis2006\n- Content\n\t- Placeholder\n\t- Abstract\n\t\t- Scientists engage in the discovery process more than any other user population, yet their day-to-day activities are often elusive. One activity that consumes much of a scientist's time is developing models that balance contradictory and redundant evidence. Driven by our desire to understand the information behaviors of this important user group, and the behaviors of scientific discovery in general, we conducted an observational study of academic research scientists as they resolved different experimental results reported in the biomedical literature. This article is the first of two that reports our findings. In this article, we introduce the Collaborative Information Synthesis (CIS) model that reflects the salient information behaviors that we observed. The CIS model emerges from a rich collection of qualitative data including interviews, electronic recordings of meetings, meeting minutes, e-mail communications, and extraction worksheets. Our findings suggest that scientists provide two information constructs: a hypothesis projection and context information. They also engage in four critical tasks: retrieval, extraction, verification, and analysis. The findings also suggest that science is not an individual but rather a collaborative activity and that scientists use the results of one analysis to inform new analyses. In Part 2, we compare and contrast existing information and cognitive models that have inadvertently reported synthesis, and then provide five recommendations that will enable designers to build information systems that support the important synthesis activity.\n- lit-context\n\t- won #BestPaper at [JASIST]\n\t- followed up by [[R- Collaborative information synthesis II]]\n- lit-notes\n\t- two main information artifact types: 1) hypothesis projection, and 2) context information\n\t- is an #example-of how [context] is critical for synthesis (in this case, showing the kinds of [context queries] scientists look for to try to do [synthesis] over contradictory findings in a [systematic review]\n\t\t- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FHMQH8rqkuk.png?alt=media\u0026token=037f6d7e-f1e4-4738-a841-58923bbacadc) (p. 1744)\n\t\t- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F1iSPHVlrcS.png?alt=media\u0026token=5700d1f3-6c77-4b9a-a51c-bc083004ae33) (p. 1744) \n\t- some types of [context] information were more contextual, depending on the particular \"hypothesis projection\" of the review, which varied across the lifecycle of the project studied (e.g., location of medical condition, amount of exposure, confounding risk factors), while others were more constant regardless of hypothesis (e.g., study- and population-context information)\n\t\t- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FB4elpsPH17.png?alt=media\u0026token=afbe16b6-a3e6-478f-9929-95c520444ad3) (p., 1744) \n\t\t- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FHMQH8rqkuk.png?alt=media\u0026token=037f6d7e-f1e4-4738-a841-58923bbacadc) (p. 1744)\n\t- authors think that there is enough regularity in the types and locations of [context] information that we could build an automated system to extract htese bits of information, which would in turn enable exploration of a variety of hypothesis projections\n\t\t- in other words: [[Z: Contextualizability is necessary for synthesis]] :)\n\t\t- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F_KER50Xs10.png?alt=media\u0026token=b19c8f59-7da5-4ccd-bcb5-4e7f2f7f2f49) (p. 1744)\n\t- director of the public health group estimated that she spent ~3 hrs per article to extract [context] information required  for a particular [systematic review]\n\t\t- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FbDAZcjYk1P.png?alt=media\u0026token=5758ef30-d816-4a53-82c0-b32abc55e667) (p. 1746)\n\t\t- this is self-reported estimate: based on work on knowledge work estimation (cc. also [[R- The Cost Structure of Sensemaking]]), could well be an underestimate, though unclear by how much","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Constraint-relaxation-and-chunk-decomposition-in-insight-problem-solving":{"title":"R- Constraint relaxation and chunk decomposition in insight problem solving","content":"- #[[references]]\n    - Title: Constraint relaxation and chunk decomposition in insight problem solving\n    - Meta:\n        - Authored by:: [[G. Knoblich]] [[S. Ohlsson]] [[H. Haider]] [[D. Rhenius]] \n        - Year: [[1999]]\n        - Publication: Journal of Experimental Psychology: Learning, Memory, and Cognition\n        - URL: [Knoblich et al. (1999). Constraint relaxation and chunk decomposition in insight problem solving. Journal of Experimental Psychology: Learning, Memory, and Cognition](undefined)\n    - Content\n        - Abstract\n            - Insight problem solving is characterized by impasses, states of mind in which the thinker does not know what to do next. ==The authors hypothesized that impasses are broken by changing the problem representation, and 2 hypothetical mechanisms for representational change are described: the relaxation of constraints on the solution and the decomposition of perceptual chunks. These 2 mechanisms generate specific predictions about the relative difficulty of individual problems and about differential transfer effects.== The predictions were tested in 4 experiments using matchstick arithmetic problems. The results were consistent with the predictions. Representational change is a more powerful explanation for insight than alternative hypotheses, if the hypothesized change processes are specified in detail. Overcoming impasses in insight is a special case of the general need to override the imperatives of past experience in the face of novel conditions.\n- #references\n    - Title: Constraint relaxation and chunk decomposition in insight problem solving\n    - Meta\n        - Tags: [[D/Solution-Diversity]] [[D/Synthesis Infrastructure]] [[D/Computational Analogy]]\n        - Authored by::  [[Gunther Knoblich]] ,  [[Stellan Ohlsson]] ,  H. Haider ,  D. Rhenius\n        - Year: [[1999]]\n        - Publication: [[Journal of Experimental Psychology: Learning, Memory, and Cognition]]\n    - #lit-context\n        - [[Stellan Ohlsson]] is prominent authority on [[Insight problem solving]]\n        - Design: have people do two blocks: in second block people got a lot btter, so probably best to consider block 1 to really feel out the ffects of problem types\n            - Vary problem difficulty by varying constraint hardness and chunk tightness, both together and independently\n    - #[[üìù lit-notes]]\n        - Two key mechanisms proposed for overcoming [[Impasses]] (aka [[fixation]], which is diagnosed as a suboptimal [[problem formulation]]): 1) [[constraint relaxation]], and [[chunk decomposition]]\n            - If constraints are not considered explicitly, possible to over/under-constrain problem representation (through implicit reminding) (p. 1535)\n                - Citations here...\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fe7MM5TgRHu.png?alt=media\u0026token=b6422974-6fd1-4cae-b8b8-bbc0a44477fa)\n                - Therefore, if problem solver is able to relax unnecessary constraints, they can find a problem representation that admits solution, and overcome the impasse\n            - Argument for [[chunk decomposition]] as a mechanism of breaking [[fixation]] and [[Impasses]]\n                - People create chunks to organize experience.\n                - Encountering a novel problem activates some chunks from past experiences. Not all of them will be helpful; and some will be actively unhelpful. And some will need to be decomposed ([[compression]] reversed??)\n                - Decomposing is harder for \"tighter\" chunks: where the components are not (obviously) meaningful units on their own\n                - Thus, problems with tight chunks are harder to solve.\n            - Evidence for this: when they made it harder to decompose chunks (even with identical constraint types), people were about half as likely to solve the problem (p. 1540).\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F8ke9W0__RI.png?alt=media\u0026token=1501eaf9-e2e8-4f49-9d81-b10fa9f8b411)\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FFaRMIEcsQM.png?alt=media\u0026token=2a393561-d69d-43ad-a28c-a6d40367b557)\n                - Said another way\n                    - Impeding [[chunk decomposition]] makes it harder to overcome [[Impasses]] during [[Insight problem solving]]\n                        - Possible context\n                            - https://www.sciencedirect.com/science/article/pii/S0361923006002292\n                            - https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.21501https://www.sciencedirect.com/science/article/pii/S1053811915002128\n                            - https://www.sciencedirect.com/science/article/pii/S0006899309016850\n                            - https://academic.oup.com/cercor/article-abstract/26/7/2991/1745029\n                            - https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11427-010-4088-z\u0026casa_token=jNdkXtH7qXEAAAAA:qUgnBL1rCGJzOQZ2rGog7mBEqDSwmZkxT8S3TI7_0ewCYNJWIZ9o42JI4QaNx2EfehJ0p-aGBKBJcd91Mw\n                            - https://www.frontiersin.org/articles/10.3389/fpsyg.2017.02001/full\n                            - https://www.sciencedirect.com/science/article/pii/S0304394015003572\n                            - https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01025\n                            - https://escholarship.org/content/qt35d004q0/qt35d004q0.pdf\n                            - https://www.sciencedirect.com/science/article/pii/S0301051119301000\n                            - https://www.frontiersin.org/articles/10.3389/fpsyg.2018.02568/full\n                            - http://journal.psych.ac.cn/xlxb/EN/abstract/abstract3575.shtml\n                            - https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/content/pdf/10.1007/s11571-020-09590-w.pdf\u0026casa_token=vy0I65jymhcAAAAA:A9ZSun2vrmJpVf2jc1hTs_pMYXoHMmDgUfY6NrN5MpachZXOOj2cki1dCikZdv8lqJPKCOP3dxstv1yCyw\n                            - https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00673/full\n                            - https://books.google.com/books?hl=en\u0026lr=\u0026id=2_IgoLXDx58C\u0026oi=fnd\u0026pg=PA142\u0026dq=chunk\u0026ots=ed2XVJ-I6f\u0026sig=vHKaYEY91Txw3w1gK9xYoQTgLck\n                            - https://digitalcommons.usu.edu/psych_facpub/1571/\n                            - https://onlinelibrary.wiley.com/doi/pdf/10.1002/jocb.442?casa_token=AgFTa5I7ei4AAAAA:k2D2q27kptU50UjLvWbBmQNh2X0sucpSXd2ejjdWhEPLYVNI15ZXOUZAcP6lczeZELT7lJ1m_qEvg8Z0\n                            - https://iccm-conference.neocities.org/2012/proceedings/papers/0033/paper0033.pdf\n                            - supported-by:: [[R: zhangChunkDecompositionContributes2015]] [[R: wellerInteractiveInsightProblem2011]]\n            - What are chunks??\n                - \"chunks\" for Knoblich are *not* similar to [[Design Patterns]] or [[Frames]] - they are more like visual features, that exist at multiple levels of hierarachies. (cf.  \n                - Knoblich characterizes [[Chunk]]s as #\u003e \"patterns that capture recurring constellations of features or components\" (p. 1535)\n                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F_jL1XWtx0l.png?alt=media\u0026token=85dd6f16-4814-40d4-975f-114c303aaddc)\n                - To [[@chaseMindEyeChess1973]] on the idea of [[Chunking]] as a mechanism for expert performance #expertise\n                    - Which itself goes back fruther to [[de Groot]], back in the mid-1960's\n                    - Be careful, though! Their idea of [[Chunking]] is closer to the idea of [[compression]] - packing a huge amount of information into a smaller unit, abstracting away a lot of things.\n        - Observed some transfer effects across blocks: once you see a constraint relaxed or decompose a chunk, a later problem that .\n            - Somewhat anecdotal in Experiment 1, and directly validated in Experiment 2","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Context-Grabbing-Assigning-Metadata-in-Large-Document-Collections":{"title":"R- Context Grabbing Assigning Metadata in Large Document Collections","content":"\n- #references\n    - Title: Context Grabbing: Assigning Metadata in Large Document Collections\n    - Meta:\n        - Tags: #ref/Paper\n        - Authored by:: [[Joachim Hinrichs]] [[Volkmar Pipek]] [[Volker Wulf]] [[Hans Gellersen]] [[Kjeld Schmidt]] [[Michel Beaudouin-Lafon]] [[Wendy Mackay]] \n        - Year: [[2005]]\n        - Publication: [[ECSCW]] 2005\n        - URL: [Hinrichs et al. (2005). Context Grabbing: Assigning Metadata in Large Document Collections. ECSCW 2005](undefined)\n        - URL: \n        - Citekey: hinrichsContextGrabbingAssigning2005\n    - Content\n        - Placeholder\n        - Abstract\n            - Classification schemes are an important issue in the collective use of large document collections. We have investigated the classification of technical documentations in two engineering domains: a steel mill and a sewerage plant company. In both cases we found a coexistence of different classification schemes and problems resulting from distributed local archives. In supporting human actors to maintain different classifications schemes while working on a common archive, we developed the concept of context grabbing. It allows assigning context information efficiently in the form of metadata. Based on a document management system, a tool kit for context grabbing was developed. Its evaluation in a sewerage service company allows us to comment on important aspects of understanding the role of classifications in collaborative work.\n        - #context-snippets\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FYhHuwMz8Tp.png?alt=media\u0026token=acf224fa-df96-4689-9ec0-abe2881f1cd9)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FpX4rNj7Qga.png?alt=media\u0026token=5919f57c-dac8-4bea-b75d-078a1629bd61)\n                - pg 372\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FJ2Sqr7nsSy.png?alt=media\u0026token=a5f60919-328e-43b9-8b79-d9043f2ba5d8)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F7zrr6dqDHb.png?alt=media\u0026token=b9d89e5a-f88c-473c-a7e3-04e6e0cac621)\n    - #lit-context\n        - [[ECSCW]] is where a lot of the [[m/Ethnography]] in context of **Work** is still alive (exited somewhat from [[CSCW]]), so we can have a bit more confidence that this is done well from a theoretical and methodological perspective\n        - last author [[Wendy Mackay]] is super-prominent in [[CSCW]]\n            - ex: chosen to summarize history of social comptuing research in the 1980's at the 2020 CSCW conference panel\n    - #[[üìù lit-notes]]\n        - useful precursor to idea of [[flexible compression]] as means for tackling [[C- Specifying context for future reuse is costly]]\n    - #lit-context\n    - #[[üìù lit-notes]]\n        - **How did the authors approach their questions/problems?** ((most of this is going to come from the methods sections, although some info about conceptualization of key concepts might be relevant to pull from intro/lit-review)) #lit-context\n        - **What did they find?** ((keep this as contextualized as possible; resist the urge to repeat claims or generalizations)) #[[observation-notes]]\n            - Decentralized project documentation made recontextualization difficult for engineers because they didn't know the history of changes for a document or if a particular document was up to date, leading to issues such as \"exploratory digging by hand\" to avoid damaging power lines pg 375\n                - [[P- Joel Chan]] comments\n                    - this is looking pretty good in terms of granularity, i can work with this!\n                    - would be nice to know more about what is meant by decentralized project documentation. \n                        - probably best to have a separate `observation note` block that summarizes that (with its own `context snippets`), and the block ref that into the \"decentralized project documentation\". ","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Could-a-Neuroscientist-Understand-a-Microprocessor":{"title":"R- Could a Neuroscientist Understand a Microprocessor","content":"\n-   #[[references]]\n    -   Title: Could a Neuroscientist Understand a Microprocessor?\n    -   Meta:\n        -   Authored by:: [[Eric Jonas]] [[Konrad Paul Kording]]\n        -   Year: [[2017]]\n        -   Publication: PLOS Computational Biology\n        -   URL: [Jonas \u0026 Kording (2017). Could a Neuroscientist Understand a Microprocessor?. PLOS Computational Biology](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268)\n    -   Content\n        -   Abstract\n            -   There is a popular belief in neuroscience that we are primarily data limited, and that producing large, multimodal, and complex datasets will, with the help of advanced data analysis algorithms, lead to fundamental insights into the way the brain processes information. These datasets do not yet exist, and if they did we would have no way of evaluating whether or not the algorithmically-generated insights were sufficient or even correct. To address this, here we take a classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the microprocessor. This suggests current analytic approaches in neuroscience may fall short of producing meaningful understanding of neural systems, regardless of the amount of data. Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Darwin-on-man-A-psychological-study-of-scientific-creativity":{"title":"R- Darwin on man A psychological study of scientific creativity","content":"\n-   Title: Darwin on man: A psychological study of scientific creativity\n-   Meta:\n    -   Tags:: #R-Book #[[references]] #synthesis\n    -   Authored by:: [[Howard E. Gruber]] [[Paul H. Barrett]]\n    -   Year: [[1974]]\n    -   Publication: undefined\n    -   URL: [Gruber \u0026 Barrett (1974). Darwin on man: A psychological study of scientific creativity.](https://www.amazon.com/Darwin-Man-Psychological-Scientific-Creativity/dp/0226310078)\n-   Content\n    -   Abstract\n        -   Presents an historical and biographical study of the development of Darwin's thought and a general discussion of the creative process from the standpoint of developmental psychology. The history of the suppression of scientific thought, Darwin's theories on religion, and the problem of philosophic materialism are also examined. (PsycINFO Database Record (c) 2016 APA, all rights reserved)\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Digital-Futures-Sociological-Challenges-and-Opportunities-in-the-Emergent-Semantic-Web":{"title":"R- Digital Futures Sociological Challenges and Opportunities in the Emergent Semantic Web","content":"\n- #references\n    - Title: Digital Futures? Sociological Challenges and Opportunities in the Emergent Semantic Web\n    - Meta:\n        - Tags: #ref/Paper\n        - Authored by::  Susan Halford ,  Catherine Pope ,  Mark Weal\n        - Year: [[2013]]\n        - Publication: Sociology\n        - URL: https://doi.org/10.1177/0038038512453798\n        - Citekey: halfordDigitalFuturesSociological2013\n    - Content\n        - Placeholder\n    - #lit-context\n        - Part of sociological sream of inquiry on [[Semantic Web]] and [[infrastructure]]. Cites [[@randallDistributedOntologyBuilding2011]]\n    - #[[üìù lit-notes]]\n        - Promise of [[Semantic Web]] rests on being able to have unique resource identifiers ([[URIs]]). This relates to [[formality]], although important to distinguish from [[compositionality]]\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FM3PpiGtvzg.png?alt=media\u0026token=e0783be4-c6dd-40fa-b76f-0805b1eb42b7)\n            - But naming entities is far from straightforward. See, e.g., [[@bowkerSortingThingsOut2000]] and [[@randallDistributedOntologyBuilding2011]]\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FzYhdbtf4dB.png?alt=media\u0026token=e9d60c6d-358e-4620-880d-e7cdf6360d82)\n            - This has significant implications for how we think about precisely in what sense [[Z: Composability is necessary for synthesis]], and how [[formality]] plays into it\n                - #[[Z: [[context]] and [[formality]] are in tension]]\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Discovering-Higher-Order-Relationships-from-Multi-Modal-EHR-Data":{"title":"R- Discovering Higher Order Relationships from Multi Modal EHR Data","content":"\n[StanfordSTARR](https://www.youtube.com/channel/UC6iGiAO1dKwuC2wOrxnKiNw)\n\nThis is an example of a discourse graph in medical research discourse. ![[Pasted image 20210912172117.png]]\n\nVisualize change in ranking ![[Pasted image 20210912172321.png]]\n\n![[Pasted image 20210912172354.png]]","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Distributed-ontology-building-as-practical-work":{"title":"R- Distributed ontology building as practical work","content":"-   Title: Distributed ontology building as practical work\n-   Meta:\n    -   Tags: #ref/Paper\n    -   Authored by:: Dave Randall , Rob Procter , Yuwei Lin , Meik Poschen , Wes Sharrock , [[Robert Stevens]]\n    -   Year: [[2011]]\n    -   Publication: International Journal of Human-Computer Studies\n    -   URL: [https://linkinghub.elsevier.com/retrieve/pii/S1071581911000024](https://linkinghub.elsevier.com/retrieve/pii/S1071581911000024)\n    -   Citekey: randallDistributedOntologyBuilding2011\n-   Content\n    -   Placeholder\n-   #lit-context\n    -   [[m/Ethnography]] studying the use of [[sys/Protege]] for building [[ontologies]]","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Epistemology-and-the-socio-cognitive-perspective-in-information-science":{"title":"R- Epistemology and the socio-cognitive perspective in information science","content":"\n   - #[[references]]\n        - Tags:: #[[references]] #[[D/Synthesis Infrastructure]] #interdisciplinarity #[[Domain-Analysis]]\n        - Title: Epistemology and the socio-cognitive perspective in information science\n        - Meta:\n            - Authored by:: [[Birger Hj√∏rland]] \n            - Year: [[2002]]\n            - Publication: Journal of the American Society for Information Science and Technology ([[JASIST]])\n            - URL: [Hj√∏rland (2002). Epistemology and the socio-cognitive perspective in information science. Journal of the American Society for Information Science and Technology](https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.10042)\n        - Content\n            - Abstract\n                - This article presents a socio-cognitive perspective in relation to information science (IS) and information retrieval (IR). The differences between traditional cognitive views and the socio-cognitive or domain-analytic view are outlined. It is claimed that, given elementary skills in computer-based retrieval, people are basically interacting with representations of subject literatures in IR. The kind of knowledge needed to interact with representations of subject literatures is discussed. It is shown how different approaches or ‚Äúparadigms‚Äù in the represented literature imply different information needs and relevance criteria (which users typically cannot express very well, which is why IS cannot primarily rely on user studies). These principles are exemplified by comparing behaviorism, cognitivism, psychoanalysis, and neuroscience as approaches in psychology. The relevance criteria implicit in each position are outlined, and empirical data are provided to prove the theoretical claims. It is further shown that the most general level of relevance criteria is implied by epistemological theories. The article concludes that the fundamental problems of IS and IR are based in epistemology, which therefore becomes the most important allied field for IS.\n    - #lit-context\n        - Seems classic (475 cites on GS, plus part of line of work around domain analysis) [[Domain-Analysis]] approach\n            - Author also seems super important in this space\n    - #[[üìù lit-notes]]\n        - Disciplines have deep-rooted and consequential differences in what counts as \"relevant\" information. Some are characterized by [[paradigmatic relevance]], where the meaning of terms, and what therefore counts as relevant results for that term, vary substantially by \"school of thought\". #Relevance\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Flzo9orBvZ2?alt=media\u0026token=df92df81-86ad-42cf-84e5-f1ef93f3b620)","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Excel-never-dies":{"title":"R- Excel never dies","content":"\n\nhttps://www.notboring.co/p/excel-never-dies","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Exploring-the-Relationship-Between-Personal-and-Public-Annotations":{"title":"R- Exploring the Relationship Between Personal and Public Annotations","content":"- #[[references]]\n    - Title: Exploring the Relationship Between Personal and Public Annotations\n    - Meta:\n        - Tags:  #[[D/Synthesis Infrastructure]]\n        - Authored by:: [[Catherine C. Marshall]] [[A. J. Bernheim Brush]] \n        - Year: [[2004]]\n        - Publication: Proceedings of the 4th ACM/IEEE-CS Joint Conference on Digital Libraries [[JCDL]]\n        - URL: [Marshall \u0026 Brush (2004). Exploring the Relationship Between Personal and Public Annotations. Proceedings of the 4th ACM/IEEE-CS Joint Conference on Digital Libraries](http://doi.acm.org/10.1145/996350.996432)\n    - Content\n        - Abstract\n            - Today people typically read and annotate printed documents even if they are obtained from electronic sources like digital libraries If there is a reason for them to share these personal annotations online, they must re-enter them. Given the advent of better computer support for reading and annotation, including tablet interfaces, will people ever share their personal digital ink annotations as is, or will they make substantial changes to them? What can we do to anticipate and support the transition from personal to public annotations? To investigate these questions, we performed a study to characterize and compare students' personal annotations as they read assigned papers with those they shared with each other using an online system. By analyzing over 1, 700 annotations, we confirmed three hypotheses: (1) only a small fraction of annotations made while reading are directly related to those shared in discussion; (2) some types of annotations - those that consist of anchors in the text coupled with margin notes - are more apt to be the basis of public commentary than other types of annotations; and (3) personal annotations undergo dramatic changes when they are shared in discussion, both in content and in how they are anchored to the source document. We then use these findings to explore ways to support the transition from personal to public annotations.\n    - #lit-context\n        - #canonical cite in our lab on annotations\n    - #[[üìù lit-notes]]\n        - Types of [[annotation]]s p.351\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F_UKK-Yjrcn?alt=media\u0026token=79854c40-f7a2-4673-be57-642006212805)\n        - Most (~70-80%) private [[annotation]]s are not useful to other people (p. 354)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FP8xVLNi95c?alt=media\u0026token=c43ee13b-f30e-4b33-a701-4c4a7be09eee)\n        - People frequently need to make substantial changes to [[annotation]]s ot make them useful to other people (p. 354)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F4p3Us2Yrvg?alt=media\u0026token=b0954b0f-7664-4125-b109-3fb4f5693955)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FIoe9FwtYc9?alt=media\u0026token=70d1b6c7-634d-452d-84a1-297eb1e59783)\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Face-Masks-Against-COVID-19":{"title":"R- Face Masks Against COVID-19","content":"\n-   #[[references]]\n    -   Title: Face Masks Against COVID-19: An Evidence Review\n    -   Meta:\n        -   Authored by:: [[Jeremy Howard]] [[Austin Huang]] [[Zhiyuan Li]] [[Zeynep Tufekci]] [[Vladimir Zdimal]] [[Helene-Mari van der Westhuizen]] [[Arne von Delft]] [[Amy Price]] [[Lex Fridman]] [[Lei-Han Tang]] [[Viola Tang]] [[Gregory L. Watson]] [[Christina E. Bax]] [[Reshama Shaikh]] [[Frederik Questier]] [[Danny Hernandez]] [[Larry F. Chu]] [[Christina M. Ramirez]] [[Anne W. Rimoin]]\n        -   Year: [[2020]]\n        -   Publication: undefined\n        -   URL: [Howard et al. (2020). Face Masks Against COVID-19: An Evidence Review. undefined](https://www.preprints.org/manuscript/202004.0203/v3)\n    -   Content\n        -   Abstract\n            -   The science around the use of masks by the general public to impede COVID-19 transmission is advancing rapidly. Policymakers need guidance on how masks should be used by the general population to combat the COVID-19 pandemic. In this narrative review, we develop an analytical framework to examine mask usage, considering and synthesizing the relevant literature to inform multiple areas: population impact; transmission characteristics; source control; PPE; sociological considerations; and implementation considerations. A primary route of transmission of COVID-19 is via respiratory droplets, and is known to be transmissible from presymptomatic and asymptomatic individuals. Reducing disease spread requires two things: first, limit contacts of infected individuals via physical distancing and other measures, and second, reduce the transmission probability per contact. The preponderance of evidence indicates that mask wearing reduces the transmissibility per contact by reducing transmission of infected droplets in both laboratory and clinical contexts. Public mask wearing is most effective at reducing spread of the virus when compliance is high. The decreased transmissibility could substantially reduce the death toll and economic impact while the cost of the intervention is low. Given the current shortages of medical masks we recommend the adoption of public cloth mask wearing, as an effective form of source control, in conjunction with existing hygiene, distancing, and contact tracing strategies. Because many respiratory droplets become smaller due to evaporation, we recommend increasing focus on a previously overlooked aspect of mask usage: mask-wearing by infectious people (\"source control\") with benefits at the population-level, rather than mask-wearing by susceptible people, such as health-care workers, with focus on individual outcomes. We recommend that public officials and governments strongly encourage the use of widespread face masks in public, including the use of appropriate regulation.\n        -   #quotes\n            -   The standard RCT paradigm is well-suited to medical interventions in which a treatment has a measurable effect at the individual level and furthermore, interventions and their outcomes are independent across persons comprising a target population.\n            -   By contrast, the effect of masks on a pandemic is a population-level outcome where individual-level interventions have an aggregate effect on their community as a system. Consider, for instance, the impact of source control ‚Äî its effect occurs to other individuals in the population, not the individual who implements the intervention by wearing a mask. This also underlies a common source of confusion ‚Äî most RCT studies in the field examine masks as personal protective equipment (PPE) because efficacy can be measured in individuals to whom treatment is applied, i.e. ‚Äúdid the mask protect the person who wore it?‚Äù Even then, ethical issues prevent the availability of an unmasked control arm (27). The lack of direct causal identifiability requires a more integrative systems view of efficacy.\n            -   We need to consider first principles ‚Äî transmission properties of the disease, controlled biophysical characterizations alongside observational data, partially informative RCTs (primarily with respect to PPE), natural experiments (28), and policy implementation considerations ‚Äî a discursive synthesis of interdisciplinary lines of evidence which are disparate by necessity (9, 29). (p. 3)\n    -   #[[üìù lit-notes]]\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Formality-Considered-Harmful":{"title":"R- Formality Considered Harmful","content":"\n- #references\n    - Title: Formality Considered Harmful: Experiences, Emerging Themes, and Directions on the Use of Formal Representations in Interactive Systems\n    - Meta\n        - Authored by:: [Frank Shipman] ,  [Catherine Marshall]\n        - Year: [1999]\n        - Publication: Computer Supported Cooperative Work (CSCW)\n    - Context\n        - #canonical paper in lab\n    - #[üìù lit-notes]\n        - Four problems with / created by formalization: 1) cognitive overhead, 2) tacit knowledge, 3) premature structure, and 4) situational structure\n            - Cognitive overhead (aka Cognitive Load): often the task of specifying formalism is extraneous to the primary task, or is just plain annoying to do\n            - Tacit knowledge: if relevant info for developing formalism is tacit, asking people to formalize it will interrupt the task, with serious consequences for the quality of the work\n            - Enforcing Premature Structure: people don't want to commit until they're sure what formalism is actually useful for their task (and what's extraneous and only annoying)\n            - Situational Structure: Useful structures and formalisms vary significantly across people, situations, and tasks\n        - [[incremental formalization]] can mitigate costs/risks of formality in interactive systems (section 4.3, p. 347-438)\n            - Basic idea: (mostly) informal entry of information, then defer formalization until later in the task when it is useful\n            - Key advantages:\n                - Reduce initial overhead of entering information\n                - Reduce risk of harm from prematurely committing to the \"wrong\" structure\n            - Examples of incremental formalization\n                - In the Hyper-Object Substrate system, users enter mostly informal text initially, and the system recognizes patterns in the textual information to suggest possible formal attributes or relations for the underlying knowledge base, which the user can then accept/modify/reject as they wish (p. 347). \n                    - example-of:: [[incremental formalization]]\n                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fnv5jGR2KtA?alt=media\u0026token=7ab4cc41-116f-41d5-a440-d75b3a6d6741)\n                    - Original cite is [[R- Supporting knowledge-base evolution with incremental formalization]]\n                - Infoscope is a news reader system that suggests filters based on users' reading patterns; this helps them make their goals explicit which can facilitate formalization after it emerges from their task behaviors (p. 347-348)\n                    - example-of:: [[incremental formalization]]\n                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fts6VgCsUgF?alt=media\u0026token=a90690af-947d-4767-922d-ca32ed3a7282)\n                - VIKI is a spatial hypertext system that includes heuristic algorithms to find recurring visual/spatial patterns in layout of objects; users can use these to specify schemas if they wish\n                    - example-of:: [[incremental formalization]]\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-From-Proteins-to-Fairytales":{"title":"R- From Proteins to Fairytales","content":"- #references\n    - Title: From Proteins to Fairytales: Directions in Semantic Publishing\n    - Meta:\n        - Tags: #ref/Paper\n        - Authored by::  [[Anita de Waard]]\n        - Year: [[2010]]\n        - Publication: IEEE Intelligent Systems\n        - URL: \n        - Citekey: waardProteinsFairytalesDirections2010\n    - Content\n        - Placeholder\n    - #lit-context\n        - Author is key figure in [[semantic publishing]], now at [[Elsevier]]\n    - #[[üìù lit-notes]]\n        - #\u003e \"The column mentions different types of projects, including efforts focusing on entity enrichment and projects that involve triple markup of documents (subject-predicate-object expressions). However, such approaches are not enough. They help us find information, but they don't help us understand it. The author argues that we need to incorporate a better understanding of how language encodes meaning into our systems, so that we can develop a richer scientific knowledge representation\"\n            - Basically wants to say we need to model [[discourse]], not just [[entities]] and [[RDF triple store]] and [[ontologies]]\n        - mentions exemplars of [[discourse]]-modeling efforts (p. 86):\n            - [[std/SWAN]] #[[@ciccareseSWANBiomedicalDiscourse2008]]\n                - which is a heroic #example-of [[specialized [[curator]] model of semantic publishing]]\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FyeTGkZb5eB.png?alt=media\u0026token=7da3494b-0159-4ec4-b93a-52f00f61fcb7)\n            - #[[@grozaSALTWeavingClaim2007]]\n            - the [[sys/Cohere]] system from the [[Knowledge Media Institute]]\n","lastmodified":"2022-05-20T00:31:26.511405553Z","tags":null},"/R-Generating-research-questions-through-problematization":{"title":"R- Generating research questions through problematization","content":"\n-   #[[references]]\n    -   Title: Generating research questions through problematization\n    -   Meta:\n        -   Authored by:: [[Mats Alvesson]] [[J√∂rgen Sandberg]]\n        -   Year: [[2011]]\n        -   Publication: Academy of management review\n        -   URL: [Alvesson \u0026 Sandberg (2011). Generating research questions through problematization. Academy of management review](https://www.jstor.org/stable/41318000)\n    -   Content\n        -   Abstract\n            -   undefined\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Genuine-semantic-publishing":{"title":"R- Genuine semantic publishing","content":"\n#### Core Questions\n\n- #references\n    - Title: Genuine semantic publishing\n        - Tags:: #references#ref/Paper #[[Semantic Web]] #[[semantic publishing]]\n    - Authored by::  [[Tobias Kuhn]] ,  Michel Dumontier\n    - Year: 2017\n    - Publication: Data Science\n    - URL: https://content.iospress.com/articles/data-science/ds010\n    - PDF\n        - Placeholder\n    - #lit-context\n        - Lead author is originator and proponent of [[std/Nanopublications]]\n    - #[[üìù lit-notes]]\n        - Argues that we don't yet have genuine semantic publishing. Most approaches so far \"tack on\" semantics (broadly construed) to existing papers, without changing what is published in the first place.\n        - Includes history of attempts, ranging from [[std/Research Objects]] to [[Structured Digital Abstracts]] to [[std/Micropublication]] and [[std/Nanopublications]], and [[Scholarly HTML]] (p. 142-145)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FCp2k6dGc07.png?alt=media\u0026token=b7794f73-426b-48a3-b424-7d7f080cf052)\n        - Claims that we have all the tech we need already; problem remaining is an [[Authoring Bottleneck]] one (p. 148)\n            - Also talks about better ontologies, but I think that is related to the [[Authoring Bottleneck]]; seems to me that the ontology needs authors to grow (can't predefine everything!)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FBtCE-k-dxP.png?alt=media\u0026token=c0103e5c-c9b6-4e41-b7be-e86785c63d0e)\n            - \"It turns out that all the technologies needed for applying genuine semantic publishing are already available and most of them are very mature and reliable. There are no __technical__ obstacles preventing us from releasing our results from today on as genuine semantic publications, even though more work is needed on ontologies that cover all relevant aspects and areas and on nice and intuitive end-user interfaces to make this process as easy as possible.\" (p. 148)\n                - #\u003e from p. 148\n        - Past efforts on technical infrastructure\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FMqFIhHPg-8.png?alt=media\u0026token=db6c3b90-6401-4583-a850-2ce62e686fba)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FKO6kU5N29G.png?alt=media\u0026token=baa411eb-864c-4157-bb87-1bb5db69735a)\n            - [[std/Semantic Publishing and Reference Ontologies (SPAR)]]\n                - [37] S. Peroni and D. Shotton, FaBiO and CiTO: Ontologies for describing bibliographic resources and citations, Web Semantics: Science, Services and Agents on the World Wide Web 17 (2012), 33‚Äì43. doi:10.1016/j.websem.2012.08.001.\n                - [38] S. Peroni, D. Shotton and F. Vitali, Scholarly publishing and Linked Data: Describing roles, statuses, temporal and contextual extents, in: Proceedings ofthe 8th International Conference on Semantic Systems, ACM, 2012, pp. 9‚Äì16.\n            - [[std/Annotation Ontology]]\n                - P. Ciccarese, M. Ocana, L.J.G. Castro, S. Das and T. Clark, An open annotation ontology for science on web 3.0, Journal ofbiomedical semantics 2(2) (2011), 1. doi:10.1186/2041-1480-2-S2-S4. [[@ciccareseOpenAnnotationOntology2011]]\n            - [[std/W3C Web Annotation recommendations]]\n            - [[std/PROV Ontology]] for [[provenance]]\n                - T. Lebo, S. Sahoo, D. McGuinness, K. Belhajjame, J. Cheney, D. Corsar, D. Garijo, S. Soiland-Reyes, S. Zednik and J. Zhao, PROV-O: The PROV ontology, W3C Recommendation, 2013, [Online]. Available at https://www.w3.org/TR/prov-o/.\n            - [[std/Semanticscience Integrated Ontology (SIO)]]\n                - M. Dumontier, C.J. Baker, J. Baran, A. Callahan, L. Chepelev, J. Cruz-Toledo, N.R. Del Rio, G. Duck, L.I. Furlong, N. Keath et al., The semanticscience integrated ontology (SIO) for biomedical research and knowledge discovery, Journal ofbiomedical semantics 5(1) (2014), 14. doi:10.1186/2041-1480-5-14.\n            - [[std/Linked Science Core Vocabulary]]\n            - Argumentation / [[discourse]] [[@schneiderReviewArgumentationSocial2013]]\n            - MOdels of evidence\n                - B.M.H. Brush, K. Shefchek and M. Haendel, SEPIO: A semantic model for the integration and analysis of scientific evidence, in: ICBO/BioCreative, CEUR-WS, 2016, [Online]. Available at http://ceur-ws.org/Vol-1747/IT605_ICBO2016.pdf. [[@brushSEPIOSemanticModel2016]]\n            - formalizing [[uncertainty]] \n                - A. De Waard and J. Schneider, Formalising uncertainty: An ontology of reasoning, certainty and attribution (ORCA), in: Semantic Technologies Applied to Biomedical Informatics and Individualized Medicine, 2012, [Online]. Available at http://dl.acm.org/citation.cfm?id=2887634. [[@dewaardFormalisingUncertaintyOntology2012]]","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Gui-Phooey":{"title":"R- Gui ‚Äî Phooey","content":"\n-   Title: Gui ‚Äî Phooey!: The Case for Text Input\n-   Authored by:: [[Max Van Kleek]] , [[Michael Bernstein]] , [[David Karger]] , mc schraefel\n-   Year: 2007\n-   Publication: UIST '07\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Hammock-Driven-Development":{"title":"R- Hammock Driven Development","content":"https://www.youtube.com/watch?v=f84n5oFoZBc\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-How-to-Find-the-Right-Questions":{"title":"R- How to Find the Right Questions","content":"\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Hypotheses-Evidence-and-Relationships":{"title":"R- Hypotheses Evidence and Relationships","content":"\n- #references\n    - Title: Hypotheses, Evidence and Relationships: The HypER Approach for Representing Scientific Knowledge Claims\n    - Meta:\n        - Tags: #ref/Paper\n        - Authored by:: [[Anita de Waard]] [[Simon Buckingham Shum]] [[Annamaria Carusi]] [[Jack Park]] [[Matthias Samwald]] [[√Ågnes S√°ndor]] \n        - Authored by::  [[Anita de Waard]] ,  [[Simon Buckingham Shum]] ,  Annamaria Carusi ,  [[Jack Park]] ,  Matthias Samwald ,  √Ågnes S√°ndor\n        - Year: [[2009]]\n        - Publication: Proceedings of the 8th International Semantic Web Conference, Workshop on Semantic Web Applications in Scientific Discourse\n        - URL: [de Waard et al. (2009). Hypotheses, Evidence and Relationships: The HypER Approach for Representing Scientific Knowledge Claims. Proceedings of the 8th International Semantic Web Conference, Workshop on Semantic Web Applications in Scientific Discourse](undefined)\n        - Citekey: dewaardHypothesesEvidenceRelationships2009\n    - Content\n        - Placeholder\n        - Abstract\n            - Biological knowledge is increasingly represented as a collection of (entity-relationship-entity) triplets. These are queried, mined, appended to papers, and published. However, this representation ignores the argumentation contained within a paper and the relationships between hypotheses, claims and evidence put forth in the article. In this paper, we propose an alternate view of the research article as a network of ‚Äòhypotheses and evidence‚Äô. Our knowledge representation focuses on scientific discourse as a rhetorical activity, which leads to a different direction in the development of tools and processes for modeling this discourse. We propose to extract knowledge from the article to allow the construction of a system where a specific scientific claim is connected, through trails of meaningful relationships, to experimental evidence. We discuss some current efforts and future plans in this area.\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-In-Defense-of-Ambiguity":{"title":"R- In Defense of Ambiguity","content":"\n-   Title: In defense of ambiguity\n-   Meta:\n    -   Authored by:: [[Patrick J Hayes]] [[Harry Halpin]]\n    -   Year: [[2008]]","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Information-Foraging-Video":{"title":"R- Information Foraging Video","content":"\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_IbTZBMHiY4?start=3622\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\n[[P- Yochai Bankler]] makes the argument in [[R- Wealth of Networks]] that people have a wealth of idle attention. [[Open source]], [[Wikipedia]], etc. has some architecture designed for many people to contribute in modular ways without needing to worry about the contributions of others. 40:30\n\n\u003e When you look at these systems, what is it that you construct in these systems that makes these systems, in the aggregate, have more knowledge in them, behave more intelligently, produce more innovations, or be more productive in general.\n\u003e\n\u003e If you're into the world of UI design, what do I do at the level of individuals and individual user interfaces that have properties that percolate upward into the aggregate social architecture?\n\n[[Q- How do people acquire and transfer knowledge and expertise in a decentralized discourse graph]]\n\n[Around 43:50](https://youtu.be/_IbTZBMHiY4?t=2635): StumbleUpon had a user rating system that went from rating content on a spectrum from 1-10, then 1-5, then upvote/downvote. Each time they reduced the range of the scale, they would get more participation. This is used to justify the claim that reducing the effort to participate increases participation. \n\nStarting from 49:00, they are talking about research in increasing participation rates in social tagging. Primarily trying to solve\n- Increase participation through lower effort tag production\n- Increase individual tag production rates\n- Increase learning and memory\n- Reduce effects of \"tag noise\"\n- Support the transfer of expertise\n\nAround 1:00:00, they introduced Mr. Taggy. This is similar to [[R- Interactive Intent Modeling for Exploratory Search]] insofar as it shows you a bunch of socially generated tags, allows you to narrow it down by filtering tags in or out of the search results (like [[Roam Research|Roam's]] backlinks filters). This allowed people without prior background knowledge to learn a new domain about as well as those who came in with prior knowledge. This addresses a core problem in exploratory search with new domains: often you can't find what you are looking for because you don't know the vocabulary of the field. You enter the space searching for answers with a vocabulary that doesn't match the space. By seeing the most frequent tags in order from top to bottom, you are able to pick up on the vocabulary more quickly. Since these social tags were generated by experts, this facilitates the transfer of expertise to newcomers. [[Waldo]] has a similar interface \n\n![[Pasted image 20210920171835.png]]\n![[Pasted image 20210920171911.png]]\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Information-and-Context":{"title":"R- Information and Context","content":"\n- #references\n    - Title: Information and Context: Lessons from a Study of Two Shared Information Systems\n    - Meta:\n        - Tags: #ref/Paper\n        - Authored by::  [[Paul Dourish]] ,  [[Wendy Mackay]]\n        - Year: [[1993]]\n        - Publication: Proceedings of the conference on organizational computing systems (COCS '93)\n        - URL: \n        - Citekey: dourishInformationContextLessons1993\n    - Content\n        - Placeholder\n        - Abstract\n            - With the increasing ease and power of wmputer netsvorking technologies, many organisations me taking information which was previously managed and distributed on paper aud making it available electronically. Such shared information systems are the basis of much organisational collaboration, and electronic distribution holds great promise. However, a primary focus of such systems is on the ease of information retrieval. We believe that an equally important component is the problem of information interpretation, and that this interpretation is .@ded by a context which many electronic systems do not fully acknowledge.\n        - #context-snippets\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fbn89OKb1ff.png?alt=media\u0026token=6c0c863d-6897-4e79-ba19-94f070efa129)\n    - #lit-context\n    - #[[üìù lit-notes]]\n        - **How did the authors approach their questions/problems?** ((most of this is going to come from the methods sections, although some info about conceptualization of key concepts might be relevant to pull from intro/lit-review)) #lit-context\n        - **What did they find?** ((keep this as contextualized as possible; resist the urge to repeat claims or generalizations)) #[[observation-notes]]\n            - In a case study of calendar systems, having metadata for event information such as the title of the event or arrival time of the speaker, in addition to who the author of the information is, were critical for the interpretation of the events. \n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F6SVrexUrhm.png?alt=media\u0026token=cfa14f44-409a-4991-83e1-6c7a6b2528a4)\n                - [[P- Joel Chan]] comments\n                    - this is looking pretty good in terms of granularity, i can work with this!\n                    - would be nice to know more about what is meant by interpretation of the events, but i can kind of get it from the snippet :) it's something about judgments of conflicts maybe? if there's more details to be had, do grab that too","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Innovation-Relies-on-the-Obscure":{"title":"R- Innovation Relies on the Obscure","content":"- [[@mccaffreyInnovationReliesObscure2012]]\n- #[[references]]\n    - Title: Innovation Relies on the Obscure: A Key to Overcoming the Classic Problem of Functional Fixedness\n    - Meta:\n        - Authored by:: [[T. McCaffrey]] \n        - Year: [[2012]]\n        - Publication: Psychological Science\n        - URL: [McCaffrey (2012). Innovation Relies on the Obscure: A Key to Overcoming the Classic Problem of Functional Fixedness. Psychological Science](undefined)\n    - Content\n        - Abstract\n            - A recent analysis of real-world problems that led to historic inventions and insight problems that are used in psychology experiments suggests that during innovative problem solving, individuals discover at least one infrequently noticed or new (i.e., obscure) feature of the problem that can be used to reach a solution. This observation suggests that research uncovering aspects of the human semantic, perceptual, and motor systems that inhibit the noticing of obscure features would enable researchers to identify effective techniques to overcome those obstacles. As a critical step in this research program, this study showed that the generic-parts technique can help people unearth the types of obscure features that can be used to overcome functional fixedness, which is a classic inhibitor to problem solving. Subjects trained on this technique solved on average 67% more problems than a control group did. By devising techniques that facilitate the noticing of obscure features in order to overcome impediments to problem solving (e.g., design fixation), researchers can systematically create a tool kit of innovation-enhancing techniques.\n- #references\n    - Title: Innovation Relies on the Obscure: A Key to Overcoming the Classic Problem of Functional Fixedness\n    - Meta:\n        - Tags: #ref/Paper\n        - Authored by::  [[Tony McCaffrey]]\n        - Year: [[2012]]\n        - Publication: Psychological Science\n        - URL: \n        - Citekey: mccaffreyInnovationReliesObscure2012\n    - Content\n        - {{iframe: https://drive.google.com/file/d/1yvFObe2j53ppmSXeB5QNCYN27jBtMSqr/preview}}\n    - #lit-context\n        - System here goes on to become commercialized. Not mainstream yet though.\n    - #[[üìù lit-notes]]\n        - \n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Institutional-Ecology-Translations-and-Boundary-Objects":{"title":"R- Institutional Ecology Translations and Boundary Objects","content":"\n  -   #references\n    -   Title: Institutional Ecology, `Translations' and Boundary Objects: Amateurs and Professionals in Berkeley's Museum of Vertebrate Zoology, 1907-39\n    -   Meta:\n        -   Tags: #ref/Paper\n        -   Authored by:: [[Susan Leigh Star]] , James R. Griesemer\n        -   Year: [[1989]]\n        -   Publication: Social Studies of Science\n        -   URL: [https://doi.org/10.1177/030631289019003001](https://doi.org/10.1177/030631289019003001)\n        -   Citekey: starInstitutionalEcologyTranslations1989\n    -   Content\n        -   Placeholder\n    -   #lit-context\n        -   #canonical (original) paper for [[boundary object]]","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Investigating-PhD-thesis-examination-reports":{"title":"R- Investigating PhD thesis examination reports","content":"\n-   Metadata:\n    -   Title: Investigating PhD thesis examination reports\n    -   Authored by:: [[Allyson Holbrook]]\n    -   Publication: International Journal of Educational Research\n    -   Tags:: #[[references]] #[[D/Synthesis Infrastructure]]\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Knowledge-Synthesis-a-conceptual-model-and-practical-guide":{"title":"R- Knowledge Synthesis- a conceptual model and practical guide","content":"\n\nhttps://oasislab.pubpub.org/pub/54t0y9mk/release/2\n\n## Challenges and desiderata for a synthesis system\n\nHere are some common failure modes for a synthesis system and process that I have experienced and observed in others (not mutually exclusive!):\n\n1.  **Too much detail** (too low-level, missing forest for trees). This manifests as a lack of higher-level synthesis of what a collection of results means. A common manifestation is the ‚Äúx said this, y said this, z said this‚Äù form of literature review.\n    \n2.  **Too little detail** (too high-level, missing the devil/diamonds in the details). This manifests as overgeneralization of claims, or glossing over critical inconsistencies or contradictions. A good example of this is debates about the role of ‚Äúchildren‚Äù in COVID-19 transmission that ignore the details of differences between young children (under 10).\n    \n3.  **Insufficient context**. This is related to the lack of details, but separate in that context can also come from connection to other claims: if this is missing, even observation notes can be lost because their significance isn‚Äôt recognized.\n    \n4.  **Information silos.** This manifests in part also due to inordinate detail-orientedness, where important connections across disciplines or topics are ignored. This can also come from too little detail! If results are described at too high a level, we might miss important connections at the subproblem level between problems and results.\n    \n5.  **Information overload**. There are often too many papers to read and process in a rigorous and iterative way, which leads to / exacerbates the preceding set of problems!","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-LiquidText-A-Flexible-Multitouch-Environment-to-Support-Active-Reading":{"title":"R- LiquidText A Flexible Multitouch Environment to Support Active Reading","content":"\n\n- Metadata\n    - Title: LiquidText: A Flexible, Multitouch Environment to Support Active Reading\n        - Tags:: #[references] #[D/Synthesis Infrastructure]\n    - Authored by::  [Craig S. Tashman] ,  W. Keith Edwards\n    - Year: 2011\n    - Publication: CHI '11\n    - PDF\n        - https://drive.google.com/file/d/1LNJG5omgCORzqbDaURkZqk1fpF76Orbl/preview\n- Context\n    - #canonical ref for probably [SOTA]-class system as of [April 7th, 2020] for [synthesis] tools\n- Reading notes\n    - Introduces [[LiquidText]] system\n    - Also has nice review of basic ideas about [active reading]\n    - #Claim To support [active reading], systems must enable people to trace excerpts back to their original [context] (p. 7)\n        - cf. [[C- Knowledge must be recontextualized to be usefully reused]]\n        - cites [[R- Student readers use of library documents]] as justification\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FBAOos0FfjB?alt=media\u0026token=b0c45a1a-6c22-40ea-9d0f-7dbed9b0198b)\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Making-the-Implicit-Explicit":{"title":"R- Making the Implicit Explicit","content":"\n- lovittsMakingImplicitExplicit2007\n- \n- #[[references]]\n    - Title: Making the Implicit Explicit: Creating Performance Expectations for the Dissertation\n    - Meta:\n        - Authored by:: [[Barbara E. Lovitts]] \n        - Year: [[2007]]\n        - Publication: undefined\n        - URL: [Lovitts (2007). Making the Implicit Explicit: Creating Performance Expectations for the Dissertation.](https://www.amazon.com/Making-Implicit-Explicit-Expectations-Dissertation/dp/1579221815)\n    - Content\n        - Abstract\n            - Despite their and other stakeholders‚Äô consistent demand for excellence, doctoral programs have rarely, if ever, been assessed in terms of the quality of the dissertations departments produce. Yet dissertations provide the most powerful, objective measure of the success of a department‚Äôs doctoral program. Indeed, assessment, when done properly, can help departments achieve excellence by providing insight into a program‚Äôs strengths and weaknesses.This book and the groundbreaking study on which it is based is about making explicit to doctoral students the tacit ‚Äúrules‚Äù for the assessment of the final of all final educational products‚Äïthe dissertation. The purpose of defining performance expectations is to make them more transparent to graduate students while they are in the researching and writing phases, and thus to help them achieve to higher levels of accomplishment. Lovitts proposes the use of rubrics to clarify performance expectations‚Äìnot to rate dissertations or individual components of dissertations to provide a summary score, but to facilitate formative assessment to support, not substitute for, the advising process. She provides the results of a study in which over 270 faculty from ten major disciplines‚Äïspanning the sciences, social sciences, and humanities‚Äïwere asked to make explicit their implicit standards or criteria for evaluating dissertations. The book concludes with a summary of the practical and research implications for different stakeholders: faculty, departments, universities, disciplinary associations, accrediting organizations, and doctoral students themselves.The methods described can easily be adapted for the formative assessment of capstone courses, senior and master‚Äôs theses, comprehensive exams, papers, and journal articles.\n- #references\n    - Title: Making the Implicit Explicit: Creating Performance Expectations for the Dissertation\n    - Meta\n        - Tags: #ref/Book #[[D/Synthesis Infrastructure]] #[[P/Teach INST 888]]\n        - Authored by:: [[Barbara E. Lovitts]]\n        - Year: [[2007]]\n        - URL: https://www.amazon.com/Making-Implicit-Explicit-Expectations-Dissertation/dp/1579221815\n    - #lit-context\n        - One of only two exemplary empirically grounded studies of doctoral students' success/challenges at [[synthesis]], operating at the artifact level (there are others that examine in interviews/surveys); the other is [[@holbrookInvestigatingPhDThesis2004]]\n        - #participants were [[N=]] 276 \"PhD-productive\" (advising on average 10s of dissertations, and participating in more) faculty across [[N=]] 74 departments in [[N=]] 10 disciplines spanning the sciences, social sciences, and humanities, across [[N=]] 9 different universities\n        - #method was #m/Qualitative analysis of ~90-minute #[[m/Semi-structured interviews]] with #[[m/Focus Group]]s\n        - Their #method: analyzing  comments for a large number of actual dissertations across a wide range of disciplines that span the humanities, social sciences, and traditional STEM, including biology, physics, ECE, math, economics, psych, sociology, english, history, and philosophy\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FRu2_E_qw_N?alt=media\u0026token=53a8fa79-3acc-4eb0-bb92-845a1b86d0e6)\n    - #[[üìù lit-notes]]\n        - Ineffective [[synthesis]] is not a reason to fail a dissertation! Qualities associated with literature reviews in \"acceptable\" (or even \"very good\")  dissertations fall well short of implicit and explicit criteria for effective synthesis (e.g., [[@strikeTypesSynthesisTheir1983]], [[@booteScholarsResearchersCentrality2005]]\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Metacrap-Putting-the-torch-to-seven-straw-men-of-the-meta-utopia":{"title":"R- Metacrap- Putting the torch to seven straw-men of the meta-utopia","content":"*This is a direct clip from Cory Doctorow's blog, [see here](https://people.well.com/user/doctorow/metacrap.htm). My commentary is included under headings.*\n\n# 1. Introduction\n\nMetadata is \"data about data\" -- information like keywords, page-length, title, word-count, abstract, location, SKU, ISBN, and so on. Explicit, human-generated metadata has enjoyed recent trendiness, especially in the world of XML. \n\n==A typical scenario goes like this: a number of suppliers get together and agree on a metadata standard -- a Document Type Definition or scheme -- for a given subject area, say washing machines. They agree to a common vocabulary for describing washing machines: size, capacity, energy consumption, water consumption, price. They create machine-readable databases of their inventory, which are available in whole or part to search agents and other databases, so that a consumer can enter the parameters of the washing machine he's seeking and query multiple sites simultaneously for an exhaustive list of the available washing machines that meet his criteria.  If everyone would subscribe to such a system and create good metadata for the purposes of describing their goods, services and information, it would be a trivial matter to search the Internet for highly qualified, context-sensitive results: a fan could find all the downloadable music in a given genre, a manufacturer could efficiently discover suppliers, travelers could easily choose a hotel room for an upcoming trip.==\n\nA world of exhaustive, reliable metadata would be a utopia. It's also a pipe-dream, founded on self-delusion, nerd hubris and hysterically inflated market opportunities.  BACK TO TOP]()\n\n- - - -\n\n# 2. The problems\n\nThere are at least seven insurmountable obstacles between the world as we know it and meta-utopia. I'll enumerate them below:\n\n- - - -\n\n## 2.1 People lie\n\nMetadata exists in a competitive world. Suppliers compete to sell their goods, cranks compete to convey their crackpot theories (mea culpa), artists compete for audience. Attention-spans and wallets may not be zero-sum, but they're damned close.\n\nThat's why:   A search for any commonly referenced term at a search-engine like Altavista will often turn up at least one porn link in the first ten results.  Your mailbox is full of spam with subject lines like \"Re: The information you requested.\"  Publisher's Clearing House sends out advertisements that holler \"You may already be a winner!\"  Press-releases have gargantuan lists of empty buzzwords attached to them.  \n\n==Meta-utopia is a world of reliable metadata. When poisoning the well confers benefits to the poisoners, the meta-waters get awfully toxic in short order.==\n\n### Robert Haisfield comments:\nthis is a user behavior / [[Behavioral Product Strategy]] problem. Dan Ariely and many others have research about honesty, and from what I read from Balaji, the blockchain and crypto may be a way to make it so you don't have to worry about people being honest. \n\n- - - -\n\n## 2.2 People are lazy\n\nYou and me are engaged in the incredibly serious business of creating information. ==Here in the Info-Ivory-Tower, we understand the importance of creating and maintaining excellent metadata for our information.==\n\n==But info-civilians are remarkably cavalier about their information. Your clueless aunt sends you email with no subject line, half the pages on Geocities are called \"Please title this page\" and your boss stores all of his files on his desktop with helpful titles like \"UNTITLED.DOC.\"==  \n\nThis laziness is bottomless. No amount of ease-of-use will end it. To understand the true depths of meta-laziness, download ten random MP3 files from Napster. Chances are, at least one will have no title, artist or track information -- this despite the fact that adding in this info merely requires clicking the \"Fetch Track Info from CDDB\" button on every MP3-ripping application.  Short of breaking fingers or sending out squads of vengeful info-ninjas to add metadata to the average user's files, we're never gonna get there.\n\n### Robert Haisfield comments:\nthis is a user behavior / [[Behavioral Product Strategy]] problem.\n\n- - - -\n\n## 2.3 People are stupid\n\n==Even when there's a positive benefit to creating good metadata, people steadfastly refuse to exercise care and diligence in their metadata creation. ==\n\n==Take eBay: every seller there has a damned good reason for double-checking their listings for typos and misspellings. Try searching for \"plam\" on eBay. Right now, that turns up nine typoed listings for \"Plam Pilots.\" Misspelled listings don't show up in correctly-spelled searches and hence garner fewer bids and lower sale-prices.== You can almost always get a bargain on a Plam Pilot at eBay.\n\nThe fine (and gross) points of literacy -- spelling, punctuation, grammar -- elude the vast majority of the Internet's users. To believe that J. Random Users will suddenly and en masse learn to spell and punctuate -- let alone accurately categorize their information according to whatever hierarchy they're supposed to be using -- is self-delusion of the first water. \n\n### Robert Haisfield comments:\nthis is a user behavior / [[Behavioral Product Strategy]] problem. You need some way to sort out the crap. Maybe [[crowdsource]] crap sorting would work. \n\n- - - -\n\n## 2.4 Mission: Impossible know thyself \n\nIn meta-utopia, everyone engaged in the heady business of describing stuff carefully weighs the stuff in the balance and accurately divines the stuff's properties, noting those results. \n\nSimple observation demonstrates the fallacy of this assumption. When Nielsen used log-books to gather information on the viewing habits of their sample families, the results were heavily skewed to Masterpiece Theater and Sesame Street. Replacing the journals with set-top boxes that reported what the set was actually tuned to showed what the average American family was really watching: naked midget wrestling, America's Funniest Botched Cosmetic Surgeries and Jerry Springer presents: \"My daughter dresses like a slut!\"  Ask a programmer how long it'll take to write a given module, or a contractor how long it'll take to fix your roof. Ask a laconic Southerner how far it is to the creek. Better yet, throw darts -- the answer's likely to be just as reliable.\n\nPeople are lousy observers of their own behaviors. Entire religions are formed with the goal of helping people understand themselves better; therapists rake in billions working for this very end.  Why should we believe that using metadata will help J. Random User get in touch with her Buddha nature? \n\n### Robert Haisfield comments:\nthis is a user behavior / [[Behavioral Product Strategy]] problem. ideally there are ways to infer metadata from people's revealed preferences. \n\n- - - -\n\n## 2.5 Schemas aren't neutral\n\nIn meta-utopia, the lab-coated guardians of epistemology sit down and rationally map out a hierarchy of ideas, something like this:  \n\nNothing: \n    Black holes\n\nEverything: \n    Matter:\n        Earth:\n            Planets\n            Washing Machines\n        Wind:\n            Oxygen\n            Poo-gas\n        Fire:\n            Nuclear fission\n            Nuclear fusion\n            \"Mean Devil Woman\" Louisiana Hot-Sauce\n\n In a given sub-domain, say, Washing Machines, ==experts agree on sub-hierarchies, with classes for reliability, energy consumption, color, size, etc.  This presumes that there is a \"correct\" way of categorizing ideas, and that reasonable people, given enough time and incentive, can agree on the proper means for building a hierarchy.==\n\n==Nothing could be farther from the truth.== Any hierarchy of ideas necessarily implies the importance of some axes over others. A manufacturer of small, environmentally conscious washing machines would draw a hierarchy that looks like this: [[Q- How do people come to agree on queryable schemas]]\n\nEnergy consumption:\n    Water consumption:\n        Size:\n            Capacity:\n                Reliability\n\nWhile a manufacturer of glitzy, feature-laden washing machines would want something like this:\n\n\nColor:\n    Size:\n        Programmability:\n            Reliability\n\n==The conceit that competing interests can come to easy accord on a common vocabulary totally ignores the power of organizing principles in a marketplace.==\n\n\n### Robert Haisfield comments:\nWhy should everything have to follow one agreed upon schema? Roam and [[P- Conor White-Sullivan]]'s critique of Wikipedia being a single source of truth where really you just need a way to navigate and evaluate many truths may be relevant here. ^72a291\n\n- - - -\n\n## 2.6 Metrics influence results\n\n==Agreeing to a common yardstick for measuring the important stuff in any domain necessarily privileges the items that score high on that metric, regardless of those items' overall suitability.==¬†\n\nIQ tests privilege people who are good at IQ tests, ==Nielsen Ratings privilege 30- and 60-minute TV shows (which is why MTV doesn't show videos any more -- Nielsen couldn't generate ratings for three-minute mini-programs, and so MTV couldn't demonstrate the value of advertising on its network)==, raw megahertz scores privilege Intel's CISC chips over Motorola's RISC chips.\n\nRanking axes are mutually exclusive: software that scores high for security scores low for convenience, desserts that score high for decadence score low for healthiness. ==Every player in a metadata standards body wants to emphasize their high-scoring axes and de-emphasize (or, if possible, ignore altogether) their low-scoring axes.==\n\nIt's wishful thinking to believe that a group of people competing to advance their agendas will be universally pleased with any hierarchy of knowledge. The best that we can hope for is a detente in which everyone is equally miserable.  \n\n### Robert Haisfield comments:\nreminds me of how many apps will show comparisons between competitors that shows themselves checking every box while their competitors only check a few, but all of the competitors do that! ^a9026e\n\n- - - -\n\n## 2.7 There's more than one way to describe something \n\n\"No, I'm not watching cartoons! It's cultural anthropology.\"  \"This isn't smut, it's art.\"  \"It's not a bald spot, it's a solar panel for a sex-machine.\"  Reasonable people can disagree forever on how to describe something. Arguably, your Self is the collection of associations and descriptors you ascribe to ideas. ==Requiring everyone to use the same vocabulary to describe their material denudes the cognitive landscape, enforces homogeneity in ideas.  And that's just not right.== ^7201b3\n\n- - - -\n\n# 3. Reliable metadata\n\nDo we throw out metadata, then?  Of course not. Metadata can be quite useful, if taken with a sufficiently large pinch of salt. The meta-utopia will never come into being, but metadata is often a good means of making rough assumptions about the information that floats through the Internet.\n\nCertain kinds of implicit metadata is awfully useful, in fact. Google exploits metadata about the structure of the World Wide Web: by examining the number of links pointing at a page (and the number of links pointing at each linker), Google can derive statistics about the number of Web-authors who believe that that page is important enough to link to, and hence make extremely reliable guesses about how reputable the information on that page is.\n\nThis sort of observational metadata is far more reliable than the stuff that human beings create for the purposes of having their documents found. It cuts through the marketing bullshit, the self-delusion, and the vocabulary collisions.\n\nTaken more broadly, this kind of metadata can be thought of as a pedigree: who thinks that this document is valuable? How closely correlated have this person's value judgments been with mine in times gone by? This kind of implicit endorsement of information is a far better candidate for an information-retrieval panacea than all the world's schema combined.\n ","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Micropublications-a-semantic-model":{"title":"R- Micropublications a semantic model","content":"\n### Micropublications: a semantic model\n- #[[references]]\n    - Title: Micropublications: a semantic model for claims, evidence, arguments and annotations in biomedical communications\n    - Meta:\n        - Tags: #context #Atomicity #Atomicity #compositionality #[[D/Synthesis Infrastructure]] #std/Micropublication\n        - Authored by:: [[Tim Clark]] [[Paolo Ciccarese]] [[Carole A. Goble]] \n        - Year: [[2014]]\n        - Publication: Journal of Biomedical Semantics\n        - URL: [Clark et al. (2014). Micropublications: a semantic model for claims, evidence, arguments and annotations in biomedical communications. Journal of Biomedical Semantics](https://doi.org/10.1186/2041-1480-5-28)\n    - Content\n        - Abstract\n            - Scientific publications are documentary representations of defeasible arguments, supported by data and repeatable methods. They are the essential mediating artifacts in the ecosystem of scientific communications. The institutional ‚Äúgoal‚Äù of science is publishing results. The linear document publication format, dating from 1665, has survived transition to the Web.\n    - #[[üìù lit-notes]]\n        - Minimal form of a [[std/Micropublication]] (p. 10) \n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F4RD5nUsFD2?alt=media\u0026token=f31624d5-1637-4859-adb5-b205701ac24e)\n        - Better form of [[std/Micropublication]]\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FXEIzMIbGby.png?alt=media\u0026token=52de2ef7-02d6-4154-bc9e-4c322bdd5071)\n        - Example [[std/Micropublication]] with a real scientific claim (p.15) \n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FhIBZKnpISn?alt=media\u0026token=056ff6ca-ed18-4166-8b61-fec4e142dc99)\n        - Introduced an interface for authoring of micropublications, integrating into [[sys/DOMEO]] - however, I'm not sure where it is now :( - last commit to [their github](https://github.com/domeo/domeo) was like 5 years ago, and I can't find any digital footprints anywhere else...\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Neural-Databases":{"title":"R- Neural Databases","content":"\n### Neural Databases\n\nURL - https://arxiv.org/pdf/2010.06973v1.pdf\n\n#### Core Questions\n[[Q- Can neural networks answer queries from natural language without a predefined schema]]?\n[[Q- What would a discourse graph without a predefined schema look like]]?\n\nrelated questions from Rob\n[[Q- What is the data structure of a graph built to facilitate decentralized knowledge synthesis]]\n[[Q- How might you allow people to query information without explicit knowledge of how that information is structured]]\n[[Q- How do people come to agree on queryable schemas]]\n[[Q- How do we solve the problem of different people referring to the same concept with different language]]\n\nrelated reading\n[[R- Polysemy and thought - Toward a generative theory of concepts]]\n\n\nWhat if there doesn't need to be a database schema? \n\u003eWe describe NeuralDB, a database system with no pre-defined schema, in which updates and queries are given in natural language. We develop query processing techniques that build on the primitives offered by the state of the art Natural Language Processing methods.\n\u003eWe begin by demonstrating that at the core, recent NLP transformers, powered by pre-trained language models, can answer select-project-join queries if they are given the exact set of relevant facts. However, they cannot scale to non-trivial databases and cannot perform aggregation queries.\n\u003eBased on these findings, we describe a NeuralDB architecture that runs multiple Neural SPJ operators in parallel, each with a set of database sentences that can produce one of the answers to the query. The result of these operators is fed to an aggregation operator if needed. \n\u003eWe describe an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators. Importantly, this algorithm can be trained by the Neural SPJ operator itself.\n\u003e In applying neural nets to data management, research has so far assumed that the data was modeled by a database schema\n\u003e What if, instead, data and queries can be represented as short natural language sentences, and queries can be answered from these sentences?\n\u003e The query processor of a NeuralDB builds on the primitives that are offered by the state of the art Natural Language Processing (NLP) techniques.\n\u003e Our experimental results suggest that it is possible to attain very high accuracy for a class of queries that involve select, project, join possibly followed by an aggregation.\n\n\nNeuralDB has no predefined schema, can be queried in natural language, is backed by NLP, and can extend to more domain knowledge with training. \n\u003eThe first, and most important benefit is that a NeuralDB, by definition, has no pre-defined schema. Therefore, the scope of the database does not need to be defined in advance and any data that becomes relevant as the application is used can be stored and queried.\n\u003eThe second benefit is that updates and queries can be posed in a variety of natural language forms, as is convenient to any user. In contrast, a traditional database query needs to be based on the database schema.\n\u003eA third benefit comes from the fact that the NeuralDB is based on a pre-trained language model that already contains a lot of knowledge.\n\u003eFurthermore, using the same paradigm, we can endow the NeuralDB with more domain knowledge by extending the pre-training corpus to that domain.\n\n\u003e The main goal of NeuralDB is to support data management applications where users do not need to pre-define a schema. Instead, they can express the facts in the database in any linguistic form they want, and queries can be posed in natural language. To that end, data and queries in a NeuralDB are represented as short sentences in natural language and the neural machinery of the NeuralDB is applied to these sentences.\n\nQ- What are neural databases good for?\n\u003e Given its benefits, Neural Databases are well suited for emerging applications where the schema of the data cannot be determined in advance and data can be stated in a wide range of linguistic patterns. A family of such applications arise in the area of storing knowledge for personal assistants that currently available for home use and in the future will accompany Augmented Reality glasses. In these applications, users store data about their habits and experiences, their friends and their preferences, and designing a schema for such an application is impractical. Another class of applications is the modeling and querying of political claims [46] (with the goal of verifying their correctness). Here too, claims can be about a huge variety of topics and expressed in many ways. \n\u003e Our first contribution is to show that state of the art transformer models [47] can be adapted to answer simple natural language queries. Specifically, the models can process facts that are relevant to a query independent of their specific linguistic form, and combine multiple facts to yield correct answers, effectively performing a join. \n\u003e Our second contribution is to propose an architecture for neural databases that uses the power of transformers at its core, but puts in place several other components in order to address the scalability and aggregation issues. Our architecture runs multiple instances of a Neural SPJ operator in parallel. The results of the operator are either the answer to the query or the input to an aggregation operator, which is done in a traditional fashion. Underlying this architecture is a novel algorithm for generating the small sets of database sentences that are fed to each Neural SPJ operator\n\u003e Finally, we describe an experimental study that validates the different components of NeuralDBs, namely the ability of the Neural SPJ to answer queries or create results for a subsequent aggregation operator even with minimal supervision, and our ability to produce support sets that are fed into each of the Neural SPJ operators. Putting all the components together, our final result shows that we can accurately answer queries over thousands of sentences with very high accuracy. To run the experiments we had to create an experimental dataset with training data for NeuralDBs, which we make available for future research.\n\nQ- What are neural databases bad at?\nQ- How large of a dataset could be used with a neural database?\n\u003e However, we identify two major limitations of these models: (1) they do not perform well on aggregation queries (e.g., counting, max/min), and (2) since the input size to the transformer is bounded and the complexity of the transformer is quadratic in the size of its input, they only work on a relatively small collection of facts. \n\u003e We also assume that pronouns (e.g., she, they) are not used or have been resolved in advance. The application of the rich body of work on entity resolution to NeuralDBs will be reserved for future work.\n\nMore research is needed:\n\u003e To fully realize the promise of NeuralDBs, more research is needed on scaling up NeuralDBs to larger databases, supporting more complex queries and increasing the accuracy of the answers. In particular, an interesting area of research noted in Section 5 is developing novel indexing techniques that enable efficient support set generation. \n\u003e Another exciting area to investigate is to consider other media in the database. For example, a database can also contain a set of images and some queries can involve combining information from language and from images. Such an extension would benefit from recent progress on visual query answering systems [3, 5].\n\u003e Finally, another interesting challenge concerns developing semantic knowledge that helps in identifying which updates should replace previous facts and which should not.\n\n\nWhat challenges arise with databases that NeuralDB solves?\n\u003e However, the following additional challenges arise in the context of NeuralDBs: \n\t\u003e ‚Ä¢ Unlike open-book QA, which typically requires extracting a span from a single document or predicting a token as an answer, answering queries in a NeuralDB may require processing a large number of facts and in some cases performing aggregations over large sets. \n\t\u003e ‚Ä¢ NeuralDBs do not enjoy the locality properties that usually hold in open-book QA. In NeuralDBs, a query may be dependent on multiple facts that can be anywhere in the database. In fact, by definition, the current facts in a database can be reordered and the query answers should not change. In contrast, in open-book QA, the fact needed to answer a given question is typically located in a paragraph or document with multiple sentences about the same subject where this additional context may help information recall. \n\t\u003e ‚Ä¢ When determining which facts to input to the transformer, NeuralDBs may require conditional retrieval from the database. For example, to answer the query Whose spouse is a doctor? we‚Äôd first need to fetch spouses and then their professions. In the NLP community this is known as multihop query answering [6], which has recently become an active area of research, but restricted to the case where we‚Äôre looking for a single answer. In NeuralDBs, we may need to perform multi-hops for sets of facts.\n\t\u003e A possible downside of using neural techniques in a database system is the potential for bias that might be encoded in the underlying language model.\n\t\t\u003e A possible approach to this important issue is to design a separate module that attacks the database with queries in order to discover hidden biases. Then, we could devise safeguards within the database that ensure that we don‚Äôt use such biased knowledge in answering queries. Developing these components is an area for future research.\n\nMethodology\n\u003e The Transformer model [47] is the most common neural architecture to operate on pre-trained language models based on the high accuracy it produces on downstream tasks including question answering on text. In our prototype experiments, detailed in Section 3, we demonstrate that these reasoning abilities enable the transformer architecture to generate correct answers to a number of queries that we might pose to a NeuralDB.\n\u003e Evaluating accuracy of answers. We measure the correctness of the answers generated by a NeuralDB by comparing them against reference data that contain the correct answers. The neural networks are trained with subset of the available data, leaving a portion of it held-out for evaluation, referred to as the test set. \n\u003e For most queries, we measure correctness using Exact Match (EM), which is 1 if a binary the answer string generated by the NeuralDB is exactly equal to the reference answer and 0 otherwise. This metric is used to score outputs where either a Boolean, null answer, string or numeric answer is expected. \n\u003e When a set of results is returned, we also consider the ùêπ1 score that weighs the precision and recall of the answer generated by the NeuralDB as compared to the reference data\n\u003e They are typically trained in one of two configurations: encoder only or encoder-decoder. In the former, each token is encoded to a vector representation that is used to predict a label. In the latter, used in sequence-to-sequence applications (e.g., question answering or machine translation), the decoder produces the output sequence. \n\u003e In both configurations, the transformer works in two phases. In the first phase, the transformer encodes the input into an intermediate representation z = (ùëß1, . . . , ùëßùëõ) where the dimension of the vector is fixed, typically where ùëëùëöùëúùëëùëíùëô = 768. In the second phase, the transformer decodes z to produce the output. For example, in sequence-to-sequence generation the output would be a sequence of tokens y = (ùë¶1, . . . , ùë¶ùëô ), ending with a special token.\n\u003e We generate training data in a controlled fashion using data from Wikidata [48] to express facts in natural language. Because of the scale of Wikidata, it is possible to generate large numbers of training instances about a wide range of relationships requiring very few templates.\n\u003e We believe that the initial experiment suggests the following: (1) if there were a way to feed the transformer the relevant facts from the database, it can produce results with reasonable accuracy, (2) aggregation queries need to be performed outside of the neural machinery, and (3) in order to handle queries that result in sets of answers and in order to prepare sets for subsequent aggregation operators, we need to develop a neural operator that can process individual (or small sets of) facts in isolation and whose results outputted as the answer or fed into a traditional (i.e. nonneural) aggregation operator.\n\nRelated Work\n\u003e NLP and data management. Bridging the gap between unstructured natural language data and database-style querying has been a longstanding theme in database research [15]. The work on information extraction has developed techniques for translating segments of natural language text into triples that can be further processed by a database system. Wikidata [48] itself is a social experiment where additions to the knowledge graph are encouraged to use already existing relation names if possible, thereby alleviating the need for information extraction. There has been significant work on translating queries posed in natural language into SQL queries on a database whose schema is known [4, 23, 58], with extensions to semi-structured data and knowledge bases [7, 30]. More recently, systems such as BREAK [57] and ShARC [41] have trained models to translate a natural language query into a sequence of relational operators (or variants thereof).\n\u003e While other works modeling the web as a knowledge bases have focused on combining multiple snippets of text together [44], their assumption is that the query is decomposed into a SPARQL program that is executed on pre-extracted information. Our innovation is that no latent program or structure is needed and that information extraction is dynamic and dependent on the query.\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Organizational-Memory-as-Objects-Processes-and-Trajectories":{"title":"R- Organizational Memory as Objects Processes and Trajectories","content":"- #[References]\n    - Title: Organizational Memory as Objects, Processes, and Trajectories: An Examination of Organizational Memory in Use\n    - Meta:\n        - Tags: #ref/Paper #[D/Synthesis Infrastructure]\n        - Authored by:: [Mark S. Ackerman] [Christine A. Halverson] \n        - Year: [[2004]]\n        - Publication: Computer Supported Cooperative Work (CSCW)\n        - URL: [Ackerman \u0026 Halverson (2004). Organizational Memory as Objects, Processes, and Trajectories: An Examination of Organizational Memory in Use. Computer Supported Cooperative Work (CSCW)](https://link.springer.com/article/10.1023/B:COSU.0000045805.77534.2a)\n    - Content\n        - Abstract\n            - For proper knowledge management, organizations must consider how knowledge is kept and reused. The term organizational memory is due for an overhaul. Memory appears to be everywhere in organizations; yet, the term has been limited to only a few uses. Based on an ethnographic study of a telephone hotline group, this paper presents a micro-level, distributed cognition analysis of two hotline calls, the work activity surrounding the calls, and the memory used in the work activity. Drawing on the work of Star, Hutchins, and Strauss, the paper focuses on issues of applying past information for current use. Our work extends Strauss' and Hutchins' trajectories to get at the understanding of potential future use by participants and its role in current information storage. We also note the simultaneously shared provenance and governance of multiple memories ‚Äì human and technical. This analysis and the theoretical framework we construct should be to be useful in further efforts in describing and analyzing organizational memory within the context of knowledge management efforts.\n    - lit-context\n        - #canonical ref for [context], helps flesh out how [[Z: Contextualizability is necessary for synthesis]]\n        - setting:: telephone hotline group (HLG) for human resources (e.g., benefits, personnel policies) in a well-established mid-sized (~1000+ employees) company in Silicon Valley\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FmipeLvxEgY.png?alt=media\u0026token=55f0a290-cbef-471d-8e5c-98c5025b1296) (p. 165)\n            - main participant whose transcript is analyzed (here called the HLG employee) was an experienced agent who had been at the company for 5 years, and at the hotline group for 1 year\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Ff0Dn98k5Mf.png?alt=media\u0026token=a7f0515a-563b-459a-9473-682b9031a93b) (p. 165)\n    -  lit-notes\n        - [observation-notes]\n            - A HLG employee drew on five discrete \"small memories\" in her handling of a hotline call, such as the telephone system's short-term memory of the group's activity, her own short-term memory, scrap paper, the CAll Tracking (CAT) system, and records in the CARL database, instead of a single monolithic memory\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FaM33W1lmur.png?alt=media\u0026token=283ac83a-fc6a-4d4e-b841-468c9c3f7243) (p. 168)\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FhfTG3GBanW.png?alt=media\u0026token=6aa3ad03-271a-4cc4-aa49-5ab2af2fb0d0) (p. 169)\n            - To properly deal with a discrepancy in benefits from the caller, a HLG employee needed to [reuse] information from the CARL database, among other sources of information, to create an escalation to the benefits group. To do this, she needed important [context] that was missing from the CARL record itself, such as details of the record's creation or maintenance (was it authoritative?), and any circumstances surrounding the caller's employment. The HLG employee dealt with this missing context by consulting an expert (a senior agent) and her own memory, rather than searching databases for additional information, even though that information could in principle be in there.\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FfTEHpl6sdR.png?alt=media\u0026token=cd09b0e1-d6f1-4f6e-b1a7-d2a5cdeecf57) (p. 170)\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FFJ8OVy5uLF.png?alt=media\u0026token=d01731dd-fef4-4e9a-98eb-9fd1c7ef0ac8) (p. 172)\n        - The issue at play in one of the case studies was that the caller was trying to use benefits from her health care coverage, but was blocked by the provider, even though she (correctly) believed that she has coverage through the employer. This was borne out in the company's CARL database, but her record was absent in the provider's db. The HLG employee's task was to resolve this discrepancy, and \n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FuAYxtIkOs8.png?alt=media\u0026token=caaa771d-a892-4c8e-81cb-b80bad1b07d4)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FcalTN86A9X.png?alt=media\u0026token=c17fdf9c-8d28-4298-9a33-71bad826c8b7) (p. 167)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F-anq5_pC_j.png?alt=media\u0026token=06a2976a-85b3-4a9c-98b6-249d9c46ba3f) (p. 168)\n- #References\n    - Title: Organizational Memory as Objects, Processes, and Trajectories: An Examination of Organizational Memory in Use\n    - Meta:\n        - Authored by::  [Mark Ackerman] ,  Christine Halverson\n        - Year: [2004]\n        - Publication: Computer Supported Cooperative Work (CSCW)\n        - URL: https://link.springer.com/article/10.1023/B:COSU.0000045805.77534.2a\n        - Citekey: ackermanOrganizationalMemoryObjects2004\n    - Content\n        - Placeholder\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Organizing-knowledge-with-multi-level-content":{"title":"R- Organizing Knowledge with multi-level content","content":"\n\n# Organizing Knowledge with multi-level content:\n## Making knowledge easier to understand, remember, and communicate\n\n## Notes:\n\nThis paper discusses how to best communicate multi-level content. Multi-level content refers to the idea that there are multiple levels at which you can describe a concept. For example, a paper's abstract may be understood as a high level overview of the paper's contents. For some, the abstract might be enough, but for others, they will need to read the paper for low level details.\n\nThe author provides The Cognitive Bias Codex as an example of multi-level content communication. Going from the outer rings to the inner rings, there are four categories that describe 20 subcategories which describe 180 specific cognitive biases. This allows readers to gain a high level conceptual overview and then narrow in on the specific subjects they care about.\n![[Cognitive Bias Codex.png]]\n\nThe paper refers to \"the problem of multiple knowledge levels.\" Essentially, knowledge has a hierarchical structure, and if high level overviews or low level details are missing, it becomes harder to understand. Additionally, if high level overviews are visually undifferentiated from low level details, then the reader might waste their time trying to understand the domain in a more difficult way.\n\nIn some ways, [[P- Francis Miller]] is attempting to solve a problem of [[Search Behavior]] in an exploratory search setting. Essentially, how can people find what they are looking for, even when they do not know how to describe what they are looking for? Starting with broad questions and progressively drilling down into specific domains may be an effective search strategy. After all, [[C- An exploratory search system should help the reader cumulatively gain information]].\n\n[[Start Here|Writing in hypertext]] attempts to solve the same problem. Whenever the reader is unclear about a concept, they can click on the hyperlink, read about it, and then bring their new knowledge back to the note that they were originally reading. When the hypertext links have descriptive titles, they provide an \"[[Search Behavior|information scent]]\" that informs the reader whether it will be worthwhile to follow that link's path.\n\nHowever, hypertext differs from what Miller proposes insofar as it describes a flat information architecture. In hypertext, you can start on any page and explore outwards. Compared to Miller's abstract to specific search strategy, however, it may be inefficient.\n\nOne solution to the problem of multi-level content is apparent in the structure of the paper. First there is a summary of the paper as a whole, followed by a summary of each of the four parts, followed by each of the four parts. While the paper itself is more than 90 pages, the expectation is that people will only need to read the subset of those pages necessary to satisfy their interests.\n\nAnother idea present in the paper is a [[ZUI|Zoomable User Interface]]. While the term was not explicitly mentioned, the author alluded to the concept. Imagine a digital map. When you zoom in, you see the names of specific streets and stores, but when you zoom out, those fade away and are replaced with the names of towns and cities. Now imagine a discourse graph where you can zoom in or out to go from increased detail to broader abstractions.\n\nThis implies the necessity of [[I- Summarization as a primitive]]. \n\n[[C- A key requirement to participating in a discourse graph for a specific domain is knowing the vocabulary used in that graph]]. ZUIs that support abstract to specific search strategies can facilitate people learning the vocabulary they need.\n\nThe paper also shows multiple types of knowledge structure which may be taken into consideration to answer [[Q- What is the data structure of a graph built to facilitate decentralized knowledge synthesis]].\n![[Pasted image 20211019173927.png]]\n![[Pasted image 20211019174114.png]]\n\nAdditionally, Miller makes the argument that the very act of making knowledge maps that communicate the multiple-level content is rewarding and fruitful. When considering the question [[Q- What are powerful interfaces for entering information into a discourse graph]] with the goal of promoting synthesis, we should bear this in mind. Map making is itself a primitive. Zooming It allows you to identify gaps and patterns that would otherwise remain undiscovered without a forcing function to lay out all of the information and its structure from top to bottom. \n\n![[Pasted image 20211019174658.png]]\n\n[[Figma]] now lets you zoom in and out on comment threads and clusters. Notice how I click on the middle right DR and that zooms into a cluster of 4 comment threads.\n![[Pasted image 20211129114041.png]]\n\n\n![[Pasted image 20211129114058.png]]\n\n![[Pasted image 20211202140908.png]]\n\n![[Pasted image 20211202140929.png]]\n![[Pasted image 20211202140919.png]]\n\n\n![[Pasted image 20211202140855.png]]\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Prune-Structural-Editor":{"title":"R- Prune Structural Editor","content":"\n[[C- A structural editor can make a DSL approachable to end-users]] [[structural editor]]\n\nPrune: A Code Editor that is Not a Text Editor\n\nCo-authored with [Thiago Hirai](https://www.facebook.com/thiago.hirai)\n\nYou want to change the tree structure of a program. You figure out what series of text editing operations, operations that manipulate a 2D grid of characters, are equivalent to the change in the tree you desire. What a waste of time and energy.\n\n![](https://scontent-lax3-2.xx.fbcdn.net/v/t1.18169-9/11887919_10153576066383675_9029286239017821809_n.jpg?_nc_cat=107\u0026ccb=1-3\u0026_nc_sid=abc084\u0026_nc_ohc=uQCinam2-PUAX-D7YgQ\u0026_nc_ht=scontent-lax3-2.xx\u0026oh=3b4eb7a08948d0f033674f4ccf8acc9b\u0026oe=60D961C1)\n\n_Programmers waste energy translating tree transformations into character grid transformations_\n\nReligious editor wars are a symptom that the whole paradigm of editing programs with a text editor is broken. Text editors for code have been universally used for decades, however.\n\nPrune lets programmers edit the tree structure directly using commands invoked by typing and rendering the resulting code in a familiar textual format. However, it's time for someone else to pick up the torch of tree-based editing.\n\n![](https://scontent-lax3-1.xx.fbcdn.net/v/t1.18169-9/11924259_10153576067688675_6046809493243199041_n.jpg?_nc_cat=110\u0026ccb=1-3\u0026_nc_sid=abc084\u0026_nc_ohc=3owH-HCk58kAX9ULsSF\u0026_nc_ht=scontent-lax3-1.xx\u0026oh=296e8d6ac531295e603b1e09f6b3573c\u0026oe=60D997A3)\n\n_In Prune, programmers transform the tree directly_\n\n## **The Dream**\n\nPrune is a code editor that operates directly on the tree structure of the code. While it renders code in a familiar textual format, it doesn't offer text editing operations. Programs are created and edited by creating, deleting, and re-organizing nodes in the program's abstract syntax tree.\n\nPrune grew out of hundreds of hours of observing programmers editing programs with text editors. Proficient users spent considerable energy learning and optimizing their use of the editor. Unskilled users spent considerable energy using the editor badly. Perhaps the problem is not their skill, but the whole idea of using a text editor, a very general tool, for the specific task of program editing.\n\nStructure editors have been around for decades, yet they never caught on for mainstream usage. Prune would have to be substantially better to have a chance of being adopted. While our overall goal was to reduce the cognitive load of manipulating programs, \"cognitive load\" is hard to measure, so we chose three quantifiable goals:\n\n-   Fewer keystrokes\n-   Fewer errors\n-   Greater overall efficiency\n\n## **Tree Transformations**\n\nThe fundamental operation in Prune takes one program tree and produces another. For example, making a statement conditional is a single operation:\n\n```\ncache = computeValue; ==\u003e if (!cache) {   cache = computeValue; }\n```\n\nThis transformation is one of a class of \"wrap\" transformation--take the current selection, make it a child of a new node, and replace the selection with the new node. Other \"wraps\" are looping over a statement or statements, surrounding statements with an exception handler, and returning the value of an expression. Mirror \"unwrap\" transformations replace a parent node with one or more of its children, such as making a statement unconditional only executing it once instead of in a loop. Other transformations create nodes, delete nodes, and reorder nodes.\n\nThe transformations are all atomic, in that they take a syntactically correct program and produce another syntactically correct program. This is one way Prune helps reduce errors--programs are syntactically correct by construction. The transformations compose--each takes a tree and a selection and produces a tree and a selection that can act as the input to another transformation. Once we had a basic library of transformations, we found that we could sometimes synthesize new transformations by composition instead of having to write new code to manipulate the tree. The transformations are also reversible, a property lacking in many text editors that have been enhanced to work on code.\n\n## **[[I should be able to leave a hole to fill in the blanks for an idea or domain|TBD Node]]**\n\nOne serendipitous discovery that deserves special mention is the TBD node, used above. Some transformations require two or more parameters. For example, creating a binary expression requires a left side, a right side, and an operator. In a panic we added a special node to the tree representing code to be filled in later. We found it useful in many situations and it never intruded on our work as users of Prune. It did require us to modify the pretty printer we used (esprima), but it was well worth it.\n\nOne piece of interface philosophy we carried through Prune is that the editor should allow the user to build a tree by introducing nodes in any order. We have so little experience coding through tree transformations that we have no confidence we have found the most efficient sequence (if one exists). \n\n## **Typeahead**\n\nOne of the risks we identified at the beginning of the project was the number of distinct transformation. If hundreds or thousands of distinct transformations were required to program effectively no one would want to learn them all.\n\nIt turns out that a hundred or so transformation suffice for editing JavaScript (our source and our target language). Even a hundred transformations is a lot to remember. When we measured our performance editing samples of code, we found selecting transformations and entering identifiers to be a bottleneck.\n\nTo accelerate editing and make Prune feel more familiar, we created a typeahead. When the user begins typing, Prune can guess what tree transformation is implied. For example, typing \"if\" implies a conditional, a guess that is confirmed when a space follows. The If transformation is invoked at that point, leaving the typeahead ready for input at the condition. \n\nWith practice, pruning with the typeahead feels a little like just typing. However, all the \"noise\" characters, put there just to make the parser happy, like parentheses and curly braces appear automatically. Typeahead adds to the efficiency of editing, reduces keystrokes, and makes Prune more approachable.\n\n## **The Project**\n\nPrune began as a part-time project, motivated, as mentioned earlier, by watching programmers struggle with text editors. The first prototype was completely simulated on paper, just enough to validate that reducing the keystroke count was possible.\n\nWith that bit of evidence in hand, the authors applied for a Hackamonth. After a year with the same team, Facebook engineers are encouraged to take a month and work on something unrelated. It's considered reasonable to fail attempting an ambitious hack, so off we went. Early in the project we created a Facebook-only group devoted to Prune, which slowly accumulated interested members as we posted the results of experiments, demo videos, and the occasional plea for someone to put us out of our misery.\n\nAfter the first month we had a version of Prune that we were able to frequently use to add features to Prune. However, reflection on usage showed that our pure \"invoke transformations through key bindings\" wasn't going give us the efficiency we wanted. We applied for a second month to implement and measure the typeahead.\n\nAt the end of the second month the developers and those who were interested in embedding Prune in their tools met to discuss its future. In the end we struck a rock we had identified in our first presentation--programmers don't spend that much time manipulating programs compared to all the other things they do. Doing the math, enabling a few hundred programmers to do a 50% better job (an ambitious goal) of a task requiring 10% of their time just doesn't make economic sense.\n\nHowever, we were surprised by what we learned working on Prune and we hope the ideas live and grow. That's why we are publishing this, in hopes that a graduate student somewhere won't mind a project that takes over his or her life and brings joy to (eventually) millions.\n\n## **The Lessons**\n\n-   Pure tree transformation is a viable option for editing code. The goals of reducing keystrokes and errors are within reach.\n-   Insisting on a pure tree transformation-based editor accelerated our research. If we had a text editor to fall back on, we wouldn't have thought hard enough about the problems that were eventually solved by the typeahead.\n-   Dogfooding is a powerful resource allocation heuristic. A complete code editor is a sizeable beast, but implementing what we needed to make the next change to Prune in Prune gave us a series of bite-sized features in a reasonable order. Bad experiences helped us quickly eliminate bad ideas.\n-   Structure research around questions, not features. We were able to get answers to critical questions about efficiency quickly because we were explicit about what questions we were asking. Working features to completion would have delayed, by months or years, those answers.\n-   Collaborate. Most Hackamonths are fairly solitary affairs, with an engineer seconded to a host team but running a separate project. We coded together daily in spite of our geographical split, which resulted in faster learning, more energy, and fewer dead-ends.\n-   We didn't miss formatting. We are both fussy about code formatting, but almost as soon as we were constrained to what the pretty-printer (esprima) gave us, we didn't waste any more thought on it.\n-   Typing is an effective way of identifying and invoking tree transformations.\n\n## **Future**\n\nOne of the curses and blessings of programming at Facebook is there is no shortage of impact work, so both of us reluctantly decided to move on. We still believe in the promise of tree-based editing and would love to use a completed version of Prune. We hope to encourage someone out there to take up the torch. Here are a few tips if it's you:\n\n-   Target a tablet first. Going head-to-head with vim and emacs is always going to be tough. Deliver on an input-deprived format like tablets, though, and you increase your chance of success. Prune's sparing use of input may also open up the possibility of voice input or other ways of making programming accessible to programmers who find typing difficult.\n-   Use a declarative language to define transformations. We defined out transformations imperatively, which was tedious and error prone.\n-   Use a predictive parser, one that can tell you what characters can legally come next and what tree transformations each character implies. We wrote an ugly, fragile state machine for our typeahead, which quickly became a source of pain and shame.\n-   Try to use the language-oriented tools from JetBrains. It seems like they have infrastructure that might make Prune an order of magnitude easier to implement.\n-   Implement macros. Letting programmers compose their own sequences of composable transformations seems like a good way to learn what operations are needed.\n-   Reuse the transformations. It seems like it should be possible to write a version control system that merges streams of transformations instead of textual diffs, potentially reducing the number of merge conflicts.\n-   Code tree transformations declaratively. We coded them imperatively and it took much too long.\n-   To further reduce the input required, consider using machine learning to predict which transformation will come next. Tune it well enough and much of programming could become \"yep, yep, yep\", saving the hard thinking for the hard problems.\n\nHappy pruning.\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Reasons-for-the-use-and-nonuse-of-electronic-journals-and-databases":{"title":"R- Reasons for the use and non‚Äêuse of electronic journals and databases","content":"- #references\n    - Title: Reasons for the use and non‚Äêuse of electronic journals and databases: A domain analytic study in four scholarly disciplines\n    - Authored by::  Sanna Talja ,  Hanni Maula\n        - Tags:: #[[D/Synthesis Infrastructure]] #[[Domain-Analysis]]\n    - Year: 2003\n    - Publication: [[Journal of Documentation]]\n- Reading notes\n    - Key ideas:\n        - [[paradigmatic relevance]] is an important and distinct notion of relevance (compared to [[Topical Relevance]]) in scholarly information seeking (p. 676)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FU9qtg2BxYc?alt=media\u0026token=70f344e8-d8c9-4ec1-83f3-82f47b57223c)\n            - Some kinds of information seeking strategies are better fits for [[paradigmatic relevance]]\n                - #Claim Subject headings and keyword searches are more effective for domains with high [[Topical Relevance]]; in domains characterized more by [[paradigmatic relevance]], these stragies are less useful, and other strategies like [[chaining]] or [[Browsing]] are more powerful. (p. 683)\n                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FLF1wkGRf96?alt=media\u0026token=11d43d24-35ab-4e25-89b1-27cf4ad94bd6)\n                    - Similar result to [[@bates2002speculations]]'s hypothesized advantage of [[chaining]] in high-[[Scatter]] domains\n            - Potentially related to idea of the degree of [[Scatter]] in a domain\n        - Tricky to draw boundaries around what we mean by [[Domain]], but it's supposed to be above an individual scholar, and includes some the scholar's [[context]], but more granular than a \"field\"\n        - One interesting finding re: field size (contra [[R: bates2002speculations]])\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Replication-Communication-and-the-Population-Dynamics-of-Scientific-Discovery":{"title":"R- Replication Communication and the Population Dynamics of Scientific Discovery","content":"","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Roam-can-loosely-be-considered-a-DSL-with-a-structural-editor":{"title":"R- Roam can loosely be considered a DSL with a structural editor","content":"\n[[P- Elzr|Eli Parra]]'s [twitter thread](https://twitter.com/elzr/status/1378821500246065154) visualizes how [[Roam Research| Roam Research]] can be expanded upon through its base structure. Before introducing his core claims, he walks through a history of 1-way linking and why it allowed the web to scale, but has prevented progress in our ability to manipulate data and present different views. The web was able to scale aggressively because of 1-way links, but [[C- Claims gain implict meaning through their backlinks|claims gain implicit meaning through their backlinks]], and added [[C- Context is necessary for knowledge reuse|context is necessary for knowledge reuse]]. The invention of wikilinks was a step to enable Roam's core structure of bidirectional linking.\n\nRoam's adaptation is making 2-way links a first class citizen. Combined with the ability for end users to program their own templates and workflows, Roam [[C- A DSL allows people to expand their use cases far beyond the imagination of the designer|allows people to expand their use cases far beyond the imagination of the designer]].\n\nThis speaks to an idea of [[I- A DSL for a discourse graph with information entry, visualization, and retrieval|a DSL for a discourse graph with information entry, visualization, and retrieval]] being part of the design of a decentralized discourse graph. [[C- Power users want to directly interface with the data structure of the app|Power users want to directly interface with the data structure of the app]], and tools like [[Roam Research|Roam Research]] allow that freedom. \n\nBy creating an automated reference list of all backlinks, Roam provided insight as to what was connected to each page. \n![[Pasted image 20210923133420.png]]\nTaken further, Roam's block structure allows users to narrow the scope to provide precise context, or as [[P- Elzr|Eli Parra]] says, a \"bookmark or an \"anchor\" back to its context.\"\n![[Pasted image 20210923133434.png]]\n\nRoam's strength is enabling humans to write into graphs in a way that's clear to us and parsable by computers. Because of the tag, wikilink, attribute structure, views are flexible and interchangable. A user can move seamlessly between a diagram, graph, or link, based on the end user programming. \n\n![[Pasted image 20210923142353.png]][+](https://twitter.com/elzr/status/1378821573516349443?s=20)\n\nRoam's query, table, diagram \u0026 kanban functions allow us to peek at what else may be possible by writing directly into the program's structural language. The end user is given the power to manipulate their experience through the structure of the data itself. This speaks downstream to the idea of [[I- A structural editor for data structures|a structural editor for data structures]].","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-ScholOnto-an-ontology-based-digital-library-server-for-research-documents-and-discourse":{"title":"R- ScholOnto an ontology-based digital library server for research documents and discourse","content":"\n- #[[references]]\n    - Title: ScholOnto: an ontology-based digital library server for research documents and discourse\n    - Meta:\n        - Authored by:: [[Simon Buckingham Shum]] [[Enrico Motta]] [[John Domingue]] \n        - Year: [[2000]]\n        - Publication: International Journal on Digital Libraries\n        - URL: [Shum et al. (2000). ScholOnto: an ontology-based digital library server for research documents and discourse. International Journal on Digital Libraries](https://link.springer.com/article/10.1007/s007990000034)\n    - Content\n        - Abstract\n            - . The internet is rapidly becoming the first place for researchers to publish documents, but at present they receive little support in searching, tracking, analysing or debating concepts in a literature from scholarly perspectives. This paper describes the design rationale and implementation of ScholOnto, an ontology-based digital library server to support scholarly interpretation and discourse. It enables researchers to describe and debate via a semantic network the contributions a document makes, and its relationship to the literature. The paper discusses the computational services that an ontology-based server supports, alternative user interfaces to support interaction with a large semantic network, usability issues associated with knowledge formalisation, new work practices that could emerge, and related work.\n\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Scholars-Before-Researchers":{"title":"R- Scholars Before Researchers","content":"\n-   #[[references]]\n    -   Title: Scholars Before Researchers: On the Centrality of the Dissertation Literature Review in Research Preparation\n    -   Meta:\n        -   Tags: #[[references]] #[[D/Synthesis Infrastructure]]\n        -   Authored by:: [[David N. Boote]] [[Penny Beile]]\n        -   Year: [[2005]]\n        -   Publication: Educational Researcher\n        -   URL: [Boote \u0026 Beile (2005). Scholars Before Researchers: On the Centrality of the Dissertation Literature Review in Research Preparation. Educational Researcher](http://journals.sagepub.com/doi/10.3102/0013189X034006003)\n    -   Content\n        -   Abstract\n            -   undefined\n    -   #lit-context\n        \n    -   #[[üìù lit-notes]]\n        -   the level of synthesis in regular manuscripts frequently subpar (p.4)\n            -   #ClaimSecondary\n            -   ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FNbJN82GKeI.png?alt=media\u0026token=94bb3cf6-b064-4fe9-a64b-7b03468bd692)\n            -   cites #[[@alton-leeTroubleshooterChecklistProspective1998]], who found that, out of 369 criticisms in 142 reviews for 58 manuscripts submitted for review to Teaching and Teacher education, approximately 33% ({{[[calc]]: (31+29+23+21+20)}} / 369) were directly related to inadequacies in [[synthesis]]\n        -   Developed a rubric for scoring literature reviews, including a category for [[synthesis]]\n            -   ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FyPK1uMfJyb?alt=media\u0026token=4a2ebef2-85f1-48e7-b5a2-8a5ad89d5c4d)\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Sharing-Knowledge-and-Expertise-The-CSCW-View-of-Knowledge-Management":{"title":"R- Sharing Knowledge and Expertise The CSCW View of Knowledge Management","content":"- #References\n    - Title: Sharing Knowledge and Expertise: The CSCW View of Knowledge Management\n    - Meta:\n        - Tags:: #[References] #[D/Synthesis Infrastructure] #[D/KNEXT] #ref/Paper\n        - Authored by::  [Mark Ackerman] ,  Juri Dachtera ,  Volkmar Pipek ,  [Volker Wulf]\n        - Year: [2013]\n        - Publication: [Computer Supported Cooperative Work (CSCW)]\n    - Content:\n        - https://drive.google.com/file/d/1e0TVK1wYSxCwDkskVnmCMLsZlDWQA_7j/preview\n    - lit-context\n        - #canonical paper in the lab\n    - lit-notes\n        - In general, CSCW research on knowledge sharing has moved from a \"repository model\", which focused on externalizing knowledge in documents and databases, to an \"expertise sharing\" model, which focuses on helping people find relevant knowledge from other knowledgeable people directly  p.532-533\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FHc-RAqA1r5?alt=media\u0026token=6554ee91-3cff-4f5b-a346-6ba0cb8764e1)\n        - The shift from the \"[repository model]\" of CSCW work to focus on \"[expertise sharing]\" was stimulated in large part by rich discoveries about the large extent to which useful knowledge is tacit and situated [[tacit knowledge]] p.547\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fj8jz9iLumu?alt=media\u0026token=aad30f8d-15a6-4ce2-8d6d-f0040f7dcb90)\n            - Cites [[R- Institutional Ecology Translations and Boundary Objects]] to note that [boundary object]s were a key learning from the first generation \"repository models\" to motivate the shift to [Expertise Sharing]\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FNytKlvx_Gq.png?alt=media\u0026token=03824ef6-44f8-4172-8c54-b8dc04c83bd7)\n                - [boundary object]s derive part of their power (for facilitating cross-organizational/boundary coordination and work) by being \"weakly structured in common use, and strongly structured in specific use\"\n        - [[C- Specifying context for future reuse is costly]]\n            - #ClaimSecondary people find adding meta-data to be laborsome [[R- Context Grabbing Assigning Metadata in Large Document Collections]]\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FFvtyx1Wepe.png?alt=media\u0026token=fee6554d-969c-4033-a6b7-b6b9d5e5a32b) (p. 540)\n            - #ClaimSecondary people tend to use the category with the lowest cognitive effort [[R- Down in the (Data)base(ment)]] \n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FFvtyx1Wepe.png?alt=media\u0026token=fee6554d-969c-4033-a6b7-b6b9d5e5a32b) (p. 540)\n                - [[R- Down in the (Data)base(ment)]] call these \"residual categories\"\n                - it can be quite hard to standardize (and therefore make efficient) expressions of [context] (e.g., in a dropdown menu of possible \"contexts\")","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Sorting-Things-Out-Classification-and-Its-Consequences":{"title":"R- Sorting Things Out Classification and Its Consequences","content":"\n-   Title: Sorting Things Out: Classification and Its Consequences\n-   Meta:\n    -   Tags: #ref/Book\n    -   Authored by:: [[Geoffrey C Bowker]] , [[Susan Leigh Star]]\n    -   Year: [[2000]]\n    -   Publication: no_info\n    -   URL:\n    -   Citekey: bowkerSortingThingsOut2000\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Student-readers-use-of-library-documents":{"title":"R- Student readers use of library documents","content":"\n- #references\n    - Title: Student readers' use of library documents: implications for library technologies\n        - Tags:: #[[references]] #[[active reading]] #[[D/Synthesis Infrastructure]]\n    - Authored by::  Kenton O'Hara ,  Fiona Smith ,  William Newman ,  Abigail Sellen\n    - Year: 1998\n    - Publication: [[conf/CHI]] '98\n    - URL: https://doi.org/10.1145/274644.274678\n- #lit-context\n    - Unassuming title: the sample was actually PhD students doing their knowledge work!\n    - Method: diary study\n    - Cited by [[@tashmanLiquidTextFlexibleMultitouch2011]]\n- #[[üìù lit-notes]]\n    - TONS of useful info about [[context]]\n        - Examples\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FbnNNNQdAqR?alt=media\u0026token=afd418c2-2f3f-4958-aad2-1feb8a26b355) (p.5)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FSgLyZ1i8Xr?alt=media\u0026token=1454d34d-de0d-4916-adf7-10984f899804)\n            - (p4)\n    - TONS of useful info about the desirable [[affordances]] of paper\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FAzATTY1Hfc?alt=media\u0026token=7735f3ca-53a7-4fae-9121-7939ce39dd41)\n        - It's not paper per se, but the ease with which people can develop/express their fluid interactions with the texts\n            - Again, cf. [[Z: Gracefully integrating formalism into interactive systems is hard]]\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-The-Cost-Structure-of-Sensemaking":{"title":"R- The Cost Structure of Sensemaking","content":"\n- #References\n    - Title: The Cost Structure of Sensemaking\n    - Meta\n        - Tags:: #[References] #[D/Synthesis Infrastructure] #sensemaking\n        - Authored by:: [Daniel M. Russell], [Mark J. Stefik], [Peter Pirolli], and [Stuart K. Card]\n        - Year: 1993\n        - Publication: [conf/CHI]\n        - Links:\n            - [PDF](https://drive.google.com/file/d/131ppP9hcxLxLy3ZC0cPfKa_XqzpSWQlg/view)\n        - Bibtex key: russellCostStructureSensemaking1993\n        - Content\n            - PDF\n                - {{iframe: https://drive.google.com/file/d/1qQCp-xopjVXYWQqd-2y6mIPjn4ZaSdQA/preview}}\n    - lit-context\n        - #canonical paper for [sensemaking] in [HCI] stream\n    - lit-notes\n        - Introduces the [Learning Loop Complex] and idea of [encodon] as bridge between data and representations ([Schema]s)that are then used for downstream tasks during [sensemaking] (p. 271)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FLdc6dvd3SX?alt=media\u0026token=daba4a52-0c6f-4ab8-b781-ede3d49e4558)\n        - Also analyzes the [cost structure] of [sensemaking] and uses that to identify the most impactful ways to ease the process of sensemaking (in their case, it was data extraction)\n            - They find that, for the cases they study, extracting data is the most costly step for [sensemaking] (p.273). \n                - This is very similar to the idea of the challenges to sastifying [[C- Composability facilitates synthesis]], and evokes the same feeling as what happens in [systematic review]s, as reported by [[R- Enslaved to the Trapped Data]]\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FEn4rDIQNGn.png?alt=media\u0026token=d5e18cdb-710c-490d-8138-7b086de05026)\n                - cf. #[[Z: Gracefully integrating formalism into interactive systems is hard]]\n                - also [[C- Specifying context for future reuse is costly]]","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-The-SWAN-biomedical-discourse-ontology":{"title":"R- The SWAN biomedical discourse ontology","content":"\n### The SWAN biomedical discourse ontology\n\nURL - http://www.sciencedirect.com/science/article/pii/S1532046408000580\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-The-anatomy-of-a-nanopublication":{"title":"R- The anatomy of a nanopublication","content":"\n- #references\n    - Title: The anatomy of a nanopublication\n    - Meta:\n        - Tags: #ref/Paper #std/Nanopublications\n        - Authored by::  Paul Groth ,  Andrew Gibson ,  Jan Velterop\n        - Year: [[2010]]\n        - Publication: Information Services \\\u0026 Use\n        - URL: https://content.iospress.com/articles/information-services-and-use/isu613\n        - Citekey: grothAnatomyNanopublication2010\n    - Content\n        - Placeholder\n        - Abstract\n            - As the amount of scholarly communication increases, it is increasingly difficult for specific core scientific statements to be found, connected and curated. Additionally, the redundancy of these statements in multiple fora makes it difficult to deter\n\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-The-minds-eye-in-chess":{"title":"R- The minds eye in chess","content":"\n\n\n\n-   Which itself goes back fruther to [[de Groot]], back in the mid-1960's\n    -   ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FCDetOUrlhl.png?alt=media\u0026token=b959da26-77bc-48d9-9fe7-e0f8d5db6626)","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-The-sensemaking-process-and-leverage-points-for-analyst-technology":{"title":"R- The sensemaking process and leverage points for analyst technology","content":"\n-   #[[references]]\n    -   Title: The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis\n    -   Meta:\n        -   Authored by:: [[Peter Pirolli]] [[Stuart Card]]\n        -   Year: [[2005]]\n        -   Publication: Proceedings of international conference on intelligence analysis\n        -   URL: [Pirolli \u0026 Card (2005). The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis. Proceedings of international conference on intelligence analysis](undefined)\n    -   Content\n        -   Abstract\n            -   undefined","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Theoretical-musings":{"title":"R- Theoretical musings","content":"  \n#references\n\nTitle: Theoretical musings\n\nMeta:\n\nAuthored by: [[Eve Marder]]\n\nYear: [[2020]]\n\nPublication: eLife\n\nURL: [Marder (2020). Theoretical musings. eLife](https://doi.org/10.7554/eLife.60703)\n\nContent\n\nAbstract\n\nThere is more to theory in biology than replicating the results of experiments ‚Äì the best theory papers help experimentalists to identify which of their results might be general and to plan a path through the maze of all possible future experiments.","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Theory-Before-the-Test":{"title":"R- Theory Before the Test","content":"\n-   #[[references]]\n    -   Title: Theory Before the Test: How to Build High-Verisimilitude Explanatory Theories in Psychological Science\n    -   Meta:\n        -   Tags:: #synthesis #[[D/Synthesis Infrastructure]]\n        -   Authored by:: [[Iris van Rooij]] [[Giosu√® Baggio]]\n        -   Year: [[2021]]\n        -   Publication: Perspectives on Psychological Science\n        -   URL: [van Rooij \u0026 Baggio (2021). Theory Before the Test: How to Build High-Verisimilitude Explanatory Theories in Psychological Science. Perspectives on Psychological Science](https://journals.sagepub.com/doi/abs/10.1177/1745691620970604)\n    -   Content\n        -   Abstract\n            -   Drawing on the philosophy of psychological explanation, we suggest that psychological science, by focusing on effects, may lose sight of its primary explananda: psychological capacities. We revisit Marr‚Äôs levels-of-analysis framework, which has been remarkably productive and useful for cognitive psychological explanation. We discuss ways in which Marr‚Äôs framework may be extended to other areas of psychology, such as social, developmental, and evolutionary psychology, bringing new benefits to these fields. We then show how theoretical analyses can endow a theory with minimal plausibility even before contact with empirical data: We call this the theoretical cycle. Finally, we explain how our proposal may contribute to addressing critical issues in psychological science, including how to leverage effects to understand capacities better.","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Towards-a-comprehensive-model-of-the-cognitive-process-and-mechanisms-of-individual-sensemaking":{"title":"R- Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking","content":"\n\n\n- Metadata\n    - Title: Towards a comprehensive model of the cognitive process and mechanisms of individual sensemaking\n    - Authored by:: [Zhang Pengyi] , Soergel Dagobert\n    - Year: 2014\n    - Publication: Journal of the Association for Information Science and Technology ([JASIST])\n- Reading notes\n    - Zhang comprehensive process model of [sensemaking] (p. 1751)\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FA3o_LGHFjE?alt=media\u0026token=82e44a78-7731-4424-94ec-b7fe8c811d48)![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FyY8nJD47RC?alt=media\u0026token=cd95f1f2-16d8-410c-b6ed-8a3b140b44ed)\n    - Most previous work on [sensemaking] has focused a lot on the [foraging] aspect (p. 1736)\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FKpbr9_xqnt?alt=media\u0026token=24f051a5-defd-4d75-b83f-3d21d27d1e3f)\n    - The task matters for the requirements of sensemaking  (p. 1750)\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FyY8nJD47RC?alt=media\u0026token=cd95f1f2-16d8-410c-b6ed-8a3b140b44ed)","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Types-of-synthesis-and-their-criteria":{"title":"R- Types of synthesis and their criteria","content":"\n\n- #[References]\n    - Title: Types of synthesis and their criteria\n    - Meta:\n        - Tags:: #[References] [D/Synthesis Infrastructure]\n        - Authored by:: [Kenneth Strike] [George Posner] \n        - Year: [1983]\n        - Publication: Knowledge Structure and Use: Implications for Synthesis and Interpretation\n        - URL: [Strike \u0026 Posner (1983). Types of synthesis and their criteria. undefined](undefined)\n    - Content\n        - Abstract\n            - undefined\n        - PDF\n            - https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FTOB_x7SD9u.pdf?alt=media\u0026token=a12f0a86-9e16-406f-b6e5-eb294ddd1770\n        - context-snippets\n            -  ![](https://firebasestorage.googleapis.com/v0/b/roampdf.appspot.com/o/public%2Fimages%2F1612374725490.png?alt=media\u0026token=ca0d7c4c-f9ca-4d3c-a73b-139a74a3e05a)\n                - (https://roampdf.web.app/?url=https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%252Fapp%252Fmegacoglab%252FTOB_x7SD9u.pdf?alt=media\u0026token=a12f0a86-9e16-406f-b6e5-eb294ddd1770)\n- lit-context\n    - Authors are Known for:: influential Theory of [conceptual change]\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FnWtNBcd4SY?alt=media\u0026token=ba704bc5-ba7c-42f0-9fea-1cb55d60cc30)\n- üìù lit-notes\n    - [[C- There are two core concepts for synthesis- 1) creating a new whole from parts, and 2) displaying at least some level of conceptual innovation]] (higher = clearer case of synthesis)\n        -  ![](https://firebasestorage.googleapis.com/v0/b/roampdf.appspot.com/o/public%2Fimages%2F1612374725490.png?alt=media\u0026token=ca0d7c4c-f9ca-4d3c-a73b-139a74a3e05a)\n    - 15 types of synthesis\n        - Inductive (empiricist)\n            - 1: Generalizing over instances (p. 349)\n            - 2: Simple theory construction (p. 349)\n            - 3: Creating a superordinate theory (p. 349-350)\n            - 4: Creating a worldview (p. 350)\n        - Dialectics\n            - 5: Dialectical resolution (p. 350)\n                - Thesis --\u003e Antithesis --\u003e Synthesis (e.g., Marx, Dewey)\n        - Kuhnian synthesis\n            - 6: Normal science (p. 351)\n                - Extending a paradigm to new cases (like Piaget's assimilation)\n            - 7: Revolutionary science (p. 351)\n                - Changing dominant assumptions in a field (like Piaget's accommodation)\n            - 8: Overcoming incommensurable points of view (p. 352)\n            - 9: Emergence of a paradigm\n        - Interdisciplinary\n            - 10: Semantic synthesis (p. 352)\n                - Creating a common language between disciplines that talk about the same concepts in different ways\n            - 11: Generating interdisciplines (p. 352-353)\n                - Develop new conceptions and approaches to a problem, modified from two or more disciplines that converge on the same problem (e.g., computational linguistics) Convergence\n            - 12: Generating multidisciplinary perspectives (p. 353)\n                - Generating a point of view that can be used to consider/weigh data from diverse sources\n        - Quasi-syntheses\n            - 13: Assessment (p.353-354)\n                - Weighing the bulk of the evidence (aka meta-analysis / [systematic review])\n                - Judgment\n            - 14: Application and program development (p. 354)\n            - 15: Assemblages (p. 354)\n    - [[C- Some types of synthesis are simpler to produce than others]](p. 355-356)\n        - Easier: 1: Generalizing over instances (p. 349), Quasi-syntheses\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FON6DPVtEP7?alt=media\u0026token=40c651d5-99c6-4d83-9b19-3b144e8f7bcf)\n        - Harder: 3: Creating a superordinate theory (p. 349-350), 4: Creating a worldview (p. 350), 5: Dialectical resolution (p. 350), 7: Revolutionary science (p. 351), 8: Overcoming incommensurable points of view (p. 352), Interdisciplinary\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FagqYSANtmB?alt=media\u0026token=b7c66a7a-ee69-4927-bec0-894a77d1f350)\n        - #Claim Simpler syntheses are not necessarily \n    - Three intellectual standards for (good) synthesis (p356 onwards)\n        - Fleshes out [[C- There are two core concepts for synthesis: 1) creating a new whole from parts, and 2) displaying at least some level of conceptual innovation]] (higher = clearer case of synthesis)\n        - \u003e A quality [synthesis] will clarify and resolve, rather than obscure inconsistencies or tensions between material synthesized\n            ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FwVfN0OkrPc?alt=media\u0026token=4c987e60-8f2a-422b-89f7-74d652f4aefd) (p357)\n        - \u003e A quality [synthesis] will result in a progressive problem shift\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FmkZPSRtGTx?alt=media\u0026token=8c9c4e90-6a91-4922-b18e-059a08d6a0d6) \" (p357)\n        - \u003e A successful [synthesis] will satisfy the formal criteria for good theories\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FU-AqM6BweT?alt=media\u0026token=617cad8f-8247-46e7-8037-24547e6a585c) (p357)\n    - [[C- A useful synthesis helps to judge how healthy a research program is, and suggests future directions for it, often by interrogating/assessing crucial assumptions]] (p. 358)\n        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FyMROHnwTRn?alt=media\u0026token=b9c4f60c-cc7c-40f0-8e4f-e3c7c2f5b528)\n    - [CASCI] reading notes\n        - Implicit ordering of \"levels\" of synthesis on p1\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FwoCuGbE3Uk?alt=media\u0026token=fed22608-c668-4746-b74e-e56446bc5d78)\n        - Possible tension on p355 between idea of going beyond the sum of parts:\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FO-o1z8_sVS?alt=media\u0026token=39a00cda-e8a9-45c1-93ba-ee683a1887b4)\n        - Good quote on how hard synthesis is (p355)\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FRk0egY9lYe?alt=media\u0026token=633509f9-9d46-4b3e-b5fa-6192105e57d0)\n        - Interdisciplinarity discussion might be a bit thin (352-353)\n        - Question about assemblage (354)\n        - What about 2nd order effects of the synthesis process?\n            - Maybe the output itself isn't a \"synthesis\", but the people participating in the project of synthesis are changed and go on to create new things that they wouldn't have\n- #References\n    - Title: Types of synthesis and their criteria\n    - Meta\n        - Year: [1983]\n        - Authored by:: [Kenneth Strike] and [George Posner]\n        - Publication: Knowledge Structure and Use: Implications for Synthesis and Interpretation\n        - Strike, K., \u0026 Posner, G. (1983). __Types of synthesis and their criteria__.\n        - Status: #In-Zotero-Bib\n        - PDF: https://drive.google.com/file/d/1wYqUZjKg-E6hN8C33Tr2_FHy_lVZQ6J9/view?usp=sharing\n    - Content\n\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Using-systematic-reviews-to-inform-NIHR-HTA-trial-planning-and-design":{"title":"R- Using systematic reviews to inform NIHR HTA trial planning and design","content":"\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Wealth-of-Networks":{"title":"R- Wealth of Networks","content":"\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-What-is-a-Distributed-Knowledge-Graph":{"title":"R- What is a Distributed Knowledge Graph","content":"[[2021-04-14]]\n\n\u003e The overarching goal of the Underlay project is to _create a distributed public knowledge graph_\n\n\u003e ‚Ä¶ underlying all such developments is the core idea of using graphs to represent data‚Ä¶ The result is most often used in application scenarios that involve integrating, managing and extracting value from diverse sources of data at large scale ‚Ä¶ we view a knowledge graph as a graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities of interest and whose edges represent relations between these entities\n\nGraphs don't always have a ton of structure to them, and in fact, allow you to postpone a schema's definition. [[Q- How do people come to agree on queryable schemas]]\n\u003e Graphs allow maintainers to postpone the definition of a schema, allowing the data ‚Äì and its scope ‚Äì to evolve in a more flexible manner than typically possible in a relational setting\n\n[[Q- How do people come to agree on queryable schemas]]\n\u003e ‚ÄúSchemas‚Äù, if they exist, may be ad-hoc and unenforced, which often results in data that feels ‚Äúhalf structured‚Äù. This is not a sign of poor quality or problematic incompleteness, but rather a core distinguishing characteristic of knowledge graphs.\n\u003e\u003e Knowledge graphs are often assembled from numerous sources, and as a result, can be highly diverse in terms of structure and granularity. To address this diversity, representations of _schema_, _identity_, and _context_ often play a key role, where a _schema_ defines a high-level structure for the knowledge graph, _identity_ denotes which nodes in the graph (or in external sources) refer to the same real-world entity, while _context_ may indicate a specific setting in which some unit of knowledge is held true\n\n[[Q- What would be a protocol that would allow various protocols to communicate with each other]]\n\u003e Simply describing the technical structure of knowledge graphs misses a key aspect, which is _where the data comes from_ and _how the database is maintained_. ==Knowledge graphs are graph databases _that come from_ the integration of data from many sources and in many domains.== In other words, you make a knowledge graph by pulling data about lots of different kinds of things together in one place. This is part of what might justify calling them _knowledge_ graphs, since it involves ==parsing out latent common structure behind ‚Äúmere data‚Äù as it exists already, and aligning that common structure in a single space.== A knowledge graph is a graph that has somehow unified or reconciled the entities and relationships shared between (previously) separate data sources.\n\n[[Q- What would be a protocol that would allow various protocols to communicate with each other]] [[Q- How do people come to agree on queryable schemas]]\n\u003e Obviously not all entities and relationships are shared, but intuitively we feel that many are: an employee in a company directory can be treated as the same class of thing as a city resident in a census. ==Within each original dataset, the two will have different set of properties, but an external human curator can reasonably infer that they refer to the same type of real-world entity.==\n\n[[Q- What user behavior is required to maintain a decentralized knowledge graph]]\n\u003e Lastly, there‚Äôs a continuous, living, process-oriented aspect to knowledge graphs:\n\u003e\u003e ==‚Ä¶ effective methods for _extraction_, _enrichment_, _quality assessment_, and _refinement_ are required for a knowledge graph to grow and improve over time==\n\n\u003e This isn't about schemas in the technical database sense. ==Two databases might have the exact same schema, but still represent slightly (or wildly) different things.== The real thing that integration accomplishes is more like [ontology alignment](https://en.wikipedia.org/wiki/Ontology_alignment), an operation on abstract structures that just happens to be expressed as a mapping from one database into another. Typically we don‚Äôt work with or even think about explicit ontologies - we just rely on natural-language property names and general context instead - but it‚Äôs useful to imagine they exist because it helps explain why integration is so notoriously painful. The complexity and similarity of the actual schemas involved isn't even that relevant; what matters is how well the underlying concepts align.\n\u003e \n\u003e To illustrate this, suppose that we have a general-purpose knowledge graph full of entities like people, places, and things. ==Let‚Äôs say we also scrape GitHub to build a collaboration graph of GitHub accounts, and now we‚Äôd like to integrate this into our knowledge graph so that we can query for collaborator relationships between people. We're immediately faced with the distinction between people and user accounts, which are not quite the same thing.== People might have several accounts - maybe different ones for personal and work projects - and any account might be used by more than one person.\n\u003e \n\u003e There are two basic strategies that we can apply to align these ontologies.\n\u003e \n\u003e **One option is to just ignore the difference and merge the two classes based on email, full name, or whatever we have, and add the new ‚Äúcollaborator‚Äù relationships directly between people in the knowledge graph.** This keeps the graph as simple as possible - we‚Äôre just adding data and not reorganizing anything - but the sacrifice we make in doing so is that we distort the data we're integrating. **People and user accounts are simply different kinds of things, and treating them as the same will give us extra or missing links due to the mismatch between the underlying ontologies. This effect could be called _dilution_ or _compression_ or _erosion_.**\n\u003e \n\u003e **Our other option is to make separate entities for each user account, and then relate them to the associated people whenever we can.** Maybe we‚Äôd even introduce a new ‚ÄúAgent‚Äù class, with both Person and User Account as subclasses, plus a ‚Äúrepresents‚Äù relationship between Agents. Then maybe we add some inference rule about ‚Äúif Agent A represents Agent B, Agent C represents Agent D, and Agent A and Agent C are collaborators, then Agent B and Agent D are collaborators too‚Äù, so that we can query for collaboration relationships between people like we wanted. **Here, we‚Äôve been more faithful with our alignment, but only at the expense of accumulating complexity in the process.**\n\u003e \n\u003e In practice, we usually end up using a combination of the two when integrating new data, leaving us with a result that is (oddly enough) both slightly simplified and slightly more complex than the data that we started with. This trade-off is unavoidable; the real value proposition of knowledge graphs is in the balancing of these extremes. ==If we apply too many simplifying assumptions, we‚Äôll wind up with data that bears so little resemblance to its sources that it is effectively useless. On the other hand, if we accumulate too much complexity, we'll get a knowledge graph that is so convoluted that it becomes effectively unusable.== The best we can do is shoot for a middle ground that's reasonably expressive and reasonably manageable.\n\n\u003e And there is some theory that we can try to apply. [[Category theory]] in particular is rich with many of these themes, like qualified relationships and compositional mappings. And sure enough, lots of applied category theory work revolves around databases, schemas, integration, ontologies, and knowledge representation - sometimes [very abstract](https://arxiv.org/abs/1102.1889v1), sometimes [shockingly practical](https://arxiv.org/abs/1909.04881). Things are only theoretical until they‚Äôre not.\n\n\u003e Humans are so adept at context-switching that we give ourselves the illusion of having a single big ontology. Our goal is to build a large-scale data system that is so adept at context-switching that it gives the illusion of being a knowledge graph.\n\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Where-the-semantic-publishing-rubber-meets-the-scholarly-practice-road":{"title":"R- Where the semantic publishing rubber meets the scholarly practice road","content":"\n- Context: \n    - Started [June 12th, 2020]\n    - In response to this meeting idea: More interesting: consider how these elements are instantiated in actual scholarly practice: what challenges emerge? \n    - Based on last year, ~3k words (~ 6 pages)\n    - https://sig-cm.github.io/news/JCDL-2020-CFP/\n- External docs\n    - Google Doc: https://docs.google.com/document/d/1I59bDkF7bJdxXgPg6IQ4BveMbGcalj_mHP4UOqY7E0A/edit#\n    - Overleaf: https://www.overleaf.com/project/5ef6e0206667020001141441\n    - Talk \n        - Outline/script\n            - Motivation\n                - Synthesis is hard (this is the ultimate problem we care about)\n                - Big part of problem is [[Z: Most scholarly communication infrastructure operates on the document as the base unit]]\n                    - See [Questions we really want to ask for synthesis]\n                - Semantic digital libraries could really help!\n                - We have warehouses\n                - But they're kind of empty\n                    - [[Z- The central bottleneck to synthesis infrastructures is authoring]]\n                - Clearly it's not the case that \"if we build it, they will come\". How then might  we fill them?\n            - Problem\n                - What we've tried (that hasn't been enough):\n                    - [specialized [curator] model of semantic publishing] \n                        - accurate but has serious sustainability issues\n                    - [text mining model of semantic publishing]\n                        - cheap but inaccurate (horizon still quite far off)\n                            - though in conjunction with human labor might be good\n                - We're exploring the [scholar-powered model of semantic publishing]\n                - In this talk our goal is to show you a new/untapped opportunity for a kind of [scholar-powered model of semantic publishing] - specifically, we're going to show\n            - Findings\n                - \n            - Discussion points:\n                - What, if any, existing [standards] and conceptual models could be suitable for gluing these individual practices to a collaborative context in a way that enables [interoperability]?\n                    - cc [[Z- Distinction between neat vs scruffy in Semantic Web engineering]] - this is more towards the \"scruffy\" end of things\n                - Challenge of bridging the personal and the global\n                    - [[idea: local semantic publishing]]?\n        - Slides:\n            -  https://docs.google.com/presentation/d/e/2PACX-1vRr1PZxly5xj7LLiSBQpnPwcZV8VRejhFFuOcXCCX2-vbVwL0iv4QQ_wykWaj8l8aO--5NE6oGMJ_Qf/embed?start=false\u0026loop=false\u0026delayms=3000\n    - Submitted manuscript\n        - https://drive.google.com/file/d/15BpvVC9s0SLHX0ZPYTVKgjVTmBgUk2kp/preview\n    - citation:\n        - [[@chanWhereRubberMeets2020]]\n- Examples / data (for results)\n    - # [[Virtuosos]]\n        - P2 from [[John Thesis]] [[@morabitoManagingContextScholarly2021]]: multiple highlight colors - green = \"context\", blue = \"quote/idea\":\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FJqNuJfGznt.png?alt=media\u0026token=90298b35-9650-47f5-8f73-4ec067e99fa0)\n        - P3 from [[John Thesis]]: rich summary of papers with many details that are useful for the current context, but maybe not as useful for others\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F-zPx7Ej5u7.png?alt=media\u0026token=216e40ad-5b52-4044-b1b8-88555faa6c1a)\n        - WW from #[[@qianOpeningBlackBox2020]]: summaries of papers, with many links to related ideas as context, snapshots of key figures / details from text\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fse1CgbdDAy.png?alt=media\u0026token=bab3aebb-71f4-4894-8253-a0a85984a20a)\n        - NB from #[[@qianOpeningBlackBox2020]]: structured summaries for papers / books to be read in detail. Includes summaries of \"main points\", but also important details and evidence, and notes about what aligns or conflicts\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FQAx1lK7E8B.png?alt=media\u0026token=e01c4095-a358-4dc9-b808-ce1daddf31e7)\n    - # [[Explorers]]\n        - P5 from [[John Thesis]] extract segments and relate to them in \"typed\" ways (but not formally) on a canvas, such as \"perspectives __from...__\"\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FA7e4yFza6r.png?alt=media\u0026token=715da9a7-eb3e-4cee-86c2-7c5a2e266328)\n            - Note: each of these excerpts have rich mechanisms for [[context]], e.g., connecting to other pieces (because they are \"disembedded\"), \"transclude\" in new contexts, in addition to auto signals to name of pdf, page number, and quick jump back to original location of excerpt. Same with the [[QDAS]] route.\n                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FljDMKgyu30.png?alt=media\u0026token=242cb4a4-6371-4660-bbaf-159c0edb3dde)\n        - P1 from [[John Thesis]]: \"code\" excerpts from papers, place in code tree\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FZc0m-4oFu1.png?alt=media\u0026token=5bdb5de0-112d-422e-902b-a2a4fd3fd115)\n        - see list of Relevant communities around [[hypertext notebooks]]\n        - #[[@omiHowTakeSmart2020]]\n    - # [[Hackers]]\n        - [List] Examples of lead users building their own thing]\n        - see list of Relevant communities around [hypertext notebooks]\n        - [[sys/Zettelkasten]] with [[sys/org-mode]] https://blog.jethro.dev/posts/zettelkasten_with_org/\n        - [Digital Gardeners Telegram group]\n        - Examples of [Digital Garden]s from [[P- Maggie Appleton]]\n            - https://twitter.com/Mappletons/status/1250532315459194880\n        - One paradigmatic example of the hacker persona is [Andy's working notes] - [[P- Andy Matuschak]]\n        - Another paradigmatic example is [Jethro Kuan], who developed [[sys/org-roam]] to be able to implement [[sys/Zettelkasten]] practices in [[sys/org-mode]].\n            - rationale described in #[[@kuanHowTakeSmart2020]]\n                - basically motivated by wanting to do [[sys/Zettelkasten]] (as described in #[[R- How to Take Smart Notes]]) on top of [[sys/org-mode]], but finding that the vanilla version wasn't enough\n        - Another good example of the hacker persona is [Ian Jones]'s [Digital Garden]. \n            - Ian isn't an actively practicing scientist: he's primarily a software developer by trade, but he works with [[egghead.io]] (with [[P- Maggie Appleton]]) on creating effective programming-related explainers and tutorials. So he is essentially a learning scientist in practice. \n            - He is also an [autodidact], committed to constantly learning, for example, [how to teach a beginner topic](https://www.ianjones.us/2020-02-10-how-to-teach-a-beginner-topic). He is very concerned with creating a \"[second brain]\", and tries to write to learn to [create grist and reference material for courses or blog posts](https://www.ianjones.us/notes).\n            - Initially he [used](https://www.ianjones.us/2019-12-19-how-we-use-notions) [[sys/Notion]] (in part through influence of being in [Building a Second Brain (BASB)] cohort), then [moved to](https://www.ianjones.us/2019-12-19-roam-research) [[sys/RoamResearch]], but recently [decided to move](https://www.ianjones.us/2020-05-05-doom-emacs) to [[sys/org-roam]] so he can \"own\" his [second brain].\n            - A key feature that he tries to accomplish is [composability] through linking, a la [[sys/Zettelkasten]]. This method of composability partially relies on effective [compression] of ideas into relatively atomic notes about key ideas, that he can then densely link with others. This partially achieves [[context]ualizability] as well.\n            - A key innovation in [[sys/org-roam]] was the inclusion of automatic [[bi-directional links]]. Other hacker-types like [[P- Andy Matuschak]] have written scripts to do this in a more plain-text, [[std/Markdown]] setting like [[sys/BearNotes]]\n                - [[P- Andy Matuschak]] wrote a script [[sys/note-link-janitor]] to enable [[bi-directional links]] on top of [[sys/BearNotes]], which he chooses to use because it's offline, and he also wants to experiment with building [Tools for Thought]\n            - What I want to highlight, though, is a hack that he made to provide richer [context]ualizability] links back to original sources, using [[sys/org-mode]] extension that can connect specific excerpts from PDFs to notes - each note then becomes a \"pointer\" back to a specific segment in the PDF. \n                - In his words, described in [this post](https://www.ianjones.us/org-roam-bibtex#orgb716c96) #[[@jonesOrgRoamBibtex2020]]\n                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FjsujVrXcM2.png?alt=media\u0026token=006eb7d9-e5a6-46f6-921d-ebdc3ee64980)\n        - Another example of a hacker persona (who may not personally have deep development knowledge, but can team up well with those who do) is [[P- Anne-Laure Le Cunff]]'s [Digital Garden]\n            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FRSQ8y5CSi6.png?alt=media\u0026token=0ef49845-45b1-461f-aeac-7b2d294d82a5)\n            - Built on top of [[sys/TiddlyWiki]], and inspired by [[P- Andy Matuschak]]'s approach, she also employs [[compression]] to enable richer [[[[context]]ualizability]] through [[bi-directional links]]. This particular instance, though, has less connection to primary/original sources, which is fairly typical of many of the [[Digital Garden]]s at play right now.\n            - [[P- Anne-Laure Le Cunff]] writes on Slack that most of what's needed is already in place, free and open-source: \"[[sys/TiddlyWiki]]+ [[sys/TiddlyBlink]] (for [[bi-directional links]]) + Transclude Popup Plugin for the previews (https://giffmex.org/gifts/transclusioninpopups.html) + export everything as a static website (tutorial here:¬†https://nesslabs.com/tiddlywiki-static-website-generator) and host it on GitHub pages for free.\"\n        - [[P- Stian H√•klev]] is another example: back in his PhD, he created a personal wiki to enable tighter integration between his ideas and his sources (strongly motivated by desire for [[[[context]]ualizability]])\n            - called it [[sys/researchr]], and was heavily based on a personal [[sys/Wiki]] approach, with some integrations with bibtex and PDFs. described in this video #[[@haklevUsingResearchrTagextract2012]]\n            - later migrated to [[sys/RoamResearch]] and is now core member of community. \n                - video on youtube describing the transition ([[@haklevReproducingPhDReading2019]]) was key to me getting interested enough in [[sys/RoamResearch]] to give it a real go\n        - [[sys/vim-wiki]]\n        - [[sys/GlamorousToolkit]] - not purpose-built for [[synthesis]], but very easy to do it in here, with huge advantages for [[Multiplicity]] due to its superior programmability\n        - Values\n            - openness, \"working with the garage door up\" / [[learning in public]]\n            - privacy, [[sustainability]] (not just the \"next hot tool\"), being \"future proof\" (is a big reason to work in plain text, in [[sys/emacs]] or [[std/Markdown]])\n        - Other notes\n            - Many are not scholars, but some are! Especially as the line between explorers and hackers blurs with the advent of more \"plugin-friendly\" systems like [[sys/Obsidian]] and [[sys/RoamResearch]] (in the latter, you can now write simple javascript to augment the experience; have personal experience of a law professor writing some simple javascript to visually, with some minimal semantics, distinguish source excerpts from his own thoughts)\n                - see Trick: use embedded JS to modify the DOM based on tags to make visual distinctions between my thoughts and others' thoughts.  from [[Roaman lit-reviewing meetup June 5th, 2020]]\n                - Can see these as instances of [[end-user programming]], probably not too far from the [[zone of proximal development]] for many of these researchers, especially as the lines between \"technical/programming\" work (e.g., move from SPSS to [[sys/R]]) become blurred\n                    - the [[ProgrammingHistorian]] is another example of this, as is [[Kieran Heasly]]\n                - We now also have [[sys/FoamBubble]]!!\n            - As norms move towards more [[open science]], [[open data]], digital-based workflows, and so on, there is significant opportunity to integrate with these favorable trends to explore how [[semantic publishing]] standards might be better integrated into [[scholarly workflows]]\n            - Hackers are important because they can create a more open ecosystem of tools (cf. [[sys/Athens]], [[sys/FoamBubble]], [[sys/Obsidian]]) that are easier to try. This broadens the base beyond those who are tied to a particular platform or price point.\n    - New tools we can integrate with\n        - https://twitter.com/MuseAppHQ/status/1273698452539609088\n- Comments/notes\n    - Important to be clear about what precisely we mean by [[semantic publishing]]: uptake of __what__ by __whom__, __where__\n        - Referring to: Yet, on the whole, we're not seeing nearly as much of this transformation as we'd like. Uptake of [[semantic publishing]] is low and restricted to a small set of power users\n        - What: not just linked entities, but the whole shebang: [[Atomicity]], [[context]], [[composability]], [[Multiplicity]]. IN other words, [[semantic publishing]] for [[synthesis]]! \n            - Not unique to us! see, e.g., #[[R- Genuine semantic publishing]], #[[@grozaSALTWeavingClaim2007]]\n            - Contra the slightly broader definition by #[[@shottonSemanticPublishingComing2009]]\n                - \"In the present context, I define '[[semantic publishing]]' as ^^anything that enhances the meaning of a published journal article, facilitates its automated discovery, enables its linking to semantically related articles, provides access to data within the article in actionable form, or facilitates integration of data between papers^^. Among other things, it involves enriching the article with appropriate metadata that are amenable to automated processing and analysis, allowing enhanced verifiability of published information and providing the capacity for automated discovery and summarization. These semantic enhancements increase the intrinsic value of journal articles, by increasing the ease by which information, understanding and knowledge can be extracted. They also enable the development of secondary services that can integrate information between such enhanced articles, providing additional business opportunities for the publishers involved. Equally importantly, readers benefit from more rapid, more convenient and more complete access to reliable information\" \n        - [[Katrina Fenlon]]'s review can help us provide specific, concrete examples, including things like [[std/Nanopublications]]: https://docs.google.com/document/d/1uI30nLzcca7gwMZ-MGXbnedk_NXFUvqVrcIpRssRz-I/edit\n    - Old/bad outline\n        - #[[Joel Chan]] 16:05 This doesn't feel right.\n        - Synthesis is a thing.\n        - It's hard.\n        - We're trying to build tools to help.\n        - Historically the focus of your work on data modeling has been on the practice of a community, or in the practice of specialized curators. Here we're focusing on what that might look like in the practice of an individual scholar.\n        - As we studied and built stuff, we started to notice a pattern of needs emerging, in terms of \"what's missing\": these 3/4 things (Compression, Context, Composability, etc.)\n        - This pattern of needs is quite similar to the requirements/motivations of a BUNCH of data models that have been developed already by your community, for alternative infrastructures for synthesis.\n    - Relevant stuff\n        - Contribution frames\n            - #[[Authoring Bottleneck]]\n            - #[[Z- The central bottleneck to synthesis infrastructures is authoring]]\n                - #[[Z: Models of authoring for semantic publishing in scholarly communication infrastructures]]\n            - #[[Q- Do scholarly synthesis infrastructures already exist]]\n            - Discussing \"where we are\" in the [[infrastructure]] (note that standards are probably ok, tooling is not) is helpful, and thinking through the challenges in the other parts of the infrastructure will yield generative conversations\n            - Renewed charge to build a semantic knowledge layer, dating back to [[Semantic Web]], [[std/Hypertext]], etc., but refined with recent advances in [[infrastructure]] studies and [[reuse]], and experiences with trying to build these layers, including the thread on [[standard/Claims]] within HCI\n            - important to think through more carefully why [[sys/Xanadu]] and the many [[Semantic Web]] and [[std/Hypertext]] or [[sys/Memex]] related projects have ‚Äúfailed‚Äù.\n            - More interesting: subtleties, nuances of the elements of the framework\n        - What do we already know (lit-wise) about the rubber meeting the road?\n            - Adding to thread on [[Authoring Bottleneck]]: [[Z: What is the user experience of dedicated semantic authoring]] and [[Q: What is the user experience of semantic authoring within regular scholarly workflows]]\n            - #[[@kuhnBroadeningScopeNanopublications2013]] has a user study that checks how easy it is to train 16 biomed researchers to convert a short text into a natural language statement (no [[formality]] though!)\n        - POtential new data sources for our observations / anecdotes to add the \"road\" to the rubber\n            - Found old thread of video recordings on my Youtube channel taht really nicely track the evolution (and constancy) of our ideas around #[[D/Synthesis Infrastructure]]!\n            - [[C- Effective individual synthesis systems (seem to mostly) exist (for a select few)]]\n        - Analogous threads\n            - Reading again the [[CEDAR project]]: motivation and lessons are similar for [[open data]]: everyone knows science is better for it if data are shared with appropriate metadata, following [[FAIR principles]]. But uptake is low, except when there are extreme incentives. Some stuff I saw at [[iConference]] last year along this vein. See also [[@tenopirDataSharingScientists2011]]\n        - Other stuff\n            - #[[@wolfCurseXanadu1995]]\n            - #[[Z: Most scholarly communication infrastructure operates on the document as the base unit]]\n    - ‚û∞ breadcrumbs\n        For \"data collection\" (FOCUS ON THIS FIRST)\n            - {{[[DONE]]}} Begin compiling a \"results\" section. \n                - For this: So far, we're seeing that a surprising amount of that labor is already happening! Completely of their own volition, away from any data models people, real scholars are trying to shape their workflows / setups to satisfy requirements of compression / context¬†/ composability / multiplicity.\n                - Maybe start with something high level like \n                    - \"How are scholars doing \"semantic publishing\"-like labor in their synthesis practice?\" #[[Z]]\n        For motivation / framing\n            - {{[[DONE]]}} Read and zettel a few seed papers on \"uptake\" of specifically \"semantic publishing\" for scholarly work\n                - Seed is [[@kuhnNanopublicationsGrowingResource2018]] reports around 10 million [[std/Nanopublications]] published at the time of writing, albeit almost all within bioinformatics, and overwhelmingly dominated by a small (N=41!!) set of authors\n            - {{[[DONE]]}} Start a thread collecting state of the art on text mining for semantic publishing\n                - Seed is #[[@kilicogluBiomedicalTextMining2017]]\n- Outline:\n    - ### 1. The problem: the [[Authoring Bottleneck]] for \"genuine\" [[semantic publishing]] for [[[[scholarly communication]] [[infrastructure]]]]\n        - We believe (as do you!) that [[semantic publishing]] could transform [[scholarly communication]] ([[@berners-leePublishingSemanticWeb2001]]) \n            - We resonate a lot with the vision cast by #[[@renearStrategicReadingOntologies2009]]: \"Scientists will still read narrative prose, even as text mining and automated processing become common; however, these reading practices will become increasingly strategic, supported by enhanced literature and ontology-aware tools. As part of the publishing workflow, scientific terminology will be indexed routinely against rich ontologies. More importantly, formalized assertions, perhaps maintained in specialized 'structured abstracts' (27), wil provide indexing and browsing tools with computational access to causal and ontological relationships. Hypertext linking will be extensive, generated both automatically and by readers providing commentary on blogs and through shared annotation databases. At the same time, more tools for enhanced searching, scanning and analyzing will appear and exploit the increasingly rich layer of indexing, linking, and annotation information.\" (p. 832)\n            - [[Z: Most scholarly communication infrastructure operates on the document as the base unit]]\n            - #@grozaSALTWeavingClaim2007\n        - It's not that we have made no progress! Indeed, the [[semantic publishing]] revolution is indeed underway! We see encouraging developments, in [[bioinformatics]] and [[archeology]], for example. \n        - Yet, on the whole, we're not seeing nearly as much of this transformation as we'd like. Uptake of [[semantic publishing]] is low and restricted to a small set of power users\n        - Discussion of what it will take to fully realize this transformation is robust and ongoing in this community. One important part of the conversation is a sense that  [[Z- The central bottleneck to synthesis infrastructures is authoring]]\n            - For example, [[R- Genuine semantic publishing]] notes, \"It turns out that all the technologies needed for applying genuine semantic publishing are already available and most of them are very mature and reliable. There are no __technical__ obstacles preventing us from releasing our results from today on as genuine semantic publications, even though more work is needed on ontologies that cover all relevant aspects and areas and on nice and intuitive end-user interfaces to make this process as easy as possible.\" (p. 148)\n    - ### 2. The general solution we're excited about: explore scholar-powered contribution models of [[semantic publishing]]\n        - The [[specialized [[curator]] model of semantic publishing]] is currently the engine of what uptake exists. But it requires a lot of funding, for a really long time! It's a great fit for well-funded domains like biomedical sciences, but is a much harder sell for many other domains of knowledge with less obvious funding implications, like the humanities and social sciences. And it's hard to predict in advance precisely which fields are going to be \"worth funding\": knowledge doesn't work that way!\n        - There is an exciting thread of conversation that is exploring how to address the [[Authoring Bottleneck]] with a more \"scholar-powered\" model of contribution. For example, many propose an [[author contribution model of semantic publishing]], in tandem with a [[text mining model of semantic publishing]] (#[[@monsWhichGeneDid2005]], #[[@grozaSALTWeavingClaim2007]], #[[@berners-leePublishingSemanticWeb2001]]). \n        - We think this is a promising approach! There is potential alignment of incentives, with [[semantic publishing]] providing high potential payoffs in visibility and impact of work. But we're unsure\n        - Exemplars of alternative approaches\n            - Integrate with [[peer review]] #[[@bucurPeerReviewingRevisited2019]]\n        - In our work, ^^we are exploring the potential of addressing the authoring bottleneck by integrating into the work that scholars are already doing^^, as a replacement/supplement to text mining and specialized labor.\n    - ### 3. Our intended contribution to this conversation: investigate integration points in existing [[scholarly workflows]] for [[synthesis]]\n        - In this paper, we want to share some preliminary findings for a foundational question: where (if at all) are there integration points in **__existing synthesis practice__** with the labor required to power synthesis infrastructures? In other words, we're investigating: ^^what (if any) [[semantic publishing]] work is already being done by scholars in their [[scholarly workflows]], not just in the authoring/publishing process^^? [[Q- What semantic publishing work is already being done by scholars in their scholarly workflows?]]\n        - Revealing these \"integration points\" would help us see where we might be able to \"accrete\" semantic authoring tools to leverage the rich semantic work that is already happening. \n        - Investigating these integration points could also help us understand how to better design \"intuitive user interfaces\" for [[authoring tools]]: beyond \"usability\", we could improve [[sustainability]] by improving, rather than disrupting, existing [[scholarly workflows]]\n        - At the same time, we could help address incentive mismatches if these integration points allow us to also significantly augment [[individual synthesis]] so that the labor of [[semantic publishing]] yields more immediate benefits.\n        - So far, we're seeing that a surprising amount of that labor is already happening! Completely of their own volition, away from any data models people, real scholars are trying to shape their workflows / setups to satisfy requirements of compression / context¬†/ composability / multiplicity.\n    - ### 4. Interlude: what do we mean by [[semantic publishing]] labor?\n        - Here is where talk about the framework of [[compression]], [[context]], [[composability]], and (maybe) [[Multiplicity]]. It's our frame for looking.\n    - ### 5. A taste of our findings\n        - By dimension\n            - [[compression]] / [[Scholars are (already) doing compression work]]\n            - [[context]] / [[Scholars are (already) doing contextualization work]]\n            - [[composability]] / [[Scholars are (already) doing composability work]]\n        - By practice\n            - ^^Semantically rich annotations^^\n                - Summary\n                    - Use annotations to identify \"subdocuments\" / ideas / building blocks for reuse [[compression]]\n                    - Include contextual details as well ([[context]])\n                    - Employ sophisticated mapping with colors to identify types of building blocks ([[composability]])\n                    - Noted as [[in-source annotations]] in #[[@qianOpeningBlackBox2020]]\n                - Examples\n                    - P2 from [[John Thesis]] [[@morabitoManagingContextScholarly2021]]: multiple highlight colors - green = \"context\", blue = \"quote/idea\":\n                        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FJqNuJfGznt.png?alt=media\u0026token=90298b35-9650-47f5-8f73-4ec067e99fa0)\n            - ^^Power tools for power context^^\n                - Summary\n                    - Others push past the limits of their worklfows towards less mainstream tools, or even repurposing other tools like [[QDAS]]\n                    - In [[QDAS]]:\n                        - Achieve [[compression]] more explicitly, creating excerpts that are manipulable by themselves, but also make sure they're typed [[composability]] and have particular semantically meaningful relationships to other compressed segments ([[composability]]). A TON of [[context]] you get for free: no need to do the tedious manual work of writing down a source name and page number. Can see this as a next-level evolution of the practice of highlighting that we've seen.\n                    - In [[sys/LiquidText]]\n                        - Note: this is interesting because [[sys/LiquidText]] explicitly builds on the deep work in [[active reading]]\n                - Examples\n                    - P1 from [[John Thesis]]: \"code\" excerpts from papers, place in code tree\n                        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FZc0m-4oFu1.png?alt=media\u0026token=5bdb5de0-112d-422e-902b-a2a4fd3fd115)\n                    - P5 from [[John Thesis]] extract segments and relate to them in \"typed\" ways (but not formally) on a canvas, such as \"perspectives __from...__\"\n                        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FA7e4yFza6r.png?alt=media\u0026token=715da9a7-eb3e-4cee-86c2-7c5a2e266328)\n                        - Note: each of these excerpts have rich mechanisms for [[context]], e.g., connecting to other pieces (because they are \"disembedded\"), \"transclude\" in new contexts, in addition to auto signals to name of pdf, page number, and quick jump back to original location of excerpt. Same with the [[QDAS]] route.\n                            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FljDMKgyu30.png?alt=media\u0026token=242cb4a4-6371-4660-bbaf-159c0edb3dde)\n            - ^^Rich TL;DRs^^\n                - Summary\n                    - Includes summaries of main claims ([[compression]]) and how they relate to each other [[composability]], as well as key details ([[context]]) to be used\n                    - This recalls some practices that others also do, see, e.g., [[memorandums]], as described and popularized by [[Raul Pacheco-Vega]]\n                    - Noted as [[per-paper summaries]] in #[[@qianOpeningBlackBox2020]]\n                - Examples\n                    - NB from #[[@qianOpeningBlackBox2020]]: structured summaries for papers / books to be read in detail. Includes summaries of \"main points\", but also important details and evidence, and notes about what aligns or conflicts\n                        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FQAx1lK7E8B.png?alt=media\u0026token=e01c4095-a358-4dc9-b808-ce1daddf31e7)\n                    - WW from #[[@qianOpeningBlackBox2020]]: summaries of papers, with many links to related ideas as context, snapshots of key figures / details from text\n                        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fse1CgbdDAy.png?alt=media\u0026token=bab3aebb-71f4-4894-8253-a0a85984a20a)\n                    - P3 from [[John Thesis]]: rich summary of papers with many details that are useful for the current context, but maybe not as useful for others\n                        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F-zPx7Ej5u7.png?alt=media\u0026token=216e40ad-5b52-4044-b1b8-88555faa6c1a)\n            - ^^Richly structured shared artifacts^^\n                - \n        - By persona\n            - Can write synthetic vignettes, then show concrete examples\n            - The personas\n                - ^^The virtuosos^^ [[Virtuosos]]\n                    - Where many people are\n                    - Often includes substantial analog work\n                - ^^The early adopters and appropriators^^ [[Explorers]]\n                    -  [[sys/LiquidText]], [[sys/RoamResearch]], [[sys/TiddlyWiki]], [[sys/TinderBox]], etc.\n                    - [[QDAS]], excel, etc.\n                    - Let's talk about the [[hypertext notebooks]]\n                        - The conceptual and technical roots of this wave can be traced to the influential ideas of [[Vannevar Bush]], [[Ted Nelson]], and [[Doug Engelbart]] around [[std/Hypertext]]. This scene has also been heavily influenced by the idea of a [[sys/Zettelkasten]] approach for [[Knowledge Management]], originated in [[Niklas Luhmann]], and later popularized in the English-speaking world by [[zettelkasten.de]], [[S√∂nke Ahrens]] with [[@ahrensHowTakeSmart2017]], and, most recently, beginning approximately in [[2019]], by the emergence of [[sys/RoamResearch]]. There is also substantial influence from [[@andy_matuschak]], an independent researcher in the US Bay Area, who has put forward the concept of [[sys/Evergreen Notes]], and was one of the first to start sharing his. Andy himself is better understood as one of the [[Hackers]], as we will discuss in more detail.\n                        - The key practices and affordances of [[hypertext notebooks]] focus on the creation and maintenance of relatively **atomic** notes (either a concept or some kind of focused \"claim\") that are densely linked together. The links are typically accomplished through [[bi-directional links]], where, every time a link is made __from__ one source note to a target note, both the source and target notes record the link. In this way, links between notes are more accessible, since links can be followed from either source or target notes. \n                        - [[bi-directional links]] are a key innovation on top of the original [[sys/Wiki]] approach, because they enables more focused usage and labor around tending to connections between notes ([[composability]]). The links also enable [[compression]] by supporting refactoring of ideas into smaller pieces, knowing that you will still be able to follow threads of logic: users can compress quite complex ideas into a single statement (e.g., \"knowledge is contextual\") while retaining links to the less compressed ideas that \"unpack\" different aspects and subtleties of the more complex idea. In this way, tending to the notes and links also enhances the [[[[context]]ualizability]] of each note.\n                            - [[Andy's working notes]] has concept of \"layers\" in a taxonomy of [[sys/Evergreen Notes]]\n                            - See also:\n                                - #[[@saschaThreeLayersEvidence2019]]\n                                - #[[@saschaTaleComplexityStructural2018]]\n                            - [[@andy_matuschak]] wrote a script [[sys/note-link-janitor]] to enable [[bi-directional links]] on top of [[sys/BearNotes]], which he chooses to use because it's offline, and he also wants to experiment with building [[Tools for Thought]]\n                        - The system emphasizes separation between \"your thoughts\" (the main content of these networked notebooks) and \"others' thoughts\" (which should live in \"literature notes\" that are linked to, but separated from, the networked notebook) The atomicity of the notes is a way to achieve [[compression]], both obviously (by breaking things down), but also in more subtle ways, by compressing quite complex ideas into a single statement (e.g., \"knowledge is contextual\") while retaining links to the less compressed ideas that \"unpack\" the complex idea. \n                        - The atomicity and [[bi-directional links]] also emphasize the deliberate practice of [[composability]]: developing ideas into more complex and better forms over a long period of time. The phrase \"finding connections\" is a staple in this community.\n                        - Since notes that collect [[bi-directional links]] can be as \"small\" as a single concept, the act of deliberately linking notes partially accomplishes the work of deliberately developing [[folksonomies]], which approximate the more formal / [[ontologies]] work of [[semantic publishing]] for [[composability]]. A key affordance in these [[hypertext notebooks]] is that it is quite easy to rename note titles --- changes to a note's title typically automateically propagate throughout the database of notes --- which enables more agile and evolving [[ontologies]]. \n                            - People in the [[Hackers]] community are actively working on ways to manage aliases and merging. \n                                - People also use [[bi-directional links]] to do [[Contextual Bootstrapping]] of ideas, creating \"empty pages\" or \"hooks\" for concepts. \n                - ^^The hackers^^ [[Hackers]]\n                - Others??\n                    - ^^The wanderer?^^\n                        - Where many people are\n                        - Moves between many things, in constant flux\n        - Some high-level takeaways:\n            - Many of these in isolation are not new! Lots of beautifully detailed work in [[active reading]]; see as far back as #[[@oharaStudentReadersUse1998]] for rich examples, or even... as far back as [[Charles Darwin]] and, arguably, as old as external representations ;). What we see here is a different lens with which to view these data, to consider what these behaviors (could) do.\n                - The sophistication of the *system* (vs. the medium or tool) might explain [[Z: The stubborn effectiveness of analog media]]\n            - Why are they doing this?\n                - They *want* to do this work (better), but often don't because it ends up being too costly\n                - But the social context is key for at least some of it: a lot of explicit work being done in collaborative settings\n                    - Advisor meetings\n                    - Large-scale collaborations\n                    - Teams\n            - Strong opportunity for progress by partnering modeling and standards work with growing user interface innovations (sort of the reverse of making [[semantic publishing]] user interfaces better). \n                - Examples\n                    - https://twitter.com/MuseAppHQ/status/1273698452539609088\n                    - See also [[sys/Hypothes.is]]\n                - Different kinds of questions:\n                    - *which* types of annnotations at which points might be valuable __for the user__ to formalize / contextualize in some way?\n                    - how might formalisms of [[semantic publishing]] deliver immediate value to the scholar at different parts of their scholarly workflow?\n                        - identify some of the frictions and pain points in their process. good clue to this is what motivates the move to [[QDAS]] and [[sys/LiquidText]] - quotes from guided tour in [[John Thesis]]?\n                    - how much of this is actually happening? is there a way to study this in a way that is analogous to the [[cognitive surplus]] idea from crowdsourcing?\n                        - see, e.g., What is the scale at which scientists are producing annotations and notes? In other words, what is the untapped opportunity here? What is the \"drag coefficient\"? How much energy is being \"wasted'?\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-Why-Hypothesis-Testers-Should-Spend-Less-Time-Testing-Hypotheses":{"title":"R- Why Hypothesis Testers Should Spend Less Time Testing Hypotheses","content":"\n-   #[[references]]\n    -   Title: Why Hypothesis Testers Should Spend Less Time Testing Hypotheses\n    -   Meta:\n        -   Authored by:: [[Anne M. Scheel]] [[Leonid Tiokhin]] [[Peder M. Isager]] [[Dani√´l Lakens]]\n        -   Year: [[2020]]\n        -   Publication: Perspectives on Psychological Science: A Journal of the Association for Psychological Science\n        -   URL: [Scheel et al. (2020). Why Hypothesis Testers Should Spend Less Time Testing Hypotheses. Perspectives on Psychological Science: A Journal of the Association for Psychological Science](https://journals.sagepub.com/doi/full/10.1177/1745691620966795)\n    -   Content\n        -   Abstract\n            -   For almost half a century, Paul Meehl educated psychologists about how the mindless use of null-hypothesis significance tests made research on theories in the social sciences basically uninterpretable. In response to the replication crisis, reforms in psychology have focused on formalizing procedures for testing hypotheses. These reforms were necessary and influential. However, as an unexpected consequence, psychological scientists have begun to realize that they may not be ready to test hypotheses. Forcing researchers to prematurely test hypotheses before they have established a sound \"derivation chain\" between test and theory is counterproductive. Instead, various nonconfirmatory research activities should be used to obtain the inputs necessary to make hypothesis tests informative. Before testing hypotheses, researchers should spend more time forming concepts, developing valid measures, establishing the causal relationships between concepts and the functional form of those relationships, and identifying boundary conditions and auxiliary assumptions. Providing these inputs should be recognized and incentivized as a crucial goal in itself. In this article, we discuss how shifting the focus to nonconfirmatory research can tie together many loose ends of psychology's reform movement and help us to develop strong, testable theories, as Paul Meehl urged.\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-clojureD-2021-Command-and-Conquer-Learnings-from-Decades-of-Code-Editing-by-Philippa-Markovics":{"title":"R- clojureD 2021- Command and Conquer- Learnings from Decades of Code Editing by Philippa Markovics","content":"\nNotes related to this talk live in the [[Nextjournal]] page.\n\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/A0TafHXszgM?start=860\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/R-newellYouCanPlay1973":{"title":"R- newellYouCanPlay1973","content":"\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Readwise":{"title":"Readwise","content":"Authored by:: [[P- Brendan Langen]]\n\nReadwise is a product that allows users to extract book and journal highlights into one place. They integrate with ebook platforms, read-it-later apps, and photos to collect annotations in one place. \n\nThis enables readers to engage in progressive summarization, as [[Z- People process complex information in multiple levels and stages of processing|people process complex information in multiple levels and stages of processing]]\n\nAs [[C- Synthesis is supported by Active Reading]],  [[Extended Universe/C- Incrementally processing notes is a key user behavior to promote synthesis|Incrementally processing notes is a key user behavior to promote synthesis]]. \n\nAnnounced in September 2021, they are releasing their own reading app to enable read-it-later functionality within the tool. \n\nIn our research, we are asking: [[Q- How might a discourse graph be designed to support incremental formalization]] and [[Q- How can people maintain a decentralized discourse graph with a high quantity of stuff in it]]? Because [[C- Curation is an important role in maintaining a decentralized discourse graph]], Readwise's design may provide insights. \n\n![[Pasted image 20210915132018.png]]","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Research-Grant-Application":{"title":"Research Grant Application","content":"[[Project Description]]\n\n[[Project success metrics]]\n\n[[Project Mission and Impact]]","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Roam-Research":{"title":"Roam Research","content":"Authored by:: [[P- Brendan Langen]]\n\nRoam Research is a prominently used networked tool for thought. Roam's early community connected many from the PKM, academic, and hacker spaces, who have built out an ecosystem of workflows and extensions alongside the platform. [[R- Roam can loosely be considered a DSL with a structural editor]] atop its Clojure base. \n\nRoam's aim is to augment collective intelligence over a long (50 years) time horizon, which has led to delays in desired user-facing updates. This has caused some early users to seek alternatives, and a spring of open source and alternative hypertext tools have arisen as competitors. \n\nco-founded by [[P- Conor White-Sullivan]] \nlink:: https://roamresearch.com/\n\n![[Pasted image 20210916111704.png]]\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Saga":{"title":"Saga","content":"Authored by:: [[P- Brendan Langen]]\n\nSaga is a document editor that aims to help you organize your knowledge quickly. Saga enables automatic linking and always-on background search to help users connect their knowledge - alone or in teams. \n\nCo-created by [[P- Filip Stanev]] - https://saga.so/?r=0\n\n![[Pasted image 20210915125220.png]]","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Search-Behavior":{"title":"Search Behavior","content":"Authored By:: [[P- Rob Haisfield]]\n\nWhen people are under conditions of uncertainty, how do they increase their certainty?\n\nSearch behavior tends to fall into a few main categories that exist on a spectrum: information retrieval, focused search, exploratory search, exploratory browsing.  \n\nWith information retrieval, you know what you're looking for and where to find it. A search bar or folder system works great in this situation.\n\nWith focused search, you know what you're looking for but not where to find it.\n\nWith exploratory search, you know where to find something but you can't quite articulate what it is you're looking for. This is what I design my digital garden for.\n\nWith exploratory browsing, people aren't searching for anything in particular, they're just consuming information.\n\nThere are two main theories of search behavior. [Information foraging theory and Berrypicking](https://doi.org/10.1177%2F0165551517713168).  \n\nIt's important to think about the lens parameters: how likely is the person to know what they are looking for or where to find it? How transparent are things?\n\n## Berrypicking\nBerrypicking is modeled off of the author's experience picking berries on vacation. People meander from information patch to information patch, and they have many different possible search paths.\n\n**Critically, as people learn a new domain, their questions evolve.** One of the most important things for a searcher is that they learn the right questions to ask as they uncover more information.\n\n- They can store resources for later.\n- They can look into the future of an idea by seeing which papers cite it.\n- They can read the literature review and citations to see how the idea was built and where it fits into the broader landscape of the theory.\n- They can ask for help from others\n\nPeople have many different strategies for searching for information, and they flexibly switch between them as demanded by the situation.\n\n## Information Foraging Theory\nInformation foraging theory is more pragmatic than Berrypicking. Every search for information begins with a goal. People follow an **information scent,** which gets stronger as people approach their goal. It's almost like a hunt.  \n\nThis is based on optimal foraging theory, so people will explore a \"patch\" of information, and based on how much friction there is to move on to the next patch weighed against the probability that a patch of information contains what they're looking for, they'll move on or dive deeper.\n\nIn my hypertext notebook, I try to title my notes in such a way that there's a high **information scent** to help people determine whether to click on the link or not. I'm planning on implementing hover previews as well in order to reduce the friction to click on a page to open it up.\n\nSome extended sections of the research point to the fact that people will find information faster if they have help from others. Experts can help beginners learn the vocabulary of the domain. *For more detail, see [[R- Information Foraging Video]]*\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Semilattice":{"title":"Semilattice","content":"Authored by:: [[P- Brendan Langen]]\n\nOperating System concept design inspired by [[Roam Research]] and [[Muse]]. Semilattice can be seen as a series of interactions and systemic concepts on top of personal knowledge management (PKM) tools. The system is block-based, with a card structure that consists of multiple lists of blocks and drawings. Each block and card can have a networked relationship, and can have multiple parents and children.\n\nSemilattice is designed to improve information processing in existing PKM tools, specifically web-based research problems. \n\nSee: [[Search Behavior]]\n\nhttps://www.semilattice.xyz/\n\nDesigned by [[P- Aosheng Ran]], with a design spec [here](https://www.aoshengran.com/blog/semilattice-problem-complex) and video demonstration [here](https://youtu.be/7MeeCeTXOyo?t=103).\n\n\n\n![[Pasted image 20211007153705.png]]\n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Spaced-repetition-systems":{"title":"Spaced repetition systems","content":"","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Standoff-Annotation":{"title":"Standoff Annotation","content":"Authored By: [[P- Brendan Langen]]\n\n\n[[P- Iian Neill]]'s goal with [[Codex OS]] is to build a system for *relation* - the principle of relatedness or [[sensemaking]]. To achieve this, he uses a modified data structure called standoff annotation, which allows information to respect multiple interpretations. \n\nThis concept carries a ton of implications, but namely that an entity can be defined and connected in multiple\n\nMost databases require an entity to carry the same relationship across the entirety of the database. This causes inherent challenges in extending knowledge across domain boundaries, as entities often carry different definitions - and thus, relationships. \n\nCodex OS is designed to enable users to frame things in multiple ways (or layers) without breaking the structure of their graph. \n\nStandoff annotation allows users to annotate atop an entity, which is stored separately from the text, yet still retains the relationship to the initial text.\nhttps://github.com/argimenes/standoff-properties-editor\n\n\u003e A standoff property is a property that 'stands off' from the text it refers to, that is stored apart from the text source.\n\u003e A property in this context is a data structure that represents a range of characters in the text along with information relevant to the annotation, such as the annotation type. Annotations can be of any type, e.g. stylistic (italics; bold; underline), semantic (line; paragraph; page), or linked entities (database record; URL).\n\n\u003e XML does not cope well with [overlapping annotations](https://en.wikipedia.org/wiki/Overlapping_markup). This is because the tree structure of XML mandates that an overlapping annotation (two or more annotations that overlap the same text sequence) cross one or more branches of that tree structure.\n\u003e These problems disappear, however, if the text and its annotation properties are kept entirely separate. The text, then, is stored in a raw or unformatted state, annotated by a collection of discrete standoff properties.\n\nTechnical challenges from standoff properties can be mitigated by creating linked-list data structures.\n\n\u003e The technical challenge posed by standoff properties is that they require indexes to link annotations to the words in the text, which suggests that the text cannot be changed without breaking the annotations. However, by using a linked list-style structure composed of SPANs it is possible to create properties that reference characters by pointers, allowing text to be freely inserted or deleted without breaking the annotations. Indexes are only calculated at the end of the session, when the annotated text is to be exported (and presumably saved).\n\n[[P- Iian Neill]] suggests that since machine generated annotations could enable a wealth of annotations, a layering structure can act as a focusing frame. \n\n\u003e As there is no defined limit on the number or types of annotations that can be added to a text, there is the chance that texts may become visually cluttered with annotations. To address this, there is an option to assign a user-defined 'layer' to an annotation for the purpose of grouping them. Layers can be shown and hidden at will, thus reducing clutter. This is particularly helpful when it comes to computer-generated annotations, such as syntactic or semantic annotations created by an NLP library or other text analysis tools.\n\nAdding a graph database atop standoff properties affords compounding benefits.\n\n\u003e While standoff properties can be stored in any format storing them as LOD entities in a graph database vastly increases their potential. For example, if you were searching for all references to a person you would not only find the texts but the exact character positions in the text. If you expanded your query from a person like Leonardo da Vinci, say, to all artists you could see every instance an artist is mentioned in any text\n\u003e Queries could also be combined across annotation types. For example, if you had the syntax tree of a text you could find every occurence of a term within a given syntactical unit. The more annotation types you record, the greater the number of text mining options become available.","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Templates/YAML-Title-Only":{"title":"\u003c% tp.file.title %\u003e","content":"","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Templates/YAML-Title-with-author":{"title":"\u003c% tp.file.title %\u003e","content":"\nAuthored By:: \n\n","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/The-common-forms-of-structure-people-add-to-their-notes":{"title":"The common forms of structure people add to their notes","content":"During our research, we encountered multiple methods people used to structure information.\n\n### Naming\n\nPeople would name concepts, and in the process, give themselves a \"handle\" to refer to a whole idea. See [[C- Hypertext enables communication with high information density]] for more details. Autocomplete supports this use case, augmenting \"tip of my tongue\" recall.\n\n### Linking \n\nPeople would connect multiple concepts together, providing themselves navigational or semantic relationships. In the case [[Roam Research]], naming, categorization, chronology, pipeline, and description are all collapsed into links.\n\nNot all links are created equal. For example, most networked notebook software simply enables users to say that two items are related without the ability to label the relationship, providing a reason **why** they are related. By displaying [[contextual backlinks]], the user can view the relationship in the context of a sentence or paragraph without requiring additional organizational overhead in requiring the user to explicitly label relationships.\n\nAdditionally, links between two nodes gain a higher degree of fidelity when the nodes have types. For example, [[P- Joel Chan]]'s [[Discourse Graph Plugin]] gives you claims, questions, evidence, and sources by default, while enabling the user to create their own types, such as decisions, ideas, and people. Simply being able to filter relationships by node type implies a lot of meaning. For example, claims related to a decision likely support arguments on either side, and evidence related to a claim likely supports or opposes the claim.\n\nLinks can also vary in whether they are directed or undirected. For example, A --leads to--\u003e C, or A \u003c-is friends with-\u003e B. Most networked notebooks primarily support undirected links.\n\n### Categorization\n\nThis is when the user places items into classes or groups. This can happen in a bottom up manner through [affinity mapping](https://www.usertesting.com/blog/affinity-mapping), where users would take previously uncategorized information, place it into groups of similar content, and then name the groups. It can also happen top down, where people would come up with types of note (like claims, questions, and resources).\n\n### Description\n\nThis is typically a more bottoms up process than categorization, where things are defined in terms of how they are described, Then, when users want to pull up a note but can't remember the title of the note, they describe it and see what's similar. This is very similar to how people might recall an item when it's on the \"tip of their tongue.\" In conjunction with labeled relationships, this can be a very effective strategy. For example, \"I know there was a paper **about** psychological responses to sudden windfalls **by** a specific author.\" With labeled relationships, this sort of query would be trivial.\n\n### Logical rules\n\nThis includes equivalence statements like \"X is the same as Y,\" encompassing statements like \"X contains Y,\" or conditional statements like \"If X or Y, then Z.\"\n\nSome apps, like [[Obsidian]], [[LogSeq]], and [[Codex OS]], enable aliases, which can be thought of similarly to an equivalence statement, but are not quite the same thing. These enable you to refer to a single concept in multiple ways, but do not enable you to refer to one concept or the other and have references show up in both, like [[I- Provide Push and Pull as inline syntax to affect the related items section for a page|push and pull do in Agora]].\n\nEncompassing statements are available in some sense with applications like [[LogSeq]] and [[Bear]] that enable hierarchical namespacing. For example, in LogSeq, I might refer to specific applications as or Here I'm referring to something like \"any time I refer to Perceptual Control Theory, I'm also referring to Behavioral Science Theories generally, but not the other way around.\n\n[[Roam Research]] similarly enables encompassing statements through nested page titles - for example, `[[I- A [[structural editor]] for [[data structures]]]]` would reference both `[[structural editor]]` and `[[data structures]]` every time the encompassing page title was referenced. One might argue that this is more flexible than LogSeq's namespacing approach, as it does not scope either of the nested pages into one strict hierarchy.\n\n### Pipeline\n\nThis is when users specify different stages in a pipeline where a note can live. In the traditional [[Zettelkasten]] practice, there are fleeting notes and permanent notes. In [[P- Maggie Appleton]]'s variation of [[P- Andy Matuschak]]'s system, [notes are \"seedlings\" before they develop into \"evergreen.\"](https://www.youtube.com/watch?v=RXXXHN516qc) In my personal writing pipeline, I have [premise, develop, and publish](https://www.youtube.com/watch?v=F2To62BcMdI). See also: [[C- People process complex information in multiple levels and stages of processing]]\n\n### Location\n\nThis can be seen in apps that use folders or outliners that use indentation to signify hierarchy. \"Which folder does this idea go into?\" \"What should I indent this under?\" We saw multiple users concerned with [[quick capture]] complain that \"[[If I‚Äôm thinking too hard about structure before I write the note, I forget what I was going to write down]].\"\n\n### Chronology\n\n[[Roam Research]] is opinionated insofar as it starts all users on the \"Daily Notes Page\" when they load the app. If all other forms of organization fail, users may still be able to find what they are looking for based on which day they wrote about it.","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/TheBrain":{"title":"TheBrain","content":"Authored by:: [[P- Brendan Langen]]\n\nThis video gives a basic intro to TheBrain. It has a graphical and textual interface for navigating and editing your documents.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QEfvCE-vKTI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e\n\nWithin the graphical interface, it occasionally uses a DSL. For example, creating many branching nodes is similar to writing out the cells of a csv, except you separate nodes with a semicolon.\n\nFor a comparison with [[Roam Research]], see the following video from [[P- Matt Goldenberg]].\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tC2iUwY6dAU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen\u003e\u003c/iframe\u003e","lastmodified":"2022-05-20T00:31:26.515405557Z","tags":null},"/Twemex":{"title":"Twemex","content":"Authored by:: [[P- Brendan Langen]]\n\nTwemex is a Twitter search overlay that enables deeper exploratory and focused search. Twemex aims to help users construct a memex within their own accounts. \n\nAnother tool to assist with [[Search Behavior]]. \n\n![[Pasted image 20210921161522.png]]\n\nCreated by [[P- Geoffrey Litt]], whose philosophy lives on the twemex.app site. \n\n\u003e As a researcher, I love using Twitter as a _[memex](https://en.wikipedia.org/wiki/Memex)_: a tool for thinking and making connections between related ideas. I've used it to find so many fascinating people, thoughts, and discussions.\n\u003e But using Twitter this way has always felt like fighting the natural design of the tool. The Twitter product is trying to get me to refresh the feed, not grow my thoughts. That might make sense for most casual users, but I wanted something more powerful.\n\u003e So I started building a browser extension to make Twitter a better place to think. It all started out with a lightweight search UI, so that I could seamlessly search through tweets to reference while I was writing new threads, weaving together old ideas with new ones. Then I added a way to see people's best tweets, so I could get more from coming across an interesting user's profile.\n\u003e I used this extension myself for about 6 months, and after a while I couldn't live without it. Other people started asking me to share it, so I decided to turn it into a product. My hope is to build Twemex into a high-quality tool that makes Twitter a better memex for everyone.","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/Voiceliner":{"title":"Voiceliner","content":"Authored by:: [[P- Rob Haisfield]]\n\nBy [[P- Max Krieger]].\n\n- [[Twitter thread]] [[P- Rob Haisfield]]: People who note down fleeting thoughts want the cream to rise to the top. Some consider the act of processing notes as a future chore \u0026amp; never get around to it\n    - Voiceliner merges the behavior of ranking relative importance with the act of recording\n    - Excellent behavioral strategy https://twitter.com/maxkriegers/status/1469977466693324803 [*](https://twitter.com/RobertHaisfield/status/1470612139660361730)\n    - Thank you, @maxkriegers, for this thought processor interaction.\n      - Key insight: people quick capture and worry about losing important thoughts. However, as you write your quick thoughts, you have a general sense of how important they might be. Salience is key metadata! [*](https://twitter.com/RobertHaisfield/status/1470612495769292801)\n      - Why is this behavioral product strategy?\n        - [[P- Max Krieger]] wants to filter only the best quick capture notes into his general corpus. Traditionally, that would require him to review all of his notes and process them. Now that happens at the time of recording a note. [*](https://twitter.com/RobertHaisfield/status/1470615768937033728)\n      - He flipped the script on processing behaviors and when people see reward.\n        - Traditional quick capture: write now, process later. Processing can disrupt your creative flow and takes a lot of energy.\n        - Voiceliner: Process notes as you go. Later only review what is important. [*](https://twitter.com/RobertHaisfield/status/1470615769788551169)\n      - What people DO while using an app dramatically changes their outcomes.\n        - Given user goal W, people have to do X, Y, and Z behaviors. Y feels like a chore, so they don't do it. They fail.\n        - @maxkriegers merges X with Y in a fluid way, increasing the user's probability of success. [*](https://twitter.com/RobertHaisfield/status/1470616865164517380)\n      - I agree with @mekarpeles. Knowledge work is work, and one must lift weights. Reviewing, refining, \u0026amp; iterating, notes is the work\n        - People want to mine and refine their best thoughts, but it's effortful. There are many approaches to this behavioral problem!\n        - https://twitter.com/mekarpeles/status/1470616391011209217?s=20 [*](https://twitter.com/RobertHaisfield/status/1470618433465114624)\n      - [[P- Mek]]: Shame it didn't include your original tweet which I think is well argued and merits attention! üòä\n        - https://twitter.com/RobertHaisfield/status/1470612139660361730 [*](https://twitter.com/mekarpeles/status/1470619069737013248)\n\n    - [[P- Mek]]: I think your whole tweet is ü•á with a caveat:\n      - \u0026gt; Some consider the act of processing notes as a future chore\n      - One may make chores less painful \u0026amp; automate minutiae, but, \"There is no royal road to geometry.\" One still must lift weights.\n      - Training is Working\n      - https://mek.fyi/posts/in-defense-of-quantified-self [*](https://twitter.com/mekarpeles/status/1470616391011209217)\n      - I agree with your take here. Some behaviors simply have to be done in order to achieve ideal outcomes.\n        - @JoelChan86 and I explore the theme of people \"eating their broccoli\" and different behavioral design approaches in this video\n        - https://www.youtube.com/watch?v=PrFfYF7A8J0\u0026t=2s [*](https://twitter.com/RobertHaisfield/status/1470618913276788738)\n        - [[P- Mek]]: thanks for sharing [*](https://twitter.com/mekarpeles/status/1470619473757491205)\n\n        - What knowledge work weights do you believe the personas being discussed in this thread might benefit from? [*](https://twitter.com/RobertHaisfield/status/1470619895641477121)\n          - [[P- Mek]]: 5/ Quality of time; @aaronsw eludes to this in http://www.aaronsw.com/weblog/productivity. Different tasks take different energy levels. Apps being aware of that and gently presenting us with suggested actions accordingly seems kind/effective. [*](https://twitter.com/mekarpeles/status/1470623259074912257)\n\n          - [[P- Mek]]: 6/ I'm sure things can get meta very quickly. Visa vie https://twitter.com/mekarpeles/status/1470623259074912257, one's emotional state, disposition, or personal context may dictate what type of content one wants, and how verbose. Do I want a Google Now card? The wiki article? Or the comprehensive library shelf? [*](https://twitter.com/mekarpeles/status/1470623913059180545)\n\n          - [[P- Mek]]: 1/ First, I perceive there as being more informed experts than myself. Second, it depends what the goal is. Memorization? Recall? Meeting deadlines? Organization?\n            - I like the idea of pairing spaced repetition (media I'd like to experience again, TODOs, etc). [*](https://twitter.com/mekarpeles/status/1470621339908296704)\n\n          - [[P- Mek]]: 2/ I think technology can help inform weights, pose us questions \u0026amp; recommendations, \u0026amp; predict labels correctly. If we want to strengthen associations in our mind, this needs to be trained somehow. SRS is one way; most activities/games in life are some form of SRS. [*](https://twitter.com/mekarpeles/status/1470621596020785153)\n\n          - [[P- Mek]]: 3/ More than the technique, I think incentive alignment and emotional appeal are important. I believe I recall @andy_matuschak + @edelwax having interesting thoughts on design oriented around happy, positive, social, \u0026amp; emotional experiences. [*](https://twitter.com/mekarpeles/status/1470622320574218245)\n\n          - [[P- Mek]]: 4/ I think another factor is geo-spatial/temporal/just-in-time-context-specific recommendations. e.g. you're at a doctor's office, you're at the grocery store. [*](https://twitter.com/mekarpeles/status/1470622784363585536)\n\n- [[Twitter thread]] [[P- Max Krieger]]: üîÆ I made an app! What if we took the \"hold to record\" interaction too far? I decided to find out and made a new kind of quick capture. Like voice memos, but with a whole lot more oomph. Meet Voiceliner.\n    - https://a9.io/voiceliner/ [*](https://twitter.com/maxkriegers/status/1469977440030060544)\n    - 4+ months of on-and-off work led to the best way to braindump I know of. It's inspired by long conversations with friends, where scattered fragments of ideas and references come and go. What if you could save all of those, quickly, in a structured way? [*](https://twitter.com/maxkriegers/status/1469977441191882759)\n      - The app auto-transcribes what you tell it, but crucially, doesn't discard the original recording. The transcription is so you can recognize keywords, but your voice is treated as the \"source of truth\" to play back. [*](https://twitter.com/maxkriegers/status/1469977442152419329)\n      - You can indent notes under other notes by swiping. While you're recording, drag up to set a \"temperature\", so you remember if something's important. These interactions don't need eyes, just gestural memory. Like an instrument, a gadget. https://t.co/dquWzCSaE3 [*](https://twitter.com/maxkriegers/status/1469977466693324803)\n        - ![](https://pbs.twimg.com/ext_tw_video_thumb/1469953265609576449/pu/img/8d4-14pjnDhcudHp.jpg)\n      - I love walking the streets of SF with Voiceliner. Letting my mind wander or focus, knowing I can capture the flow of whatever I‚Äôm thinking about without stopping and typing. I have a special \"üö∂Walking\" list for these more reflective notes. Their locations end up on a map! https://t.co/QuV9l6FQgE [*](https://twitter.com/maxkriegers/status/1469977473009872897)\n        - ![](https://pbs.twimg.com/media/FGZS7K0VUAAMgw7.jpg)\n      - I owe many thanks to my alpha tester friends. As well as @lumar_isa, since it is an understatement to call her a magician.\n        - The app still has its bugs, and you're welcome to contribute if you know any Flutter https://github.com/maxkrieger/voiceliner [*](https://twitter.com/maxkriegers/status/1469977475262279680)\n      - So sorry, Android users! Need to get to the bottom of this and don‚Äôt know enough android users. It worked perfectly in the simulator‚Ä¶ https://twitter.com/NoteApps/status/1470050993056501763 [*](https://twitter.com/maxkriegers/status/1470210876225429510)\n      - I think we fixed it! üé§üé§üé§ [*](https://twitter.com/maxkriegers/status/1470477484084928521)\n      - Re: apple watch.. I need to buy one first.... https://twitter.com/michielrutjes/status/1470088275486519309 [*](https://twitter.com/maxkriegers/status/1470919068127416320)\n\n    - [[P- Tyler Angert]]: Lfg boi can‚Äôt wait to put all my lil braindumps all over the map [*](https://twitter.com/tylerangert/status/1470197646858067970)\n      - hope it can handle the tangert dumptruck üööüöö [*](https://twitter.com/maxkriegers/status/1470318434839785472)\n","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/Waldo":{"title":"Waldo","content":"Authored by:: [[P- Brendan Langen]]\n\nWaldo is a browser extension that amplifies Google search via advanced operators - scanning all keywords, images, and sentences on each webpage without having to open any links. [[C- Search terms express intentions]], and Waldo's search matches keywords and highlights as they appear in each sentence from the webpages found.\n\nWaldo's unique benefit comes from search within Waldo's own search engine. Users can add on keywords within the search to hone the accuracy of what they are searching for [(2:20 mark)](https://www.youtube.com/watch?v=YROf9Iixq7o)\n![[Pasted image 20210918124429.png]]. [[C- An exploratory search system should help the reader cumulatively gain information]], and Waldo enables this through its design. \n\nWaldo is built almost as a DSL, affording users to improve their level of search depth as time goes on [[C- A DSL enables semantic self-expression]], and Waldo has [[C- Search terms should have smart defaults so people don't have to always use semantic self-expression|smart defaults so people don't have to always use semantic self-expression]].\n\n[[P- Geoffrey Litt]]'s [[Twemex]] offers similar functionality as an override to Twitter's search. \n\nWaldo also offers on-demand research as a service, an interesting add-on to consider regarding [[Q- What community roles are necessary in a decentralized knowledge graph|roles needed in a decentralized discourse graph]].\n\nThis informs [[I- Search as a part of the primitive design]], and may speak to the development of a new feature. \n\nhttps://chrome.google.com/webstore/detail/waldo-%E2%80%93-search-less-see-m/codlehhdmfjjenohbcibmhhedmfmefib\n\nhttps://www.waldo.fyi/\n\nSearch enablement.\n![[Pasted image 20210918123446.png]]\nExtracting data. \n![[Pasted image 20210918123522.png]]\nFocused views. \n![[Pasted image 20210918123618.png]]\n","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/Why-people-prefer-to-structure-later-or-not-at-all":{"title":"Why people prefer to structure later or not at all","content":"\nThis page can be thought of as a contextual index page - so point to other pages while discussing how that's relevant to the common forms of structure people add to their notes. This is a hypertext hub.\n\n[[It takes too much work to create structure]]. Adding structure up front can be too restrictive, preventing people from [[My tooling gets in the way of working at the speed of thought|work at the speed of thought]]. [[If I‚Äôm thinking too hard about structure before I write the note, I forget what I was going to write down]]. If they don't add structure later, it's because [[Adding structure later feels like a chore]].\n\n[[C- Context is necessary for knowledge reuse]], but [[C- Specifying context for future reuse requires predicting trajectories of future reuse]] and [[C- Predicting trajectories of future reuse of information objects is hard]]. [[C- Specifying context for future reuse is costly]] and [[C- It is difficult to predict whether structure now will be worthwhile later]], so people determine that it is unlikely to be worth the effort.\n\nThrough all of this, an underlying problem exists in most tools: [[C- An increasing amount of structure leads to entropy]].\n\nFor more, see [[R- Formality Considered Harmful]] and [[C- Incremental formalization can mitigate risks of formalism in interactive systems]].\n","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/Wikum":{"title":"Wikum","content":"Authored by:: [[P- Brendan Langen]]\n\nWikum is a tool for rapidly organizing comments via recursive summarization in internet forums, affording a reader different views at the most important bits of information. \n\nWikum aims to help readers see a summary tree of the main points of discussion. Because [[C- Compression facilitates synthesis]], a version of this could act as a [[primitive design]]. \n\n[[C- Wikum allows you to summarize groups of comments on a Hacker News style forum]]","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/WorldBrain-Memex":{"title":"WorldBrain Memex","content":"Authored by:: [[P- Brendan Langen]]\n\nWorldBrain's Memex is an open-source, privacy-focused browser extension to search, annotate, organize and share your online research.  \n\nWorldBrain's Memex aims to eliminate time spent bookmarking, retracing steps to recall an old webpage, or copy-pasting notes into scattered documents.\n\nHeavily inspired by Vannevar Bush's vision of a Memex. \n\nCreated by - [[P- Oliver Sauter]]\n\nhttps://github.com/WorldBrain/Memex\n\nBase UI\n![[Pasted image 20210915181051.png]]\n\nAnnotate in page, like [[Hypothesis]]\n![[Pasted image 20210915181259.png]]","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/ZUI":{"title":"ZUI","content":"Authored by:: [[P- Brendan Langen]]\n\nA Zoomable User Interface, or ZUI, affords different views and layers to explore within a tool. \n\nExamples: \nThis can be seen in the form of:\n- graph views (like in [[Obsidian|Obsidian]]),\n- multiple layers of description (like [[Kosmik]]), or\n- foldable comment summaries (like in [[Wikum|Wikum]])\n- Many examples here: https://twitter.com/MatthewWSiu/status/1228155105683263490?ref_src=twsrc%5Etfw\n\n","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/content-addressable":{"title":"content addressable","content":"Authored by:: [[P- Brendan Langen]]\n\nRefers to conditions when a structure can be modified, as discussed in the possibility of a [[I- A structural editor for data structures]]. An important piece is the content retaining the same identity while being addressed in a specific state. \n\nYou can see examples of this in the block-based nature of [[Roam Research|Roam]]'s [[primitive design]]. Each block has its own stable ID that can be, and comments or additional indented blocks can exist as their own IDs. The original block can always be referenced by the initial ID, no matter the variations that come later. \n","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/conversations-chats-with-others":{"title":"conversations, chats with others","content":"","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/end-user-programming":{"title":"end-user programming","content":"Authored by:: [[P- Brendan Langen]]\n\n Refers to the act of software users (which are not necessarily developers) altering and adding on to their programs, perhaps through a [[DSL]]. End user programming is a a prominent affordance of modern distributed tools and systems, as it empowers self-expression through the modification of the tool. \n \n See the excellent work from Ink + Switch [2019](https://www.inkandswitch.com/end-user-programming.html) describing the past and future of end-user programming. ","lastmodified":"2022-05-20T00:31:26.519405561Z","tags":null},"/flexible-compression":{"title":"flexible compression","content":"Authored by:: [[P- Brendan Langen]]\n\nCompression and contextualizability are both are needed for synthesis. Flexible compression aims to be lossless to ensure context is retained through transclusion. Yet, this is often difficult, because [[C- Compression and contextualizability are in tension]]. \n\nWhen we are viewing something at different levels of abstraction, we are bound to miss certain details. Thus, we ask [[Q- What is an interface for going up and down the ladder of abstraction]]?","lastmodified":"2022-05-20T00:31:26.551405593Z","tags":null},"/functional-programming":{"title":"functional programming","content":"\n","lastmodified":"2022-05-20T00:31:26.551405593Z","tags":null},"/incremental-formalization":{"title":"incremental formalization","content":"Authored by:: [[P- Brendan Langen]]\n\nIncremental formalization is a powerful design pattern for overcoming risks of formality, as described in [[R- Formality Considered Harmful]]. [[C- Synthesis tools need to support incremental formalization]]. Information is added in an informal fashion to begin, before formalizing once the task becomes clearer. [[C- Context is necessary for knowledge reuse]], but this is difficult before we understand the task at hand.\n\nThis is a similar approach to [[P- Tiago Forte]]'s progressive summarization.","lastmodified":"2022-05-20T00:31:26.551405593Z","tags":null},"/primitive-design":{"title":"primitive design","content":"Authored by:: [[P- Brendan Langen]]\n\nComponents of a tool's base layer that afford user behavior. \n\nThese are the building blocks of [[Q- What are powerful interfaces for entering information into a discourse graph|powerful interfaces for entering information into a discourse graph]].","lastmodified":"2022-05-20T00:31:26.551405593Z","tags":null},"/quick-capture":{"title":"quick capture","content":"Quick capture refers to the general idea of getting a thought down quickly. Some apps, like Drafts and Weavit, are designed around this concept.","lastmodified":"2022-05-20T00:31:26.555405597Z","tags":null},"/search-or-create":{"title":"search-or-create","content":"Authored By:: [[P- Rob Haisfield]]\n\nIn [[Notational Velocity]], this was a mechanic where you create new pages through the search bar. You could either open one of the results or create a new page with your search query as the title. This is conceptually similar to [[autocomplete]].","lastmodified":"2022-05-20T00:31:26.555405597Z","tags":null},"/sensemaking":{"title":"sensemaking","content":"Authored by:: [[P- Rob Haisfield]]\n\n[[C- Sensemaking models partially model scholarly synthesis]]\n\n[[C- Sensemaking involves creating and manipulating a representation from raw data that makes some downstream task easier]]\n\n[[C- Synthesis is sensemaking on hard mode]]","lastmodified":"2022-05-20T00:31:26.555405597Z","tags":null},"/smart-default":{"title":"smart default","content":"\nSmart defaults are an absurdly powerful tool for product designers.\n\nWant to give users a choice while not requiring manual input every time? Implement the option that will work 80% of the time or guides user behavior in a particular way as a smart default, while giving users the option to manually adjust from there. This reduces the amount of effort people need to put in.\n\nAnother situation to use smart defaults: It is almost always advantageous for users to do X over Y. However, Y is the default (outcome if user does not make a choice). **Why not switch the default to X?**\n\nSmart defaults can ease the workload for power users. If the app can learn from user behavior, the user shouldn't have to tell it anything twice, Infer user intent and fill out the best guess as a default based on the user's history of manual input. [[Jump]] claims to place a focus on this.\n","lastmodified":"2022-05-20T00:31:26.555405597Z","tags":null},"/structural-editor":{"title":"structural editor","content":"","lastmodified":"2022-05-20T00:31:26.555405597Z","tags":null},"/structure-in-hindsight":{"title":"structure in hindsight","content":"","lastmodified":"2022-05-20T00:31:26.555405597Z","tags":null},"/structure-now-vs.-later-uncertain-payoff-regret":{"title":"structure now vs. later (uncertain payoff, regret)","content":"Beginning our research, we were excited to see all of the interesting structures that people use to manage their research, thoughts, and ideas. \n\n## [[The common forms of structure people add to their notes|What forms of structure do we see in thought processors?]]\n\nStructuring information enables a consistent synthesis process, and structure can be seen explicitly and implicitly. During our interviews, we saw people explicitly structure their thoughts in a variety of ways. \n\nAt times, the structure in place was seemingly everywhere.  People showed digital notebooks with a variety of page and block names describing projects and ideas (naming), inline wikilinks to other pages (linking), YAML metadata on each note (attributes), indenting in an outline (explicit relationships), color-coordinated highlights (attributes), detailed file/folder structures (explicit relationships), and block references (explicit relationships).\n\nIn other tools, namely spatial canvases like Figma, Miro, or Kosmik, structure was apparent implicitly.  A lasso drawn around a group of items signified they were to be considered together. A word in larger font above a list of items with smaller size shows significance in a grouped form. A line drawn between two items on the canvas connected the ideas.  This implicit structure was based on layout, type, and references to other items.\n\nYet, in the end, we were surprised to find that people simply don‚Äôt structure their work consistently.  We can think of this \"lack of structure\" as something that lacks formal machine, or even obvious human-readable, classification.  A few examples of this appeared as ‚Äúhalf-baked ideas,‚Äù fuzzy representations in their head or in conversation with others, quick capture notes, informal sketches, or margin notes and highlights.\n\nGiven the choice between structuring now vs. later, people frequently err on the side of having too little structure and rarely, if ever, add structure in hindsight.  We found that this often leaves them without useful artifacts they can build on with others (or their future selves).\n\n## This decision of how, when, and what to structure is a core problem of current thought processors\n\n### [[C- Specifying context for future reuse is costly]]\n\n[[C- Context is necessary for knowledge reuse]], but [[C- Specifying context for future reuse requires predicting trajectories of future reuse]] and [[C- Predicting trajectories of future reuse of information objects is hard]].\n\nAs seen in [[R- Formality Considered Harmful]], four problems arise from formalizing too early in the process. \n\n- Cognitive overhead - [[If I‚Äôm thinking too hard about structure before I write the note, I forget what I was going to write down]], because often the task of formalism is extraneous to the primary task. \n- Tacit knowledge - [[It takes too much work to create structure]] and [[My tooling gets in the way of working at the speed of thought]] \n- Premature structure - [[C- It is difficult to predict whether structure now will be worthwhile later]]. Further, It's hard to know what formalism is actually useful for their task up front.  A formalism that excludes speculation, for example, or rough scraps that we don't know what to do with yet, will prematurely kill the creative process.  Some [[People don‚Äôt intentionally review old notes]], or otherwise review notes so rarely that any effort expended on structure upfront would be wasted time.\n- Situational structure - People structure their work in a variety of different ways depending on the people, situation, and task.\n\n## Primary solutions\n\n### 1. Reduce the cost of structuring\n\nPeople tend to prefer just-in-time structuring because specifying context for future reuse is costly, difficult, and has unpredictable benefits. Enabling low-cost structure at the time of writing can help.\n\nWe should be careful as too much structure can inhibit the user's ability to maintain a system. [[C- An increasing amount of structure leads to entropy]], so ideally the system is able to infer a lot of structure from minimal inputs.\n\nNatural language processing can also reduce the user's burden, inferring relationships directly from the user's writing without requiring any special formatting. [[Jump]] is an attempt at this approach. We should note that relying too heavily on a computer's interpretation of natural language may lose some meaning in translation, so it is important to have an [[intermediate interface]] that both the user and the computer understand. Jump does that, but also will ask the user to confirm or reject its inferences.\n\nWe might also gather metadata from the structure of a workspace. If we use [[I- Workspaces as a primitive]], then we might infer that items within the same workspace are related to each other.\n\n[[C- Synthesis tools need to support incremental formalization]] as an additional measure - one example of support might be automatically placing notes on a spaced repetition schedule for review, ensuring that if you don't add all of the structure required at the time of writing, it will resurface at some point for you to add structure later.\n\n### 2. Increase the benefits of structuring.\n\n[[C- Reviewing past notes in the process of creating new notes is a key user behavior to promote synthesis]]. However, [[People don‚Äôt intentionally review old notes]]. Therefore, a promising direction could be to show associated items as you are writing. This is fleshed out in [[C- Designing for ambient review is a rich opportunity space]].\n\n### 3. Enable structure in hindsight\n\n[[I- User-created consistent rules as a primitive]] can help. As we can see in [[I- Enable composable queries to facilitate structure in hindsight]], the user might query for mentions of X or Y, and apply a new tag Z to all results.\n\nThe user might also create rules such as \"if X is indented underneath Y, then the two are related.\" In [[P- Joel Chan]]'s [[Discourse Graph Plugin]], one can even specify relationships such as \"Any evidence note indented underneath a claim note informs the claim.\"","lastmodified":"2022-05-20T00:31:26.555405597Z","tags":null},"/wasted-repeated-effort":{"title":"wasted repeated effort","content":"","lastmodified":"2022-05-20T00:31:26.555405597Z","tags":null},"/work-at-the-speed-of-thought":{"title":"work at the speed of thought","content":"","lastmodified":"2022-05-20T00:31:26.555405597Z","tags":null}}